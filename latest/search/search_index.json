{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#posted-potsdam-open-source-techno-economic-database","title":"POSTED: Potsdam Open-Source Techno-Economic Database","text":"<p>POSTED stands for Potsdam Open-Source Techno-Economic Database. It is pronounced the same way as the English word posted /\u02c8p\u0259\u028as.t\u026ad/.</p> <p>POSTED is a public database of techno-economic data on energy and climate-mitigation technologies and a framework for consistent handling of this database.</p> <ul> <li> <p> User Guide</p> <p>Understand the basic concepts of POSTED.</p> <p> Read documentation</p> </li> <li> <p> Database</p> <p>Look at the data currently available in the database.</p> <p> Look at the data</p> </li> <li> <p> API Reference</p> <p>Inspect the functions and classes of the POSTED framework written in Python.</p> <p> Read the code docs</p> </li> </ul>"},{"location":"#credits-and-thanks","title":"Credits and Thanks","text":"<ul> <li>The software creation and data curation have been primarily conducted at the Potsdam Institute for Climate Impact Research (PIK), a German research institute conducting integrated research for global sustainability.</li> <li>This work has been completed as part of the Ariadne project with funding from the German Federal Ministry of Research, Technology and Space (grant nos. 03SFK5A, 03SFK5A0-2).</li> <li>The software code has been written by P.C. Verpoort, with support by L. Heidweiler and P. Effing.</li> <li>The data has been curated primarily by P.C. Verpoort, with many valuable contributions by other colleagues from PIK (see <code>CITATION.cff</code> for a full list).</li> </ul>"},{"location":"#how-to-cite","title":"How to cite","text":"<ul> <li>To cite a release (recommended), please refer to a specific version archived on Zenodo.</li> <li>To cite a specific commit, please refer to the citation information in <code>CITATION.cff</code> and include the commit hash.</li> <li>In addition to citing this database, you may also want to cite the original sources. Bibliographic information for individual sources can be found under Sources or in the BibTeX file <code>sources.bib</code>.</li> </ul>"},{"location":"#licenses","title":"Licenses","text":"<p>The software code located in <code>posted/</code> in this repository is licensed under an MIT Licence.</p> <p>The data located in <code>posted/database/</code> in this repository is licensed under a CC-BY-4.0.</p>"},{"location":"sources/","title":"Sources","text":"<p>This page contains bibliographic information for the sources referenced in the <code>source</code> column of the techno-economic data files.</p> Identifier Bibliographic information Link <p>Abdin-2022</p> Zainul Abdin, Kaveh Khalilpour, and Kylie Catchpole. Projecting the levelized cost of large scale hydrogen storage for stationary applications. Energy Conversion and Management, 270:116241, 2022.  DOI  <p>Agora-Energiewende-2022</p> Agora Energiewende, FutureCamp, and Wuppertal Institut. Klimaschutzvertr\u00e4ge f\u00fcr die Industrietransformation: Rechner f\u00fcr die Absch\u00e4tzung der Transformationskosten einer klimafreundlichen Prim\u00e4rstahlproduktion. 2022. Version 1.1.  Link  <p>Al-Qahtani-2021</p> Amjad Al-Qahtani, Brett Parkinson, Klaus Hellgardt, Nilay Shah, and Gonzalo Guillen-Gosalbez. Uncovering the true cost of hydrogen production routes using life cycle monetisation. Applied Energy, 281:115958, Jan 2021.  DOI  <p>ArnaizDelPozo-2022</p> Carlos Arnaiz del Pozo and Schalk Cloete. Techno-economic assessment of blue and green ammonia as energy carriers in a low-carbon future. Energy Conversion and Management, 255:115312, Mar 2022.  DOI  <p>Bazzanella-2017</p> Alexis\u00a0Michael Bazzanella and Florian Ausfelder. Low carbon energy and feedstock for the European chemical industry. Technical Report, German Society for Chemical Engineering and Biotechnology (DECHEMA) and released by the European Chemical Industry Council (Cefic), 2017.  Link  <p>Bos-2020</p> M.J. Bos, S.R.A. Kersten, and D.W.F. Brilman. Wind power to methanol: Renewable methanol production using electricity, electrolysis of water and CO2 air capture. Applied Energy, 264:114672, 2020.  DOI  <p>Boundy-2011</p> Robert\u00a0Gary Boundy, Susan\u00a0W Diegel, Lynn\u00a0L Wright, and Stacy\u00a0Cagle Davis. Biomass Energy Data Book: Edition 4. Oak Ridge National Laboratory, 2011.  DOI  <p>Brandle-2021</p> Gregor Br\u00e4ndle, Max Sch\u00f6nfisch, and Simon Schulte. Estimating long-term global supply costs for low-carbon hydrogen. Applied Energy, 302:117481, Nov 2021.  DOI  <p>Buchenberg-2023</p> Patrick Buchenberg, Thushara Addanki, David Franzmann, Christoph Winkler, Felix Lippkau, Thomas Hamacher, Philipp Kuhn, Heidi Heinrichs, and Markus Blesl. Global Potentials and Costs of Synfuels via Fischer&amp;ndash;Tropsch Process. Energies, 2023.  Link  <p>Connelly-2019</p> Elizabeth Connelly, Michael Penev, Amgad Elgowainy, and Chad Hunter. Current Status of Hydrogen Liquefaction Costs. Technical Report, DOE, 2019. <p>DEA-TDRF-2023</p> Danish Energy Agency. Technology Data for Renewable Fuels. Aug 2023.  Link  <p>DeLena-2019</p> Edoardo De Lena, Maurizio Spinelli, Manuele Gatti, Roberto Scaccabarozzi, Stefano Campanari, Stefano Consonni, Giovanni Cinti, and Matteo\u00a0C. Romano. Techno-economic analysis of calcium looping processes for low CO2 emission cement plants. International Journal of Greenhouse Gas Control, 82:244\u2013260, March 2019.  DOI  <p>Devlin-2022</p> Alexandra Devlin and Aidong Yang. Regional supply chains for decarbonising steel: Energy efficiency and green premium mitigation. Energy Conversion and Management, 254:115268, Feb 2022.  DOI  <p>Dutta-2019</p> Arnab Dutta, Iftekhar\u00a0A. Karimi, and Shamsuzzaman Farooq. Technoeconomic Perspective on Natural Gas Liquids and Methanol as Potential Feedstocks for Producing Olefins. Industrial &amp; Engineering Chemistry Research, 58(2):963\u2013972, January 2019. Publisher: American Chemical Society.  DOI  <p>ECORYS-2008</p> ECORYS. Study on the Competitiveness of the European Steel Sector. Technical Report, ECORYS, Danish Technological Institute, Cambridge Econometrics, CES IFO, and IDEA Consult, 2008.  Link  <p>ETB-ND</p> ETB-ND. Fuels. Higher and Lower Calorific Values.  Link  <p>EUComm-2007</p> European Commission. Reference Document on Best Available Techniques for the Manufacture of Large Volume Inorganic Chemicals \u2014 Ammonia, Acids and Fertilisers. Technical Report, European Commission, 2007.  Link  <p>Fasihi-2015</p> M.\u00a0Fasihi, D.\u00a0Bogdanov, and C.\u00a0Breyer. Economics of Global LNG Trading Based on Hybrid PV-Wind Power Plants. 31st European Photovoltaic Solar Energy Conference and Exhibition; 3051-3067, 2015.  DOI  <p>Fasihi-2016</p> Mahdi Fasihi, Dmitrii Bogdanov, and Christian Breyer. Techno-Economic Assessment of Power-to-Liquids (PtL) Fuels Production and Global Trading Based on Hybrid PV-Wind Power Plants. Energy Procedia, 99:243\u2013268, 2016.  DOI  <p>Fasihi-2019</p> Mahdi Fasihi, Olga Efimova, and Christian Breyer. Techno-economic assessment of CO2 direct air capture plants. Journal of Cleaner Production, 224:957\u2013980, Jul 2019.  DOI  <p>Fiamelda-2020</p> L\u00a0Fiamelda, Suprihatin, and Purwoko. Analysis of water and electricity consumption of urea fertilizer industry: case study PT. X. IOP Conference Series: Earth and Environmental Science, 472(1):012034, Apr 2020.  DOI  <p>Franz-2009</p> Berit Franz and Stefan Franz. 1 x 1 der Gase. Physikalische Daten f\u00fcr Wissenschaft und Praxis, 2009. <p>Ghafri-2022</p> Saif ZS\u00a0Al Ghafri, Stephanie Munro, Umberto Cardella, Thomas Funke, William Notardonato, J.\u00a0P. Martin Trusler, Jacob Leachman, Roland Span, Shoji Kamiya, Garth Pearce, Adam Swanger, Elma Dorador Rodriguez, Paul Bajada, Fuyu Jiao, Kun Peng, Arman Siahvashi, Michael L. Johns, and Eric F. May. Hydrogen liquefaction: a review of the fundamental physics, engineering practice and future opportunities. Energy &amp; Environmental Science, 15(7):2690\u20132731, 2022. Publisher: Royal Society of Chemistry.  DOI  <p>Gorre-2019</p> Jachin Gorre, Fabian Ruoss, Hannu Karjunen, Tero Tynj\u00e4l\u00e4, and Johannes Schaffert. Cost benefits of optimizing hydrogen storage and methanation capacities for Power-to-Gas plants in dynamic operation. Applied Energy, 257:, 10 2019.  DOI  <p>Graf-2014</p> Frank Graf, Manuel G\u00f6tz, Marco Henel, Tanja Schaaf, and Robert Tichler. Techno\u00f6konomische Studie von Power-to-Gas-Konzepten. DVGW: Bonn, Germany, 2014. <p>Graham-2020</p> Paul Graham, Jenny Hayward, James Foster, and Lisa Havas. GenCost 2019-20. CSIRO publications repository, 2020.  DOI  <p>Grahn-2022</p> Maria Grahn, Elin Malmgren, Andrei\u00a0D Korberg, Maria Taljegard, James\u00a0E Anderson, Selma Brynolf, Julia Hansson, Iva\u00a0Ridjan Skov, and Timothy\u00a0J Wallington. Review of electrofuel feasibility\u2014cost and environmental impact. Progress in Energy, 4(3):032010, jun 2022.  DOI  <p>GrinbergDana-2016</p> Alon Grinberg Dana, Oren Elishav, Andr\u00e9 Bardow, Gennady\u00a0E. Shter, and Gideon\u00a0S. Grader. Nitrogen-Based Fuels: A Power-to-Fuel-to-Power Analysis. Angewandte Chemie International Edition, 55(31):8798\u20138805, 2016. arXiv:https://onlinelibrary.wiley.com/doi/pdf/10.1002/anie.201510618.  DOI  <p>PNNL-H2tools-ND</p> PNNL. Hydrogen Tools. 2023.  Link  <p>Hauser-2021</p> Philipp\u00a0D. Hauser, Helen Burmeister, Paul\u00a0J. M\u00fcnnich, Wido\u00a0K. Witecka, and Thomas M\u00fchlpointner. Klimaschutzvertr\u00e4ge f\u00fcr die Industrietransformation. Analyse zur Stahlbranche. Technical Report, Agora Energiewende, 2021. <p>Haynes-2014</p> W.M.\u00a0(Ed.) Haynes. CRC Handbook of Chemistry and Physics (95th ed.). CRC Press, 2014.  DOI  <p>Hegemann-2020</p> Karl-Rudolf Hegemann and Ralf Guder. Stahlerzeugung. Springer Fachmedien Wiesbaden, 2020. ISBN 9783658290917.  DOI  <p>Holling-2017</p> Marc H\u00f6lling, Matthias Weng, and Sebastian Gellert. Bewertung der Herstellung von Eisenschwamm unter Verwendung von Wasserstoff. Stahl und Eisen, 137(6):47\u201353, 2017. <p>Holst-2021</p> Marius Holst, Stefan Aschbrenner, Tom Smolinka, Christopher Voglst\u00e4tter, and Gunter Grimm. Cost forecast for low temperature electrolysis \u2013 technology driven bottom-up prognosis for PEM and Alkaline water electrolysis systems. Technical Report, Fraunhofer Institute for Solar Energy Systems ISE, 2021.  DOI   PDF  <p>ICAO-2017</p> ICAO. Sustainable Aviation Fuels Guide. Technical Report, International Civil Aviation Organization (ICAO), 2017.  Link  <p>IEA-Ammonia-2021</p> IEA. Ammonia Technology Roadmap \u2013 Towards more sustainable nitrogen fertiliser production. Technical Report, International Energy Agency, 2021.  Link  <p>IEA-DAC-2022</p> IEA. Energy needs of L-DAC and S-DAC. Technical Report, International Energy Agency, 2023.  Link  <p>IEA-FOH-2019</p> IEA. The Future of Hydrogen. Technical Report, International Energy Agency, 2019.  Link  <p>IEA-GHG-2013</p> IEA. Iron and Steel CCS Study (Techno-Economics Integrated Steel Mill). Technical Report, International Energy Agency, 2013.  Link  <p>IEA-GHR-2021</p> IEA. Global Hydrogen Review. Technical Report, International Energy Agency, 2021.  Link  <p>IEA-GHG-2017</p> IEA Greenhouse Gas R&amp;D Programme. Techno - Economic Evaluation of SMR Based Standalone (Merchant) Hydrogen Plant with CCS. feb 2017.  Link  <p>Ikaheimo-2018</p> Jussi Ik\u00e4heimo, Juha Kiviluoma, Robert Weiss, and Hannele Holttinen. Power-to-ammonia in future North European 100 % renewable power and heat system. International Journal of Hydrogen Energy, 43(36):17295\u201317308, Sep 2018.  DOI  <p>IRENA-2022</p> IRENA. Global hydrogen trade to meet the 1.5\u00b0C climate goal: Part III \u2013 Green hydrogen cost and potential. Technical Report, International Renewable Energy Agency, 2022.  Link  <p>Jacobasch-2021</p> Eric Jacobasch, Gregor Herz, Christopher Rix, Nils M\u00fcller, Erik Reichelt, Matthias Jahn, and Alexander Michaelis. Economic evaluation of low-carbon steelmaking via coupling of electrolysis and direct reduction. Journal of Cleaner Production, 328:129502, Dec 2021.  DOI  <p>Jarvis-2018</p> Sean\u00a0M. Jarvis and Sheila Samsatli. Technologies and infrastructures underpinning future CO 2 value chains: A comprehensive review and comparative analysis. Renewable and Sustainable Energy Reviews, 85:46\u201368, Apr 2018.  DOI  <p>Jasper-2015</p> Sarah Jasper and Mahmoud El-Halwagi. A Techno-Economic Comparison between Two Methanol-to-Propylene Processes. Processes, 3(3):684\u2013698, Sep 2015.  DOI  <p>Keith-2018</p> David\u00a0W. Keith, Geoffrey Holmes, David St. Angelo, and Kenton Heidel. A Process for Capturing CO2 from the Atmosphere. Joule, 2(8):1573\u20131594, Aug 2018.  DOI  <p>Kent-1974</p> James\u00a0A Kent. Riegel's handbook of industrial chemistry. Springer, 1974. <p>Kiani-2019</p> Ali Kiani, Michael Lejeune, Chaoen Li, Jim Patel, and Paul Feron. Liquefied synthetic methane from ambient CO2 and renewable H2 - A technoeconomic study. Journal of Natural Gas Science and Engineering, 94:104079, 2021.  DOI  <p>Korberg-2021</p> A.\u00a0D. Korberg, S.\u00a0Brynolf, M.\u00a0Grahn, and I.\u00a0R. Skov. Techno-economic assessment of advanced fuels and propulsion systems in future fossil-free ships. Renewable and Sustainable Energy Reviews, 142:110861, 2021.  DOI  <p>Lemmon-1997</p> Eric\u00a0W. Lemmon, Ian\u00a0H. Bell, Marcia\u00a0L. Huber, and Mark\u00a0O. McLinden. Thermophysical Properties of Fluid System. In Eds.\u00a0P.J. Linstrom and W.G. Mallard, editors, NIST Chemistry WebBook, NIST Standard Reference Database Number 69. National Institute of Standards and Technology, 1997.  DOI  <p>Lewis-2022</p> Eric Lewis, Shannon McNaul, Matthew Jamieson, Megan Henriksen, H.\u00a0Matthews, Liam Walsh, Jadon Grove, Travis Shultz, Timothy Skone, and Robert Stevens. Comparison of Commercial, State-of-the-Art, Fossil-Based Hydrogen Production Technologies. National Energy Technology Laboratory, April 2022.  DOI  <p>Lux-2021</p> Benjamin Lux, Johanna Gegenheimer, Katja Franke, Frank Sensfu\u00df, and Benjamin Pfluger. Supply curves of electricity-based gaseous fuels in the MENA region. Computers &amp; Industrial Engineering, 162:107647, 2021.  DOI  <p>Lux-2023</p> Benjamin Lux, Niklas Schneck, Benjamin Pfluger, Wolfgang M\u00e4nner, and Frank Sensfu\u00df. Potentials of direct air capture and storage in a greenhouse gas-neutral European energy system. Energy Strategy Reviews, 45:101012, 2023.  DOI  <p>Madhu-2021</p> Kavya Madhu, Stefan Pauliuk, Sumukha Dhathri, and Felix Creutzig. Understanding environmental trade-offs and resource demand of direct air capture technologies through comparative life-cycle assessment. Nature Energy, 6(11):1035\u20131044, Oct 2021.  DOI  <p>Matzen-2015</p> Michael\u00a0J. Matzen, Mahdi\u00a0H. Alhajji, and Yasar Demirel. Technoeconomics and Sustainability of Renewable Methanol and Ammonia Productions Using Wind Power-based Hydrogen. Journal of Advanced Chemical Engineering, 2015.  DOI  <p>Michalski-2017</p> Jan Michalski, Ulrich B\u00fcnger, Fritz Crotogino, Sabine Donadei, Gregor-S\u00f6nke Schneider, Thomas Pregger, Karl-Ki\u00ean Cao, and Dominik Heide. Hydrogen generation by electrolysis and storage in salt caverns: Potentials, economics and systems aspects with regard to the German energy transition. International Journal of Hydrogen Energy, 42(19):13427\u201313443, 2017. Special Issue on The 21st World Hydrogen Energy Conference (WHEC 2016), 13-16 June 2016, Zaragoza, Spain.  DOI  <p>Milanzi-2018</p> Sarah Milanzi, Carla Spiller, Benjamin Grosse, Lisa Hermann, and Joachim M\u00fcller-Kirchenbauer. Technischer Stand und Flexibilit\u00e4t des Power-to-Gas-Verfahrens. 2018.  DOI  <p>Molarne-1991</p> M.\u00a0Molnarne. CHEMSAFE \u2014 A Database for Safety Characteristic Data. In J\u00fcrgen Gmehling, editor, Software Development in Chemistry 5, 45\u201348. Berlin, Heidelberg, 1991. Springer Berlin Heidelberg. <p>Morgan-2014</p> Eric Morgan, James Manwell, and Jon McGowan. Wind-powered ammonia fuel production for remote islands: A case study. Renewable Energy, 72:51\u201361, 2014.  DOI  <p>NASEM-2019</p> National Academies of Sciences, Engineering and Medicine (NASEM). Negative Emissions Technologies and Reliable Sequestration: A Research Agenda. The National Academies Press, Washington, DC, 2019. ISBN 978-0-309-48452-7.  DOI  <p>Nyari-2020</p> Judit Ny\u00e1ri, Mohamed Magdeldin, Martti Larmi, Mika J\u00e4rvinen, and Annukka Santasalo-Aarnio. Techno-economic barriers of an industrial-scale methanol CCU-plant. Journal of CO2 Utilization, 39:101166, 2020.  DOI  <p>Oliveira-2021</p> Carina Oliveira. TNO's Technology Factsheet: Advanced Methanol to olefines process. Technical Report, TNO EnergieTransitie, 2021.  Link  <p>Otto-2017</p> Alexander Otto, Martin Robinius, Thomas Grube, Sebastian Schiebahn, Aaron Praktiknjo, and Detlef Stolten. Power-to-Steel: Reducing CO2 through the Integration of Renewable Energy and Hydrogen into the German Steel Industry. Energies, 10(4):451, Apr 2017.  DOI  <p>Ozkan-2022</p> Mihrimah Ozkan, Saswat\u00a0Priyadarshi Nayak, Anthony\u00a0D. Ruiz, and Wenmei Jiang. Current status and pillars of direct air capture technologies. iScience, 25(4):103990, 2022.  DOI  <p>Palys-2023</p> Matthew\u00a0J Palys and Prodromos Daoutidis. Techno-economic optimization of renewable urea production for sustainable agriculture and CO2 utilization. Journal of Physics: Energy, 6(1):015013, dec 2023.  DOI  <p>Perez-Fortes-2016</p> Mar P\u00e9rez-Fortes, Jan\u00a0C. Sch\u00f6neberger, Aikaterini Boulamanti, and Evangelos Tzimas. Methanol synthesis using captured CO2 as raw material: Techno-economic and environmental assessment. Applied Energy, 161:718\u2013732, Jan 2016.  DOI  <p>Ram-2018</p> Manish Ram, Michael Child, Arman Aghahosseini, Dmitrii Bogdanov, Alena Lohrmann, and Christian Breyer. A comparative analysis of electricity generation costs from renewable, fossil fuel and nuclear sources in G20 countries for the period 2015-2030. Journal of Cleaner Production, 199:687\u2013704, Oct 2018.  DOI  <p>Realmonte-2019</p> Giulia Realmonte, Laurent Drouet, Ajay Gambhir, James Glynn, Adam Hawkes, Alexandre\u00a0C K\u00f6berle, and Massimo Tavoni. An inter-model assessment of the role of direct air capture in deep mitigation pathways. Nature communications, 10(1):3277, 2019. <p>Rechberger-2020</p> Katharina Rechberger, Andreas Spanlang, Amaia Sasiain Conde, Hermann Wolfmeir, and Christopher Harris. Green Hydrogen?Based Direct Reduction for Low?Carbon Steelmaking. steel research international, 91(11):2000110, Jun 2020.  DOI  <p>Sasiain-2020</p> A.\u00a0Sasiain, K.\u00a0Rechberger, A.\u00a0Spanlang, I.\u00a0Kofler, H.\u00a0Wolfmeir, C.\u00a0Harris, and T.\u00a0B\u00fcrgler. Green Hydrogen as Decarbonization Element for the Steel Industry. BHM Berg- und H\u00fcttenm\u00e4nnische Monatshefte, 165(5):232\u2013236, Mar 2020.  DOI  <p>Schemme-2020</p> Steffen Schemme, Janos\u00a0Lucian Breuer, Maximilian K\u00f6ller, Sven Meschede, Fiona Walman, Remzi\u00a0Can Samsun, Ralf Peters, and Detlef Stolten. H2-based synthetic fuels: A techno-economic comparison of alcohol, ether and hydrocarbon production. International Journal of Hydrogen Energy, 45(8):5395\u20135414, 2020.  DOI  <p>Sens-2022</p> Lucas Sens, Ulf Neuling, and Martin Kaltschmitt. Capital expenditure and levelized cost of electricity of photovoltaic plants and wind turbines \u2013 Development by 2050. Renewable Energy, 185:525\u2013537, Feb 2022.  DOI  <p>Shen-2020</p> Wei Shen, Xi\u00a0Chen, Jing Qiu, Jennifier\u00a0A Hayward, Saad Sayeef, Peter Osman, Ke\u00a0Meng, and Zhao\u00a0Yang Dong. A comprehensive review of variable renewable energy levelized cost of electricity. Renewable and Sustainable Energy Reviews, 133:110301, Nov 2020.  DOI  <p>Socolow-2011</p> Robert Socolow, Michael Desmond, Roger Aines, Jason Blackstock, Olav Bolland, Tina Kaarsberg, Nathan Lewis, Marco Mazzotti, Allen Pfeffer, Karma Sawyer, Jeffrey Siirola, Berend Smit, and Jennifer Wilcox. Direct Air Capture of CO2 with Chemicals: A Technology Assessment for the APS Panel on Public Affairs. Technical Report, American Physical Society, 06 2011.  Link   PDF  <p>Soler-2022</p> Alba Soler, Victor Gordillo, William Liley, Patrick Schmidt, Werner Weindorf, Tom Houghton, and Stefano Dell'Orco. E-Fuels: A techno-economic assessment of European domestic production and imports towards 2050. Nov 2022.  Link  <p>Spallina-2017</p> Vincenzo Spallina, Ildefonso\u00a0Campos Velarde, Jos\u00e9 Antonio\u00a0Medrano Jimenez, Hamid\u00a0Reza Godini, Fausto Gallucci, and Martin Van Sint Annaland. Techno-economic assessment of different routes for olefins production through the oxidative coupling of methane (OCM): Advances in benchmark technologies. Energy Conversion and Management, 154:244\u2013261, 2017.  DOI  <p>Stolz-2022</p> Boris Stolz, Maximilian Held, Gil Georges, and Konstantinos Boulouchos. Techno-economic analysis of renewable fuels for ships carrying bulk cargo in Europe. Nature Energy, 7(2):203\u2013212, 2022.  DOI  <p>Takunju-2021</p> Roy\u00a0Tarh Takunju. Life cycle assessment of the production of a solar MTG fuel based on electrochemical hydrogen production with energy supply by a PV/CSP hybrid solar power plant. Master's thesis, Ruhr-Universit\u00e4t Bochum, Dezember 2021.  Link  <p>Tenhumberg-2020</p> Nils Tenhumberg and Karsten B\u00fcker. Ecological and Economic Evaluation of Hydrogen Production by Different Water Electrolysis Technologies. Chemie Ingenieur Technik, 92(10):1586\u20131595, Aug 2020.  DOI   PDF  <p>Thema-2019</p> M.\u00a0Thema, F.\u00a0Bauer, and M.\u00a0Sterner. Power-to-Gas: Electrolysis and methanation status review. Renewable and Sustainable Energy Reviews, 112:775\u2013787, 2019.  DOI  <p>Ueckerdt-2021</p> Falko Ueckerdt, Christian Bauer, Alois Dirnaichner, Jordan Everall, Romain Sacchi, and Gunnar Luderer. Potential and risks of hydrogen-based e-fuels in climate change mitigation. Nature Climate Change, 11:1\u201310, 05 2021.  DOI  <p>Vartiainen-2022</p> Eero Vartiainen, Christian Breyer, David Moser, Eduardo Rom\u00e1n Medina, Chiara Busto, Ga\u00ebtan Masson, Elina Bosch, and Arnulf J\u00e4ger-Waldau. True Cost of Solar Hydrogen. Solar RRL, 6(5):2100487, Sep 2022.  DOI   PDF  <p>Vogl-2018</p> Valentin Vogl, Max \u00c5hman, and Lars\u00a0J. Nilsson. Assessment of hydrogen direct reduction for fossil-free steelmaking. Journal of Cleaner Production, 203:736\u2013745, Dec 2018.  DOI  <p>Wang-2023</p> Michael Wang, Amgad Elgowainy, Uisung Lee, Kwang\u00a0H. Baek, Sweta Balchandani, Pahola\u00a0T. Benavides, Andrew Burnham, Hao Cai, Peter Chen, Yu\u00a0Gan, Ulises\u00a0R. Gracida-Alvarez, Troy\u00a0R. Hawkins, Tai-Yuan Huang, Rakesh\u00a0K. Iyer, Saurajyoti Kar, Jarod\u00a0C. Kelly, Taemin Kim, Christopher Kolodziej, Kyuha Lee, Xinyu Liu, Zifeng Lu, Farhad Masum, Michele Morales, Clarence Ng, Longwen Ou, Tuhin Poddar, Krishna Reddi, Siddharth Shukla, Udayan Singh, Lili Sun, Pingping Sun, Tom Sykora, Pradeep Vyawahare, and Jingyi Zhang. Greenhouse gases, Regulated Emissions, and Energy use in Technologies Model \u00ae (2023 Excel). oct 2023.  DOI  <p>Worrell-2007</p> Ernst Worrell, Lynn Price, Maarten Neelis, Christina Galitsky, and Nan Zhou. World Best Practice Energy Intensity Values for Selected Industrial Sectors. Technical Report, Lawrence Berkeley National Laboratory, 2007.  Link  <p>Wortler-2013</p> Martin W\u00f6rtler, Felix Schuler, Nicole Voigt, Torben Schmidt, Peter Dahlmann, Hans\u00a0Bodo L\u00fcngen, and Jean-Theo Ghenda. Steel's Contribution to a Low-Carbon Europe 2050. Technical Report, The Boston Consulting Group, 2013.  Link  <p>Yates-2020</p> Jonathon Yates, Rahman Daiyan, Robert Patterson, Renate Egan, Rose Amal, Anita Ho-Baille, and Nathan\u00a0L. Chang. Techno-economic Analysis of Hydrogen Electrolysis from Off-Grid Stand-Alone Photovoltaics Incorporating Uncertainty Analysis. Cell Reports Physical Science, 1(10):100209, Oct 2020.  DOI   PDF  <p>Zhang-2020</p> Jinrui Zhang, Hans Meerman, Ren\u00e9 Benders, and Andr\u00e9 Faaij. Comprehensive review of current natural gas liquefaction processes on technical and economic performance. Applied Thermal Engineering, 166:114736, 2020.  DOI  <p>Zhao-2021</p> Zhitong Zhao, Jingyang Jiang, and Feng Wang. An economic analysis of twenty light olefin production pathways. Journal of Energy Chemistry, 56:193\u2013202, 2021.  DOI"},{"location":"api/","title":"API Reference","text":""},{"location":"api/#posted.TEDF","title":"<code>TEDF</code>","text":"<p>Class to handle Techno-Economic Data Files (TEDFs).</p> <p>Parameters:</p> Name Type Description Default <code>parent_variable</code> <code>str | None</code> <p>Variable from which Data should be collected</p> <code>None</code> <code>database_id</code> <code>str | None</code> <p>Database from which to load data</p> <code>None</code> <code>file_path</code> <p>File Path from which to load file</p> required <code>data</code> <p>Specific Technoeconomic data</p> required <p>Methods:</p> Name Description <code>load</code> <p>Load TEDF if it has not been read yet.</p> <code>read</code> <p>Read TEDF from CSV file.</p> <code>write</code> <p>Write TEDF to CSV file.</p> <code>check</code> <p>Check if TEDF is consistent.</p> <code>check_row</code> <p>Check that row in TEDF is consistent and return all inconsistencies found for row.</p> Source code in <code>posted/noslag/tedf.py</code> <pre><code>class TEDF:\n    \"\"\"\n    Class to handle Techno-Economic Data Files (TEDFs).\n\n    Parameters\n    ----------\n    parent_variable: str\n        Variable from which Data should be collected\n    database_id: str, default: public\n        Database from which to load data\n    file_path: Path, optional\n        File Path from which to load file\n    data: pd.DataFrame, optional\n        Specific Technoeconomic data\n\n    Methods\n    ----------\n    load\n        Load TEDF if it has not been read yet.\n    read\n        Read TEDF from CSV file.\n    write\n        Write TEDF to CSV file.\n    check\n        Check if TEDF is consistent.\n    check_row\n        Check that row in TEDF is consistent and return all\n        inconsistencies found for row.\n    \"\"\"\n\n    def __init__(\n        self,\n        df: pd.DataFrame,\n        parent_variable: str | None = None,\n        database_id: str | None = None,\n        variables: dict | None = None,\n        custom_fields: dict | None = None,\n        custom_comments: dict | None = None,\n        masks: list[Mask] | None = None,\n    ):\n        \"\"\"Initialise parent class and object fields\"\"\"\n        self._parent_variable: str | None = parent_variable\n        self._database_id: str | None = database_id\n        self._variables: dict[str, dict] = variables or {}\n        self._masks: list[Mask] = masks or []\n        self._validated: pd.DataFrame | None = None\n\n        # Combine all fields.\n        source_column = base_column_src()\n        self._fields: dict[str, AbstractFieldDefinition] = (\n            {\"source\": source_column}\n            | (custom_fields or {})\n        )\n\n        # Combine all comments.\n        self._comments: dict[str, CommentDefinition] = (\n            base_columns_src_detail\n            | {\"comment\": CommentDefinition}\n            | (custom_comments or {})\n        )\n\n        # Combine all columns.\n        self._columns: dict[str, AbstractColumnDefinition] = (\n            {\"source\": source_column}\n            | base_columns_src_detail\n            | (custom_fields or {})\n            | base_columns_other\n        )\n\n        # Deal with unknown columns.\n        unknown_cols = [c for c in df if c not in self._columns]\n        if unknown_cols:\n            i = len(unknown_cols)\n            warn(\n                f\"Unknown column{'s'[:i^1]} treated as comment{'s'[:i^1]}: \"\n                + \", \".join(str(c) for c in unknown_cols),\n                POSTEDWarning,\n            )\n            unknown_cols = {\n                c: CommentDefinition(\n                    name=str(c),\n                    description=\"\",\n                    required=False,\n                )\n                for c in unknown_cols\n            }\n            self._comments |= unknown_cols\n            self._columns |= unknown_cols\n\n        self._df: pd.DataFrame = df[list(self._columns)]\n\n    @property\n    def raw(self) -&gt; pd.DataFrame:\n        return self._df\n\n    @property\n    def parent_variable(self) -&gt; str:\n        return self._parent_variable\n\n    @property\n    def fields(self) -&gt; dict[str, AbstractFieldDefinition]:\n        return self._fields\n\n    @property\n    def comments(self) -&gt; dict[str, CommentDefinition]:\n        return self._comments\n\n    @property\n    def columns(self) -&gt; dict[str, AbstractColumnDefinition]:\n        return self._columns\n\n    @property\n    def variables(self) -&gt; dict[str, dict]:\n        return self._variables\n\n    @property\n    def validated(self) -&gt; pd.DataFrame:\n        return self._validated\n\n    @classmethod\n    def load(cls, parent_variable: str, database_id: str = \"public\"):\n        if not isinstance(parent_variable, str):\n            raise POSTEDException(\n                \"Argument `variable` must be a valid string.\"\n            )\n        if not (database_id in databases):\n            raise POSTEDException(\n                \"Argument `database_id` must correspond to a valid ID in the \"\n                \"`databases` registered in the POSTED package.\"\n            )\n\n        database_path = databases[database_id]\n        rel_path = \"/\".join(parent_variable.split(\"|\"))\n\n        # Load data.\n        df = read_tedf_from_csv(\n            database_path / \"tedfs\" / (rel_path + \".csv\")\n        )\n\n        # Load config.\n        variables = {}\n        custom_columns = {}\n        masks = []\n\n        for database_path in databases.values():\n            fpath = database_path / \"tedfs\" / (rel_path + \".yaml\")\n            if fpath.is_file():\n                fcontents = read_yaml(fpath)\n                if \"variables\" in fcontents:\n                    if \"predefined\" in fcontents[\"variables\"]:\n                        for predefined in fcontents[\"variables\"][\"predefined\"]:\n                            variables |= read_yaml(\n                                database_path\n                                / \"variables\"\n                                / \"definitions\"\n                                / (predefined + \".yaml\")\n                            )\n                    if \"custom\" in fcontents[\"variables\"]:\n                        variables |= fcontents[\"variables\"][\"custom\"]\n                if \"columns\" in fcontents:\n                    custom_columns |= fcontents[\"columns\"]\n\n            fpath = database_path / \"masks\" / (rel_path + \".yaml\")\n            if fpath.is_file():\n                fcontents = read_yaml(fpath)\n                masks += [Mask(**mask_specs) for mask_specs in fcontents]\n\n        custom_fields, custom_comments = read_fields_comments(custom_columns)\n\n        return TEDF(\n            df=df,\n            parent_variable=parent_variable,\n            database_id=database_id,\n            variables=variables,\n            custom_fields=custom_fields,\n            custom_comments=custom_comments,\n            masks=masks,\n        )\n\n    def edit_data(self):\n        from ..widget import build_edit_grid\n        return build_edit_grid(self)\n\n    def validate(self):\n        # Load sources for validation.\n        from ..sources import load_sources\n        sources = list(load_sources(database_id=self._database_id).entries)\n        self._fields[\"source\"].set_bibtex_codes(sources)\n\n        self._validated = pd.DataFrame()\n        for col_id, col_def in self._columns.items():\n            self._validated[col_id] = col_def.validate(self._df[col_id])\n\n    def _prepare(self) -&gt; pd.DataFrame:\n        df = self._df.replace(\"\", np.nan)\n\n        # Value, uncertainty, and reference value must be floats.\n        for col_id in [\"value\", \"uncertainty\", \"reference_value\"]:\n            df[col_id] = pd.to_numeric(df[col_id])\n\n        # TODO: Turn fields into categories.\n\n        return df\n\n    def normalise(\n        self, units: Optional[dict[str, str]] = None, with_parent: bool = False\n    ) -&gt; pd.DataFrame | None:\n        \"\"\"\n        Normalise data by converting reference values to 1.0 and converting to\n        default unit for each variable.\n\n        Parameters\n        ----------\n        units: dict[str,str], optional\n            Dictionary with key-value pairs of units to use for variables.\n        with_parent: bool, optional\n            Whether to prepend the parent variable. Default is False.\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame containing normalised raw data.\n        \"\"\"\n        normalised, units = self._normalise(units)\n\n        # Insert unit, reference value, and reference unit.\n        normalised.insert(\n            normalised.columns.tolist().index(\"uncertainty\") + 1,\n            \"unit\",\n            np.nan,\n        )\n        normalised[\"unit\"] = normalised[\"variable\"].map(units)\n        normalised.insert(\n            normalised.columns.tolist().index(\"unit\") + 1,\n            \"reference_value\",\n            1.0,\n        )\n        normalised.insert(\n            normalised.columns.tolist().index(\"reference_value\") + 1,\n            \"reference_unit\",\n            np.nan,\n        )\n        normalised[\"reference_unit\"] = normalised[\"reference_variable\"].map(\n            units\n        )\n\n        # Prepend parent variable.\n        if with_parent:\n            if self._parent_variable is None:\n                raise Exception(\n                    \"Can only prepend parent variable if not None.\"\n                )\n            normalised[\"variable\"] = normalised[\"variable\"].str.cat(\n                [self._parent_variable], sep=\"|\"\n            )\n\n\n        # Order columns.\n        normalised = normalised[\n            [col for col in self._columns if col in normalised]\n        ]\n\n        return normalised\n\n    def _normalise(\n        self, units: dict[str, str] | None\n    ) -&gt; tuple[pd.DataFrame, dict[str, str]]:\n        units = units or {}\n        df = self._prepare()\n\n        # Get full list of variables and corresponding units.\n        df_vars_units = pd.concat(\n            [\n                df[[\"variable\", \"unit\"]],\n                df[[\"reference_variable\", \"reference_unit\"]]\n                .dropna(how=\"all\")\n                .rename(\n                    columns={\n                        \"reference_variable\": \"variable\",\n                        \"reference_unit\": \"unit\",\n                    }\n                ),\n            ]\n        )\n\n        # Determine default units for all variables.\n        currencies_pattern = rf\"({'|'.join(ureg.currencies)})_\\d{{4}}\"\n        units = (\n            df_vars_units.assign(\n                unit=df_vars_units[\"unit\"].str.replace(\n                    currencies_pattern, defaults[\"currency\"], regex=True\n                ),\n            )\n            .groupby(\"variable\")[\"unit\"]\n            .agg(lambda x: x.mode()[0])\n            .to_dict()\n        ) | units\n\n        # Determine unit conversion factors.\n        conv_factors = (\n            df_vars_units.groupby(\"variable\")[\"unit\"]\n            .apply(\n                lambda group: pd.Series(\n                    {\n                        u: ureg(u).to(units[group.name]).m\n                        for u in group.unique()\n                    }\n                )\n            )\n            .reset_index()\n            .rename(columns={\"unit\": \"conv_factor\", \"level_1\": \"unit\"})\n        )\n\n        # For now, we simply assume that there is no column called `conv_factor`.\n        assert (\n            s not in df\n            for s in [\"factor\", \"conv_factor\", \"reference_conv_factor\"]\n        )\n\n        # Then we can just merge in the conversion factors and apply.\n        normalised = (\n            df\n            .merge(\n                conv_factors,\n                on=[\"variable\", \"unit\"],\n                how=\"left\",\n            )\n            .merge(\n                conv_factors.rename(columns=lambda s: \"reference_\" + s),\n                on=[\"reference_variable\", \"reference_unit\"],\n                how=\"left\",\n            )\n            .assign(\n                factor=lambda df: df[\"conv_factor\"]\n                / (df[\"reference_value\"] * df[\"reference_conv_factor\"]).where(\n                    df[\"reference_variable\"].notnull(), other=1.0\n                ),\n                value=lambda df: df[\"value\"] * df[\"factor\"],\n                uncertainty=lambda df: df[\"uncertainty\"] * df[\"factor\"],\n            )\n            .drop(\n                columns=[\n                    \"factor\",\n                    \"conv_factor\",\n                    \"reference_conv_factor\",\n                    \"reference_value\",\n                    \"unit\",\n                    \"reference_unit\",\n                ]\n            )\n        )\n\n        # Return normalised data and variable units.\n        return normalised, units\n\n    # Prepare data for selection.\n    def select(\n        self,\n        units: Optional[dict[str, str]] = None,\n        reference_activity: Optional[str] = None,\n        reference_capacity: Optional[str] = None,\n        drop_singular_fields: bool = True,\n        period_mode: str | PeriodMode = PeriodMode.INTER_AND_EXTRAPOLATION,\n        expand_not_specified: bool | list[str] = True,\n        with_parent: bool = False,\n        append_references: bool = False,\n        **field_vals_select,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Select desired data from the dataframe.\n\n        Parameters\n        ----------\n        units: dict[str,str], optional\n            Dictionary with key-value pairs of units to use for variables.\n        reference_activity: str, optional\n            Reference activity.\n        reference_capacity: str, optional\n            Reference capacity.\n        drop_singular_fields: bool, optional\n            If True, drop custom fields with only one value\n        interpolate_period: bool, optional\n            If True, determine values by interpolation between known points,\n            if no value for a requested period is given. Default is True.\n        extrapolate_period: bool, optional\n            If True, determine values by extrapolation outside of range of\n            known data, if no value for a requested period is given. Default\n            is False.\n        expand_not_specified: bool | list[str], optional\n            Whether to expand fields with value `N/S` (not specified) to all\n            allowed values. If `True` is passed, then allow `N/S` is expanded\n            for all fields. If a list of strings is passed, then only the\n            contained fields are expanded. If False is passed, then no field\n            is expanded. Default is True.\n        with_parent: bool, optional\n            Whether to prepend the parent variable. Default is False.\n        **field_vals_select\n            IDs of values to select\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame with selected Values\n        \"\"\"\n        selected, units, ref_vars = self._select(\n            units=units,\n            reference_activity=reference_activity,\n            reference_capacity=reference_capacity,\n            drop_singular_fields=drop_singular_fields,\n            period_mode=period_mode,\n            expand_not_specified=expand_not_specified,\n            **field_vals_select,\n        )\n\n        # Finalise dataframe and return.\n        return self._finalise(\n            df=selected,\n            append_references=append_references,\n            group_cols=[c for c in self._fields if c in selected],\n            ref_vars=ref_vars,\n            units=units,\n            with_parent=with_parent,\n        )\n\n    def _select(\n        self,\n        units: dict[str, str] | None,\n        reference_activity: str | None,\n        reference_capacity: str | None,\n        drop_singular_fields: bool,\n        period_mode: str | PeriodMode,\n        expand_not_specified: bool | list[str],\n        **field_vals_select,\n    ) -&gt; tuple[pd.DataFrame, dict[str, str], dict[str, str]]:\n        # Start from normalised data.\n        normalised, units = self._normalise(units)\n        selected = normalised\n\n        # Drop columns containing comments and the uncertainty column (which is currently unsupported).\n        selected.drop(\n            columns=[\"uncertainty\"] + list(self._comments),\n            inplace=True,\n        )\n\n        # Raise exception if fields listed in arguments that is not in the columns.\n        for field_id in field_vals_select:\n            if not any(field_id == col_id for col_id in self._fields):\n                raise Exception(\n                    f\"Field '{field_id}' does not exist and cannot be used for \"\n                    f\"selection.\"\n                )\n\n        # Order fields for selection: period must be selected last due to the interpolation.\n        fields_select_order = list(set(field_vals_select) | set(self._fields))\n        if \"period\" in fields_select_order:\n            fields_select_order.remove(\"period\")\n            fields_select_order.append(\"period\")\n\n        # Expand non-specified values in fields if requested.\n        if expand_not_specified is True:\n            expand_not_specified = self._fields\n        elif expand_not_specified is False:\n            expand_not_specified = []\n        else:\n            if any(f not in self._fields for f in expand_not_specified):\n                raise Exception(\n                    \"N/S values can only be expanded on fields: \"\n                    + \", \".join(self._fields)\n                )\n        for field_id in expand_not_specified:\n            selected[field_id].replace(\"N/S\", \"*\")\n\n        # Convert str to PeriodMode if needed.\n        if isinstance(period_mode, str):\n            period_mode = PeriodMode.from_str(period_mode)\n\n        # Select and expand fields.\n        for field_id in fields_select_order:\n            selected = self._fields[field_id].select_and_expand(\n                df=selected,\n                col_id=field_id,\n                field_vals=field_vals_select.get(field_id, None),\n                expand_not_specified=expand_not_specified,\n                period_mode=period_mode,\n            )\n\n        # Check for duplicates.\n        field_var_cols = selected[\n            list(self._fields) + [\"variable\", \"reference_variable\"]\n        ]\n        duplicates = field_var_cols.duplicated()\n        if duplicates.any():\n            raise POSTEDException(\n                \"Duplicate field/variable entries:\\n\"\n                + str(field_var_cols.loc[duplicates])\n            )\n\n        # Drop fields with only one value.\n        if drop_singular_fields:\n            selected.drop(\n                columns=[\n                    col_id\n                    for col_id in self._fields\n                    if selected[col_id].nunique() &lt; 2\n                ],\n                inplace=True,\n            )\n\n        # Determine activity and capacity variables and their references.\n        activities = [\n            _var_pattern(var_name, keep_token_names=False)\n            for var_name, var_specs in self._variables.items()\n            if var_specs.get(\"reference\", None) == \"activity\"\n        ]\n        reference_activity = reference_activity or _get_reference(\n            self._df[\"reference_variable\"], activities\n        )\n        capacities = [\n            _var_pattern(var_name, keep_token_names=False)\n            for var_name, var_specs in self._variables.items()\n            if var_specs.get(\"reference\", None) == \"capacity\"\n        ]\n        reference_capacity = reference_capacity or _get_reference(\n            self._df[\"reference_variable\"], capacities\n        )\n\n        # Map variables.\n        fields = [c for c in self._fields if c in selected]\n        mapped, units = map_variables(\n            selected=selected,\n            units=units,\n            fields=fields,\n            activities=activities,\n            capacities=capacities,\n            reference_activity=reference_activity,\n            reference_capacity=reference_capacity,\n        )\n\n        # Drop rows with failed mappings.\n        mapped = mapped.dropna(subset=\"value\").reset_index(drop=True)\n\n        # Get dict of variables and corresponding reference variables.\n        ref_vars = (\n            mapped[[\"variable\", \"reference_variable\"]]\n            .drop_duplicates()\n            .set_index(\"variable\")[\"reference_variable\"]\n        )\n\n        # Check for multiple reference variables per reported variable.\n        if not ref_vars.index.is_unique:\n            duplicated_vars = ref_vars.index[ref_vars.index.duplicated()]\n            raise Exception(\n                f\"Multiple reference variables per reported variable found:\\n\"\n                + ref_vars[duplicated_vars].to_string()\n                + \"\\n\\n\"\n                + \"These are the rows:\\n\"\n                + mapped.loc[\n                    mapped[\"variable\"].isin(duplicated_vars)\n                ].to_string()\n            )\n        ref_vars = ref_vars.to_dict()\n\n        # Remove reference_variable column.\n        mapped.drop(columns=[\"reference_variable\"], inplace=True)\n\n        # Return.\n        return mapped, units, ref_vars\n\n    def aggregate(\n        self,\n        units: Optional[dict[str, str]] = None,\n        reference_activity: Optional[str] = None,\n        reference_capacity: Optional[str] = None,\n        drop_singular_fields: bool = True,\n        period_mode: PeriodMode | str = PeriodMode.INTER_AND_EXTRAPOLATION,\n        agg: Optional[str | list[str] | tuple[str]] = None,\n        masks: Optional[list[Mask]] = None,\n        masks_database: bool = True,\n        expand_not_specified: bool | list[str] = True,\n        with_parent: bool = False,\n        append_references: bool = False,\n        **field_vals_select,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Aggregates data based on specified parameters, applies masks,\n        and cleans up the resulting DataFrame.\n\n        Parameters\n        ----------\n        units: dict[str, str], optional\n            Dictionary with key, value paris of variables to override\n        reference_activity: str, optional\n            The activity variable to align all activities on.\n        reference_capacity: str, optional\n            The capacity variable to align all capacities on.\n        drop_singular_fields: bool, optional\n            If True, drop custom fields with only one value\n        extrapolate_period: bool, optional\n            If True, extrapolate values by extrapolation, if no value\n            for this period is given\n        expand_not_specified: bool | list[str], optional\n            Whether to expand fields with value `N/S` (not specified) to all\n            allowed values. If `True` is passed, then allow `N/S` is expanded\n            for all fields. If a list of strings is passed, then only the\n            contained fields are expanded. If False is passed, then no field\n            is expanded. Default is True.\n        agg : Optional[str | list[str] | tuple[str]]\n            Specifies which fields to aggregate over.\n        masks : Optional[list[Mask]]\n            Specifies a list of Mask objects that will be applied to the\n            data during aggregation. These masks can be used to filter\n            or weight the data based on certain conditions defined in\n            the Mask objects.\n        masks_database : bool, optional\n            Determines whether to include masks from databases in the\n            aggregation process. If set to `True`, masks from databases\n            will be included along with any masks provided as function\n            arguments. If set to `False`, only the masks provided as\n            function arguments will be applied.\n\n        Returns\n        -------\n        pd.DataFrame\n            The `aggregate` method returns a pandas DataFrame that has\n            been cleaned up and aggregated based on the specified\n            parameters and input data. The method performs aggregation\n            over component fields and cases fields, applies weights\n            based on masks, drops rows with NaN weights, aggregates with\n            weights, inserts reference variables, sorts columns and\n            rows, rounds values, and inserts units before returning the\n            final cleaned and aggregated DataFrame.\n        \"\"\"\n\n        # Run select().\n        selected, units, ref_vars = self._select(\n            units=units,\n            reference_activity=reference_activity,\n            reference_capacity=reference_capacity,\n            period_mode=period_mode,\n            drop_singular_fields=drop_singular_fields,\n            expand_not_specified=expand_not_specified,\n            **field_vals_select,\n        )\n\n        # Compile masks from databases and from argument into one list.\n        if masks is not None and any(not isinstance(m, Mask) for m in masks):\n            raise Exception(\n                \"Function argument 'masks' must contain a list of \"\n                \"posted.masking.Mask objects.\"\n            )\n        masks = (self._masks if masks_database else []) + (masks or [])\n\n        # Aggregate over fields that should be aggregated.\n        component_fields = [\n            col_id\n            for col_id, field in self._fields.items()\n            if field.field_type == \"component\"\n        ]\n        if agg is None:\n            agg = component_fields + [\"source\"]\n        else:\n            if isinstance(agg, tuple):\n                agg = list(agg)\n            elif not isinstance(agg, list):\n                agg = [agg]\n            for a in agg:\n                if not isinstance(a, str):\n                    raise Exception(\n                        f\"Field ID in argument 'agg' must be a \"\n                        f\"string but found: {a}\"\n                    )\n                if not any(a == col_id for col_id in self._fields):\n                    raise Exception(\n                        f\"Field ID in argument 'agg' is not a valid field: {a}\"\n                    )\n\n        # Aggregate over component fields.\n        group_cols = [\n            c\n            for c in selected.columns\n            if not (c == \"value\" or (c in agg and c in component_fields))\n        ]\n        aggregated = (\n            selected.groupby(group_cols, dropna=False)\n            .agg({\"value\": \"sum\"})\n            .reset_index()\n        )\n\n        # Aggregate over cases fields.\n        group_cols = [\n            c for c in aggregated.columns if not (c == \"value\" or c in agg)\n        ]\n        ret = []\n        for keys, rows in aggregated.groupby(group_cols, dropna=False):\n            # Set default weights to 1.0.\n            rows = rows.assign(weight=1.0)\n\n            # Update weights by applying masks.\n            for mask in masks:\n                if mask.matches(rows):\n                    rows[\"weight\"] *= mask.get_weights(rows)\n\n            # Drop all rows with weights equal to nan.\n            rows.dropna(subset=\"weight\", inplace=True)\n\n            if not rows.empty:\n                # Aggregate with weights.\n                out = rows.groupby(group_cols, dropna=False)[\n                    [\"value\", \"weight\"]\n                ].apply(\n                    lambda cols: pd.Series(\n                        {\n                            \"value\": np.average(\n                                cols[\"value\"],\n                                weights=cols[\"weight\"],\n                            ),\n                        }\n                    )\n                )\n\n                # Add to return list.\n                ret.append(out)\n\n        # If nothing is found, return empty dataframe.\n        if not ret:\n            add_cols = (\n                []\n                if append_references\n                else [\"reference_variable\", \"reference_unit\"]\n            )\n            return pd.DataFrame(\n                columns=group_cols + [\"variable\", \"value\", \"unit\"] + add_cols\n            )\n        aggregated = pd.concat(ret).reset_index()\n\n        # Finalise dataframe and return.\n        return self._finalise(\n            df=aggregated,\n            append_references=append_references,\n            group_cols=group_cols,\n            ref_vars=ref_vars,\n            units=units,\n            with_parent=with_parent,\n        )\n\n    def _finalise(\n        self,\n        df: pd.DataFrame,\n        append_references: bool,\n        group_cols: list[str],\n        ref_vars: dict[str, str],\n        units: dict[str, str],\n        with_parent: bool,\n    ) -&gt; DataFrame:\n        # Append reference variables.\n        if append_references:\n            var_ref_unique = {\n                ref_vars[var]\n                for var in df[\"variable\"].unique()\n                if not pd.isnull(ref_vars[var])\n            }\n            to_append = []\n            for ref_var in var_ref_unique:\n                to_append.append(\n                    pd.DataFrame(\n                        {\n                            \"variable\": [ref_var],\n                            \"value\": [1.0],\n                        }\n                        | {\n                            col_id: [\"*\"]\n                            for col_id, field in self._fields.items()\n                            if col_id in df\n                        }\n                    )\n                )\n\n            if to_append:\n                to_append = pd.concat(to_append, ignore_index=True)\n                for col_id, field in self._fields.items():\n                    if col_id not in df.columns:\n                        continue\n                    to_append = field.select_and_expand(\n                        to_append,\n                        col_id,\n                        df[col_id].unique().tolist(),\n                    )\n                df = (\n                    pd.concat([df, to_append], ignore_index=True)\n                    .sort_values(by=group_cols + [\"variable\"])\n                    .reset_index(drop=True)\n                )\n        else:\n            df[\"reference_variable\"] = (\n                df[\"variable\"].map(ref_vars)\n            )\n            df[\"reference_unit\"] = (\n                df[\"reference_variable\"].map(units)\n            )\n\n        # Insert unit(s).\n        df[\"unit\"] = df[\"variable\"].map(units)\n\n        # Prepend parent variable.\n        if with_parent:\n            if self._parent_variable is None:\n                raise Exception(\n                    \"Can only prepend parent variable if not None.\"\n                )\n            df[\"variable\"] = df[\"variable\"].str.cat(\n                [self._parent_variable], sep=\"|\"\n            )\n\n        # Order columns.\n        df = df[\n            [col for col in self._columns if col in df]\n        ]\n\n        return df\n</code></pre>"},{"location":"api/#posted.TEDF.__init__","title":"<code>__init__(df, parent_variable=None, database_id=None, variables=None, custom_fields=None, custom_comments=None, masks=None)</code>","text":"<p>Initialise parent class and object fields</p> Source code in <code>posted/noslag/tedf.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    parent_variable: str | None = None,\n    database_id: str | None = None,\n    variables: dict | None = None,\n    custom_fields: dict | None = None,\n    custom_comments: dict | None = None,\n    masks: list[Mask] | None = None,\n):\n    \"\"\"Initialise parent class and object fields\"\"\"\n    self._parent_variable: str | None = parent_variable\n    self._database_id: str | None = database_id\n    self._variables: dict[str, dict] = variables or {}\n    self._masks: list[Mask] = masks or []\n    self._validated: pd.DataFrame | None = None\n\n    # Combine all fields.\n    source_column = base_column_src()\n    self._fields: dict[str, AbstractFieldDefinition] = (\n        {\"source\": source_column}\n        | (custom_fields or {})\n    )\n\n    # Combine all comments.\n    self._comments: dict[str, CommentDefinition] = (\n        base_columns_src_detail\n        | {\"comment\": CommentDefinition}\n        | (custom_comments or {})\n    )\n\n    # Combine all columns.\n    self._columns: dict[str, AbstractColumnDefinition] = (\n        {\"source\": source_column}\n        | base_columns_src_detail\n        | (custom_fields or {})\n        | base_columns_other\n    )\n\n    # Deal with unknown columns.\n    unknown_cols = [c for c in df if c not in self._columns]\n    if unknown_cols:\n        i = len(unknown_cols)\n        warn(\n            f\"Unknown column{'s'[:i^1]} treated as comment{'s'[:i^1]}: \"\n            + \", \".join(str(c) for c in unknown_cols),\n            POSTEDWarning,\n        )\n        unknown_cols = {\n            c: CommentDefinition(\n                name=str(c),\n                description=\"\",\n                required=False,\n            )\n            for c in unknown_cols\n        }\n        self._comments |= unknown_cols\n        self._columns |= unknown_cols\n\n    self._df: pd.DataFrame = df[list(self._columns)]\n</code></pre>"},{"location":"api/#posted.TEDF.aggregate","title":"<code>aggregate(units=None, reference_activity=None, reference_capacity=None, drop_singular_fields=True, period_mode=PeriodMode.INTER_AND_EXTRAPOLATION, agg=None, masks=None, masks_database=True, expand_not_specified=True, with_parent=False, append_references=False, **field_vals_select)</code>","text":"<p>Aggregates data based on specified parameters, applies masks, and cleans up the resulting DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>Optional[dict[str, str]]</code> <p>Dictionary with key, value paris of variables to override</p> <code>None</code> <code>reference_activity</code> <code>Optional[str]</code> <p>The activity variable to align all activities on.</p> <code>None</code> <code>reference_capacity</code> <code>Optional[str]</code> <p>The capacity variable to align all capacities on.</p> <code>None</code> <code>drop_singular_fields</code> <code>bool</code> <p>If True, drop custom fields with only one value</p> <code>True</code> <code>extrapolate_period</code> <p>If True, extrapolate values by extrapolation, if no value for this period is given</p> required <code>expand_not_specified</code> <code>bool | list[str]</code> <p>Whether to expand fields with value <code>N/S</code> (not specified) to all allowed values. If <code>True</code> is passed, then allow <code>N/S</code> is expanded for all fields. If a list of strings is passed, then only the contained fields are expanded. If False is passed, then no field is expanded. Default is True.</p> <code>True</code> <code>agg</code> <code>Optional[str | list[str] | tuple[str]]</code> <p>Specifies which fields to aggregate over.</p> <code>None</code> <code>masks</code> <code>Optional[list[Mask]]</code> <p>Specifies a list of Mask objects that will be applied to the data during aggregation. These masks can be used to filter or weight the data based on certain conditions defined in the Mask objects.</p> <code>None</code> <code>masks_database</code> <code>bool</code> <p>Determines whether to include masks from databases in the aggregation process. If set to <code>True</code>, masks from databases will be included along with any masks provided as function arguments. If set to <code>False</code>, only the masks provided as function arguments will be applied.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The <code>aggregate</code> method returns a pandas DataFrame that has been cleaned up and aggregated based on the specified parameters and input data. The method performs aggregation over component fields and cases fields, applies weights based on masks, drops rows with NaN weights, aggregates with weights, inserts reference variables, sorts columns and rows, rounds values, and inserts units before returning the final cleaned and aggregated DataFrame.</p> Source code in <code>posted/noslag/tedf.py</code> <pre><code>def aggregate(\n    self,\n    units: Optional[dict[str, str]] = None,\n    reference_activity: Optional[str] = None,\n    reference_capacity: Optional[str] = None,\n    drop_singular_fields: bool = True,\n    period_mode: PeriodMode | str = PeriodMode.INTER_AND_EXTRAPOLATION,\n    agg: Optional[str | list[str] | tuple[str]] = None,\n    masks: Optional[list[Mask]] = None,\n    masks_database: bool = True,\n    expand_not_specified: bool | list[str] = True,\n    with_parent: bool = False,\n    append_references: bool = False,\n    **field_vals_select,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Aggregates data based on specified parameters, applies masks,\n    and cleans up the resulting DataFrame.\n\n    Parameters\n    ----------\n    units: dict[str, str], optional\n        Dictionary with key, value paris of variables to override\n    reference_activity: str, optional\n        The activity variable to align all activities on.\n    reference_capacity: str, optional\n        The capacity variable to align all capacities on.\n    drop_singular_fields: bool, optional\n        If True, drop custom fields with only one value\n    extrapolate_period: bool, optional\n        If True, extrapolate values by extrapolation, if no value\n        for this period is given\n    expand_not_specified: bool | list[str], optional\n        Whether to expand fields with value `N/S` (not specified) to all\n        allowed values. If `True` is passed, then allow `N/S` is expanded\n        for all fields. If a list of strings is passed, then only the\n        contained fields are expanded. If False is passed, then no field\n        is expanded. Default is True.\n    agg : Optional[str | list[str] | tuple[str]]\n        Specifies which fields to aggregate over.\n    masks : Optional[list[Mask]]\n        Specifies a list of Mask objects that will be applied to the\n        data during aggregation. These masks can be used to filter\n        or weight the data based on certain conditions defined in\n        the Mask objects.\n    masks_database : bool, optional\n        Determines whether to include masks from databases in the\n        aggregation process. If set to `True`, masks from databases\n        will be included along with any masks provided as function\n        arguments. If set to `False`, only the masks provided as\n        function arguments will be applied.\n\n    Returns\n    -------\n    pd.DataFrame\n        The `aggregate` method returns a pandas DataFrame that has\n        been cleaned up and aggregated based on the specified\n        parameters and input data. The method performs aggregation\n        over component fields and cases fields, applies weights\n        based on masks, drops rows with NaN weights, aggregates with\n        weights, inserts reference variables, sorts columns and\n        rows, rounds values, and inserts units before returning the\n        final cleaned and aggregated DataFrame.\n    \"\"\"\n\n    # Run select().\n    selected, units, ref_vars = self._select(\n        units=units,\n        reference_activity=reference_activity,\n        reference_capacity=reference_capacity,\n        period_mode=period_mode,\n        drop_singular_fields=drop_singular_fields,\n        expand_not_specified=expand_not_specified,\n        **field_vals_select,\n    )\n\n    # Compile masks from databases and from argument into one list.\n    if masks is not None and any(not isinstance(m, Mask) for m in masks):\n        raise Exception(\n            \"Function argument 'masks' must contain a list of \"\n            \"posted.masking.Mask objects.\"\n        )\n    masks = (self._masks if masks_database else []) + (masks or [])\n\n    # Aggregate over fields that should be aggregated.\n    component_fields = [\n        col_id\n        for col_id, field in self._fields.items()\n        if field.field_type == \"component\"\n    ]\n    if agg is None:\n        agg = component_fields + [\"source\"]\n    else:\n        if isinstance(agg, tuple):\n            agg = list(agg)\n        elif not isinstance(agg, list):\n            agg = [agg]\n        for a in agg:\n            if not isinstance(a, str):\n                raise Exception(\n                    f\"Field ID in argument 'agg' must be a \"\n                    f\"string but found: {a}\"\n                )\n            if not any(a == col_id for col_id in self._fields):\n                raise Exception(\n                    f\"Field ID in argument 'agg' is not a valid field: {a}\"\n                )\n\n    # Aggregate over component fields.\n    group_cols = [\n        c\n        for c in selected.columns\n        if not (c == \"value\" or (c in agg and c in component_fields))\n    ]\n    aggregated = (\n        selected.groupby(group_cols, dropna=False)\n        .agg({\"value\": \"sum\"})\n        .reset_index()\n    )\n\n    # Aggregate over cases fields.\n    group_cols = [\n        c for c in aggregated.columns if not (c == \"value\" or c in agg)\n    ]\n    ret = []\n    for keys, rows in aggregated.groupby(group_cols, dropna=False):\n        # Set default weights to 1.0.\n        rows = rows.assign(weight=1.0)\n\n        # Update weights by applying masks.\n        for mask in masks:\n            if mask.matches(rows):\n                rows[\"weight\"] *= mask.get_weights(rows)\n\n        # Drop all rows with weights equal to nan.\n        rows.dropna(subset=\"weight\", inplace=True)\n\n        if not rows.empty:\n            # Aggregate with weights.\n            out = rows.groupby(group_cols, dropna=False)[\n                [\"value\", \"weight\"]\n            ].apply(\n                lambda cols: pd.Series(\n                    {\n                        \"value\": np.average(\n                            cols[\"value\"],\n                            weights=cols[\"weight\"],\n                        ),\n                    }\n                )\n            )\n\n            # Add to return list.\n            ret.append(out)\n\n    # If nothing is found, return empty dataframe.\n    if not ret:\n        add_cols = (\n            []\n            if append_references\n            else [\"reference_variable\", \"reference_unit\"]\n        )\n        return pd.DataFrame(\n            columns=group_cols + [\"variable\", \"value\", \"unit\"] + add_cols\n        )\n    aggregated = pd.concat(ret).reset_index()\n\n    # Finalise dataframe and return.\n    return self._finalise(\n        df=aggregated,\n        append_references=append_references,\n        group_cols=group_cols,\n        ref_vars=ref_vars,\n        units=units,\n        with_parent=with_parent,\n    )\n</code></pre>"},{"location":"api/#posted.TEDF.normalise","title":"<code>normalise(units=None, with_parent=False)</code>","text":"<p>Normalise data by converting reference values to 1.0 and converting to default unit for each variable.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>Optional[dict[str, str]]</code> <p>Dictionary with key-value pairs of units to use for variables.</p> <code>None</code> <code>with_parent</code> <code>bool</code> <p>Whether to prepend the parent variable. Default is False.</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing normalised raw data.</p> Source code in <code>posted/noslag/tedf.py</code> <pre><code>def normalise(\n    self, units: Optional[dict[str, str]] = None, with_parent: bool = False\n) -&gt; pd.DataFrame | None:\n    \"\"\"\n    Normalise data by converting reference values to 1.0 and converting to\n    default unit for each variable.\n\n    Parameters\n    ----------\n    units: dict[str,str], optional\n        Dictionary with key-value pairs of units to use for variables.\n    with_parent: bool, optional\n        Whether to prepend the parent variable. Default is False.\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame containing normalised raw data.\n    \"\"\"\n    normalised, units = self._normalise(units)\n\n    # Insert unit, reference value, and reference unit.\n    normalised.insert(\n        normalised.columns.tolist().index(\"uncertainty\") + 1,\n        \"unit\",\n        np.nan,\n    )\n    normalised[\"unit\"] = normalised[\"variable\"].map(units)\n    normalised.insert(\n        normalised.columns.tolist().index(\"unit\") + 1,\n        \"reference_value\",\n        1.0,\n    )\n    normalised.insert(\n        normalised.columns.tolist().index(\"reference_value\") + 1,\n        \"reference_unit\",\n        np.nan,\n    )\n    normalised[\"reference_unit\"] = normalised[\"reference_variable\"].map(\n        units\n    )\n\n    # Prepend parent variable.\n    if with_parent:\n        if self._parent_variable is None:\n            raise Exception(\n                \"Can only prepend parent variable if not None.\"\n            )\n        normalised[\"variable\"] = normalised[\"variable\"].str.cat(\n            [self._parent_variable], sep=\"|\"\n        )\n\n\n    # Order columns.\n    normalised = normalised[\n        [col for col in self._columns if col in normalised]\n    ]\n\n    return normalised\n</code></pre>"},{"location":"api/#posted.TEDF.select","title":"<code>select(units=None, reference_activity=None, reference_capacity=None, drop_singular_fields=True, period_mode=PeriodMode.INTER_AND_EXTRAPOLATION, expand_not_specified=True, with_parent=False, append_references=False, **field_vals_select)</code>","text":"<p>Select desired data from the dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>units</code> <code>Optional[dict[str, str]]</code> <p>Dictionary with key-value pairs of units to use for variables.</p> <code>None</code> <code>reference_activity</code> <code>Optional[str]</code> <p>Reference activity.</p> <code>None</code> <code>reference_capacity</code> <code>Optional[str]</code> <p>Reference capacity.</p> <code>None</code> <code>drop_singular_fields</code> <code>bool</code> <p>If True, drop custom fields with only one value</p> <code>True</code> <code>interpolate_period</code> <p>If True, determine values by interpolation between known points, if no value for a requested period is given. Default is True.</p> required <code>extrapolate_period</code> <p>If True, determine values by extrapolation outside of range of known data, if no value for a requested period is given. Default is False.</p> required <code>expand_not_specified</code> <code>bool | list[str]</code> <p>Whether to expand fields with value <code>N/S</code> (not specified) to all allowed values. If <code>True</code> is passed, then allow <code>N/S</code> is expanded for all fields. If a list of strings is passed, then only the contained fields are expanded. If False is passed, then no field is expanded. Default is True.</p> <code>True</code> <code>with_parent</code> <code>bool</code> <p>Whether to prepend the parent variable. Default is False.</p> <code>False</code> <code>**field_vals_select</code> <p>IDs of values to select</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with selected Values</p> Source code in <code>posted/noslag/tedf.py</code> <pre><code>def select(\n    self,\n    units: Optional[dict[str, str]] = None,\n    reference_activity: Optional[str] = None,\n    reference_capacity: Optional[str] = None,\n    drop_singular_fields: bool = True,\n    period_mode: str | PeriodMode = PeriodMode.INTER_AND_EXTRAPOLATION,\n    expand_not_specified: bool | list[str] = True,\n    with_parent: bool = False,\n    append_references: bool = False,\n    **field_vals_select,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Select desired data from the dataframe.\n\n    Parameters\n    ----------\n    units: dict[str,str], optional\n        Dictionary with key-value pairs of units to use for variables.\n    reference_activity: str, optional\n        Reference activity.\n    reference_capacity: str, optional\n        Reference capacity.\n    drop_singular_fields: bool, optional\n        If True, drop custom fields with only one value\n    interpolate_period: bool, optional\n        If True, determine values by interpolation between known points,\n        if no value for a requested period is given. Default is True.\n    extrapolate_period: bool, optional\n        If True, determine values by extrapolation outside of range of\n        known data, if no value for a requested period is given. Default\n        is False.\n    expand_not_specified: bool | list[str], optional\n        Whether to expand fields with value `N/S` (not specified) to all\n        allowed values. If `True` is passed, then allow `N/S` is expanded\n        for all fields. If a list of strings is passed, then only the\n        contained fields are expanded. If False is passed, then no field\n        is expanded. Default is True.\n    with_parent: bool, optional\n        Whether to prepend the parent variable. Default is False.\n    **field_vals_select\n        IDs of values to select\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with selected Values\n    \"\"\"\n    selected, units, ref_vars = self._select(\n        units=units,\n        reference_activity=reference_activity,\n        reference_capacity=reference_capacity,\n        drop_singular_fields=drop_singular_fields,\n        period_mode=period_mode,\n        expand_not_specified=expand_not_specified,\n        **field_vals_select,\n    )\n\n    # Finalise dataframe and return.\n    return self._finalise(\n        df=selected,\n        append_references=append_references,\n        group_cols=[c for c in self._fields if c in selected],\n        ref_vars=ref_vars,\n        units=units,\n        with_parent=with_parent,\n    )\n</code></pre>"},{"location":"data/","title":"Data","text":"<p>Each TEDF will be made available here for quick inspection. For full detail, please consult the repository on GitHub or install and run the Python package.</p> <ul> <li>Direct Reduction of Iron</li> <li>Electrolysis</li> <li>Electric Arc Furnace</li> </ul> <p>Note: This area is still under development. More datasets will follow soon.</p>"},{"location":"data/Direct%20Reduction%20of%20Iron/","title":"Direct Reduction of Iron","text":"<p>The techno-economic data is distinguished across the following additional fields.</p> <ul> <li>H2: Operate fully with hydrogen.</li> <li>NG: Operate fully with natural gas.</li> </ul> <p>All data added to the POSTED database is aggregated automatically using the POSTED framework. The result yields the following parameters:</p>      Loading ITables v2.6.2 from the internet...     (need help?) <p>The figure below gives an overview of the energy demand reported by different sources across different energy carriers.</p> <p>The table below contains the raw data contained in the public POSTED database. This data has not be automatically normalised or harmonised in any way. You can also find this data in the GitHub repo in this file: posted/database/tedfs/Tech/Direct Reduction of Iron.csv</p>      Loading ITables v2.6.2 from the internet...     (need help?)"},{"location":"data/Direct%20Reduction%20of%20Iron/#direct-reduction-of-iron","title":"Direct Reduction of Iron\u00b6","text":""},{"location":"data/Direct%20Reduction%20of%20Iron/#fields","title":"Fields\u00b6","text":""},{"location":"data/Direct%20Reduction%20of%20Iron/#operation-mode-mode","title":"Operation mode (<code>mode</code>)\u00b6","text":""},{"location":"data/Direct%20Reduction%20of%20Iron/#aggregated-parameters","title":"Aggregated parameters\u00b6","text":""},{"location":"data/Direct%20Reduction%20of%20Iron/#energy-demand","title":"Energy demand\u00b6","text":""},{"location":"data/Direct%20Reduction%20of%20Iron/#raw-data","title":"Raw data\u00b6","text":""},{"location":"data/Electric%20Arc%20Furnace/","title":"Electric Arc Furnace","text":"<p>The techno-economic data is distinguished across the following additional fields.</p> <ul> <li>Primary: Operate with direct reduced iron (primary route).</li> <li>Secondary: Operate with scrap (secondary route).</li> </ul> <ul> <li>w/ reheating: Working with cold direct reduced iron that needs reheating.</li> <li>w/o reheating: Working with hot direct reduced iron that needs no reheating.</li> </ul> <p>All data added to the POSTED database is aggregated automatically using the POSTED framework. The result yields the following parameters:</p>      Loading ITables v2.6.2 from the internet...     (need help?) <p>The figure below gives an overview of the energy demand reported by different sources across different energy carriers.</p> <p>The table below contains the raw data contained in the public POSTED database. This data has not be automatically normalised or harmonised in any way. You can also find this data in the GitHub repo in this file: posted/database/tedfs/Tech/Electric Arc Furnace.csv</p>      Loading ITables v2.6.2 from the internet...     (need help?)"},{"location":"data/Electric%20Arc%20Furnace/#electric-arc-furnace","title":"Electric Arc Furnace\u00b6","text":""},{"location":"data/Electric%20Arc%20Furnace/#fields","title":"Fields\u00b6","text":""},{"location":"data/Electric%20Arc%20Furnace/#operation-mode-mode","title":"Operation Mode (<code>mode</code>)\u00b6","text":""},{"location":"data/Electric%20Arc%20Furnace/#reheating-reheating","title":"Reheating (<code>reheating</code>)\u00b6","text":""},{"location":"data/Electric%20Arc%20Furnace/#aggregated-parameters","title":"Aggregated parameters\u00b6","text":""},{"location":"data/Electric%20Arc%20Furnace/#energy-demand","title":"Energy demand\u00b6","text":""},{"location":"data/Electric%20Arc%20Furnace/#raw-data","title":"Raw data\u00b6","text":""},{"location":"data/Electrolysis/","title":"Electrolysis","text":"<p>The techno-economic data is distinguished across the following additional fields.</p> <ul> <li>AEL: Alkaline electrolysis</li> <li>PEM: Proton-exchange membrane electrolysis</li> <li>SOEC: High-temperature solid-oxide electrolysis</li> </ul> <p>Mainly CAPEX but also other parameters crucially depend on the size of an electrolysis plant. The size can be added as a non-coded field. For some sources, this may not be reported and hence set to <code>N/S</code> (not specified).</p> <p>All data added to the POSTED database is aggregated automatically using the POSTED framework. The result yields the following parameters:</p>      Loading ITables v2.6.2 from the internet...     (need help?) <p>The figure below gives an overview of CAPEX values reported by different sources across times, subtechnology (Alkaline, PEM, Solid Oxide), and plant size.</p> <p>The table below contains the raw data contained in the public POSTED database. This data has not be automatically normalised or harmonised in any way. You can also find this data in the GitHub repo in this file: posted/database/tedfs/Tech/Electrolysis.csv</p>      Loading ITables v2.6.2 from the internet...     (need help?)"},{"location":"data/Electrolysis/#electrolysis","title":"Electrolysis\u00b6","text":""},{"location":"data/Electrolysis/#fields","title":"Fields\u00b6","text":""},{"location":"data/Electrolysis/#subtechnologies","title":"Subtechnologies\u00b6","text":""},{"location":"data/Electrolysis/#size","title":"Size\u00b6","text":""},{"location":"data/Electrolysis/#aggregated-parameters","title":"Aggregated parameters\u00b6","text":""},{"location":"data/Electrolysis/#capex","title":"CAPEX\u00b6","text":""},{"location":"data/Electrolysis/#electricity-demand","title":"Electricity demand\u00b6","text":""},{"location":"data/Electrolysis/#raw-data","title":"Raw data\u00b6","text":""},{"location":"guide/contribute/","title":"How to contribute to POSTED","text":"<p>Contributions to the data curated in POSTED and to the functionalities implemented in its code are very much encouraged and greatly appreciated.</p>"},{"location":"guide/contribute/#by-creating-a-pull-request-on-github","title":"By creating a pull request on GitHub","text":"<p>POSTED \u2014 including its public database and the source code of its underlying Python framework \u2014 is managed on a public GitHub repository.</p> <p>You can contribute your data directly by: * Forking POSTED on GitHub * Clone your fork and make local edits * Commit your changes to your forked repository * Create a pull request here</p> <p>Generic information on creating pull requests can be found here.</p> <p>This method has many advantages, e.g. automated testing procedures, discussing changes through the GitHub interface, full public transparency, etc.</p>"},{"location":"guide/contribute/#by-sending-a-data-file-to-the-maintainers","title":"By sending a data file to the maintainers","text":"<p>GitHub pull requests may be a barrier for contribution to some users. We therefore also encourage users to send us their raw data directly.</p> <p>Data files can be sent directly via email to <code>philipp DOT verpoort AT pik MINUS potsdam DOT de</code> (email address obfuscated to protect against web crawlers). Files can be attached directly to emails (if less than 10 MB in size) or uploaded to an appropriate file-sharing service. Files may be shared as comma-separated value (CSV) ASCII files, as XLSX files (typically created using Microsoft\u00ae Excel\u00ae or compatible software), or ODS files (typically created using LibreOffice\u2122, Apache OpenOffice\u2122, or other compatible software).</p> <p>Contributors are encouraged to try to use the correct format when providing their data. However, the data does not have to be in the perfect and final format yet when sharing. The authors of POSTED are happy to help convert data to the correct format.</p>"},{"location":"guide/format/","title":"Format","text":""},{"location":"guide/format/#databases","title":"Databases","text":"<p>POSTED is extensible via public and private databases. The public database is part of the public GitHub repository (in the <code>posted/database</code> directory). Private project-specific databases can be added to POSTED by adding a respective database path to the <code>databases</code> dictionary of the path module before executing any other POSTED code.</p> <pre><code>from pathlib import Path\nfrom posted import databases\n\ndatabases |= {\"my_private_database\": Path(\"/path/to/database/\")}\n</code></pre> <p>The public database is intended for the curation of a comprehensive set of general-purpose resources that should suit the needs of most users. Private databases may be used for low-threshold extensibility, for more project-specific work that is not in the interest of a broad audience, or for confidential data that cannot be made available publicly.</p> <p>Each database may contain the following components, which should each be contained in a directory.</p> <ul> <li><code>tedfs/</code>: This directory contains TEDFs (techno-economic data files), which are UTF8-encoded CSV files following a specific format (see below). The files are organised in a directory structure following a hierarchical system. Moreover, each TEDF file is accompanied by a metadata file in YAML format outlining the column structure (fields and comments, see below) and the allowed variables (see below). Both column and variable definitions may be defined explicitly or may refer to predefined sets.</li> <li><code>variables/definitions/</code>: This directory may contain variable definitions in YAML format that can be referred to by TEDF metadata files (see above).</li> <li><code>variables/mappings/</code>: This directory defines automated variable mappings as Python code.</li> <li><code>masks/</code>: This directory contains masks, which add weights to different rows when aggregating the data using <code>TEDF.aggregate()</code>. It will make sense to define these manually for each case, but some masks useful generally will be stored here as part of the database.</li> </ul>"},{"location":"guide/format/#base-column-format","title":"Base column format","text":"<p>The TEDF base format contains the following columns:</p> <ul> <li><code>variable</code> \u2014 The reported variable, as defined in the metadata or a predefined set under <code>variables/definitions/</code>.</li> <li><code>reference_variable</code> \u2014 The reference variable. This is only used for</li> <li><code>value</code> \u2014 The value corresponding to the reported variable.</li> <li><code>uncertainty</code> \u2014 The uncertainty corresponding to the reported variable.</li> <li><code>unit</code> \u2014 The unit corresponding to the reported variable. This must be a valid unit string known to <code>cet-units</code>.</li> <li><code>reference_value</code> \u2014 The value corresponding to the reference value. This can only be entered if <code>reference_variable</code> is set.</li> <li><code>reference_unit</code> \u2014 The unit corresponding to the reference variable. This can only be entered if <code>reference_variable</code> is set. This must be a valid unit string known to <code>cet-units</code>.</li> <li><code>comment</code> \u2014 A free-text comment on the entry. This column should especially be used to report any conversion performed if the value originally reported by a source had to be modified manually before reporting. It should also be used to report additional assumptions or deviating from original units.</li> <li><code>source</code> \u2014 A valid BibTeX identifier from the <code>sources.bib</code> file.</li> <li><code>source_detail</code> \u2014 Detailed information on where exactly in the source this entry can be found.</li> </ul> <p>Note: The <code>uncertainty</code> column can currently only be reported but not processed further with the POSTED framework. This feature may be added at a later stage.</p>"},{"location":"guide/format/#fields","title":"Fields","text":"<p>Fields are additional columns that can help report data that varies across cases or components. Fields can currently be of type <code>case</code> or <code>component</code>.</p> <ul> <li>A case field: POSTED treats these columns as different competing options for data. When data is aggregated in the select method of the NO-SL-AG framework, case fields are averaged over.</li> <li>A component field: POSTED treats these columns as components of the same data that require summing up. When data is aggregated in the select method of the NO-SL-AG framework, component fields are simply summed up.</li> </ul>"},{"location":"guide/overview/","title":"User Guide","text":""},{"location":"guide/overview/#purpose","title":"Purpose","text":"<p>POSTED is aimed at researchers who want to perform techno-economic assessments or modelling exercises in the context of the energy transition, industrial ecology, climate mitigation, and related fields.</p> <p>Such work often relies on techno-economic parameters, i.e. a combination of technological parameters (concerning the physical performance of a technology) and economic parameters (concerning its cost of investment and operation). Other important parameters may include market prices, technology lifetimes, or emissions intensities.</p> <p>Such data is often compartmentalised and scattered across many sources (academic literature, grey literature, etc). Collecting this data for analysis, transparently tracing its origin, and consistently harmonising it is a cumbersome and tedious endeavour.</p> <p>Therefore, the POSTED database and framework aims to achieve the following.</p> <ul> <li>Curate a comprehensive collection of techno-economic data to alleviate researchers from performing this task manually for each analysis.</li> <li>Store this data in a machine-readable format to ease interoperability.</li> <li>Transparently report where original data can be found to make research and its assumptions understandable and reproducible.</li> <li>Simplify the conversion of raw data into specific formats to ease further processing in other tools.</li> <li>Avoid misunderstandings when storing and reading data through the development and explication of clear standards.</li> <li>Be extendable to meet a variety of requirements by different users groups.</li> </ul> <p>In particular, POSTED aspires to comply with the FAIR principles of research data.</p>"},{"location":"guide/overview/#workflow","title":"Workflow","text":"<p>A typical workflow looks like this:</p> <ol> <li>Find techno-economic data in a original source, e.g. an academic journal article or a technical report.</li> <li>Add this data to the POSTED database in its original format (unit, reference, etc). This is done by editing the <code>*.tedf</code> (techno-economic data file) files, which are simple UTF8-encoded comma-separated value (CSV) files.</li> <li>Use the POSTED framework to harmonise this data alongside other data (common units, reference, etc), using the <code>TEDF.normalise()</code> function.</li> <li>Select the specific case and/or component of the data required, e.g. a specific year, subtechnology, etc, using the <code>TEDF.select()</code> function.</li> <li>Aggregate data over multiple sources, cases, or components using the <code>TEDF.aggregate()</code> function.</li> <li>Use the resulting harmonised data as input for some further processing in other tools.</li> </ol>"}]}