{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to POSTED's documentation","text":"<p>POSTED (the Potsdam Open-Source Techno-Economic Database) is a public database of techno-economic data on energy and climate-mitigation technologies, along with a framework for consistent data handling and an open-source toolbox for techno-economic assessments (TEA). In particular, it provides a structure for and contains data on investment cost, energy and feedstock demand, other fixed and variable costs, emissions intensities, and other characteristics of conversion, storage, and transportation technologies in the energy and related sectors. The accompanying software code is intended for consistent maintenance of this data and for deriving straight-forward results from them, such as levelised cost, greenhouse-gas emission intensities, or marginal abatement cost.</p>"},{"location":"installation/","title":"How to work with and contribute to POSTED","text":"<p>If you want to use POSTED or even contribute to its development, you first need to get/install POSTED in one of two ways:</p> <ul> <li>By installing the <code>posted</code> package as a dependency.</li> <li>By cloning the git repository on GitHub.</li> </ul>"},{"location":"installation/#installing-posted-as-a-package","title":"Installing POSTED as a package","text":"PythonR <p>You can install the <code>posted</code> Python package via: <pre><code># when using poetry\npoetry add git+https://github.com:PhilippVerpoort/posted.git\n\n# when using pip\npip install git+https://github.com:PhilippVerpoort/posted.git\n</code></pre> A PyPI package will be made available at a later stage.</p> <p>You can install the <code>posted</code> R package using <code>install_github</code> from <code>devtools</code> via: <pre><code>install_github('PhilippVerpoort/posted')\n</code></pre> A CRAN package will be made available at a later stage.</p> <p>This will allow you to use the data contained in POSTED's public database and general-purpose functions from the NOSLAG and TEAM frameworks.</p>"},{"location":"installation/#cloning-posted-from-source","title":"Cloning POSTED from source","text":"<ul> <li>The POSTED source code and public database are available on GitHub.</li> <li>Please submit any questions as issues and changes/additions as pull requests.</li> </ul>"},{"location":"methodology/","title":"Methodology of POSTED","text":""},{"location":"methodology/#purpose-and-aims","title":"Purpose and aims","text":"<p>The development of the POSTED framework pursues several goals:</p> <p>Obtain a comprehensive collection of techno-economic data. The data needed for techno-economic assessments and various types of modelling is often scattered across many sources and formats. One aim of POSTED is to collect all required data in one place with a consistent format. This data will forever be publicly available under a permissive licence in order to overcome existing barriers of collaboration.</p> <p>Make data easily available for manipulation and techno-economic assessments. Techno-economic data often comes in the form of Excel Spreadsheets, which is difficult to work with when performing assessments. Calculating the levelized cost of production or comparing parameters across different sources should be easy and straightforward with a few lines of code.</p> <p>Make data sources traceable and transparent. When working with techno-economic data, the origin and underlying assumptions are often intransparent and hard to trace back. By being explicit about sources and reporting data only in the original units, the process of data curation becomes more open and transparent. Through the development and explication of clear standards, the misunderstandings can be avoided. Be extendible to meet users\u2019 requirements. The POSTED database can be extended, allowing users\u2019 to meet the requirements of their own projects.</p>"},{"location":"methodology/#database-format","title":"Database format","text":"<p>POSTED is extensible via public and private databases. The public database is part of the public GitHub repository and is located in the <code>inst/extdata</code> subdirectory. Private project-specific databases can be added to POSTED by adding a respective database path to the <code>databases</code> dictionary of the path module before executing any other POSTED code.</p> PythonR <pre><code>from pathlib import Path\nimport posted.path\ndatabases |= {'database_name': Path('/path/to/database/')}\n</code></pre> <pre><code>library(POSTED)\ndatabases$database_name &lt;- \"/path/to/database/\"\n</code></pre> <p>The public database is intended for the curation of a comprehensive set of general-purpose resources that should suit the needs of most users. Private databases may be used for low-threshold extensibility, for more project-specific work that is not in the interest of a broad audience, or for confidential data that cannot be made available publicly.</p> <p>The format mandates the following components for all databases. If these components have contents, they should be placed as subdirectories in the database directory (see here: https://github.com/PhilippVerpoort/posted/tree/refactoring/inst/extdata/database).   </p> <ul> <li>Variable definitions (in <code>definitions/</code>). In this directory, variables (and later perhaps regions) are defined in a scheme similar to that used by the IAMC for creating reporting templates for IAMs (see https://github.com/IAMconsortium/common-definitions/).</li> <li>Techno-economic data files (in <code>tedfs/</code>). This directory contains the actual data as CSV files in the techno-economic data file (TEDF) format and are organised following the hierarchical system of variable definitions (see below). Each file follows the TEDF format with columns either in the base format or specified as a field.</li> <li>Fields (in <code>fields/</code>). This directory contains additional fields (i.e. columns) for the various variables. TEDFs may only report additional columns that are either part of the base format or defined as a field. The fields are given as YAML files and are organised following the hierarchical system of variable definitions (see below).</li> <li>Masks (in <code>masks/</code>). This directory contains masks, which help with complex selections of data in the NPS framework and allow for default settings of selecting or dropping certain data entries (e.g. false entries that should always be disregarded and are only kept for reference).</li> </ul> <p>TEDFs, fields, and masks are organised in a hierarchical system of variable definitions. This means that the file <code>.../database/tedfs/Tech/Electrolysis.csv</code> defines entries for variables <code>Tech|Electrolysis|...</code>, and so on. The columns <code>variable</code> and <code>reference_variable</code> in the TEDFs are attached to the end of the parent variable defined by the path of the file.</p>"},{"location":"methodology/#flow-types","title":"Flow types","text":"<p>POSTED defines flow types, which are used throughout the TEDF format and NOSLAG and unit framework. Flow types may be energy carriers (electricity, heat, fossil gas, hydrogen, etc), feedstocks (naphtha, ethylene, carbon-dioxide), or materials (steel, cement, etc).</p> <p>They are defined in the <code>flow_types.csv</code> file in each database. Flow types may be overridden by other databases in the order in which the databases are added to POSTED (i.e. private databases will normally override the public database). Flow types are also automatically loaded as tags for the variable definitions.</p> <p>Flow types come with a unique ID, the so-called <code>flow_id</code>, which is used throughout POSTED (<code>Electricity</code>, <code>Hydrogen</code>, <code>Ammonia</code>, etc). Moreover, the following attributes may be defined for them as attributes:</p> <ul> <li>Name (mandatory): Just a longer explanation of the flow type.</li> <li>Default unit (mandatory): A default unit used for default unit conversion. For instance, while natural gas can be expressed in units of mass, volume, or energy, POSTED will default expressing it in units of energy according to its lower-heating value.</li> <li>Conversion factors (optional): These are used for converting units of the flow between different dimensionalities (e.g. between mass, volume, and energy) and variants (lower or higher heating value; norm or standard density). For instance, 1 kg of ammonia can be converted to 1 MWh according to its lower-heating value with the conversion factor of <code>energycontent_LHV=18.90 MJ/kg</code>. The available conversion factors are the energy content (for converting mass to density and back) for the lower and higher heating values (LHV and HHV), and the density (for converting volume to mass and back) for standard and normal temperature and pressure (STP and NTP).</li> </ul> <p>Attributes can be assigned a source by adding the respective BibTeX handle (see below) in the <code>source</code> column.</p>"},{"location":"methodology/#technology-types","title":"Technology types","text":"<p>POSTED defines technology types, which are used throughout the TEDF format and the NOSLAG framework. Technology types should represent generic classes of technologies (electrolysis, electric-arc furnaces, direct-air capture, etc).</p> <p>Technologies are defined in the <code>tech_types.csv</code> file in each database. Technology types may be overridden by other databases in the order in which the databases are added to POSTED (i.e. private databases will normally override the public database). Technology types are also automatically loaded as tags for the variable definitions.</p> <p>Technology types come with a unique ID, the so-called <code>tech_id</code>, which is used throughout POSTED (<code>Electrolysis</code> for water electrolysis, <code>Haber-Bosch with ASU</code> for Haber-Bosch synthesis with an air-separation unit, <code>Iron Direct Reduction</code> for direct reduction of iron ore based on either fossil gas or hydrogen, etc). Moreover, the following attributes may be defined for them in separate columns:</p> <ul> <li>Description (mandatory): Just a longer explanation of the tech type.</li> <li>Long description (mandatory): An even longer explanation.</li> <li>Class: This may take the value conversion, storage, or transportation and helps distinguish between broad classes of technologies serving these different purposes.</li> <li>Sector: This may help group technologies according to different sectors (energy, industry, transport, buildings, CTS, CDR, etc).</li> <li>Primary output (mandatory): The primary output flow that should be used as default to harmonise all data.</li> </ul>"},{"location":"methodology/#sources","title":"Sources","text":"<ul> <li>Sources must be added to the respective databases in the <code>sources.bib</code> file in BibTeX format\u00a0(see\u00a0https://github.com/PhilippVerpoort/posted/blob/refactoring/inst/extdata/database/sources.bib).</li> <li>The BibTeX identifier should be used in the <code>source</code> column in each TEDF.</li> <li>Only one source can belong to a row of data in a TEDF! Adding multiple sources for one entry is invalid. These should be reported on separate rows.</li> </ul>"},{"location":"methodology/#techno-economic-data-files-tedfs","title":"Techno-economic data files (TEDFs)","text":""},{"location":"methodology/#base-format","title":"Base format","text":"<p>The TEDF base format contains the following columns:</p> <ul> <li>variable \u2014 The reported variable. This column should only contain the trailing part of the variable, whereas the parent variable is given by the file path in the database (see database format from above). The combined variable (parent + reported) must be defined in the definitions.</li> <li>reference_variable \u2014 The reference variable. This column should only contain the trailing part of the variable, whereas the parent variable is given by the file path in the database (see database format from above). The combined variable (parent + reported) must be defined in the definitions.</li> <li>region \u2014 The region, e.g. a country or supranational region. Currently this data is disregarded by POSTED.</li> <li>period \u2014 The period, e.g. an integer or floating number.</li> <li>value \u2014 The value corresponding to the reported variable.</li> <li>uncertainty \u2014 The uncertainty corresponding to the reported variable.</li> <li>unit \u2014 The unit corresponding to the reported variable.</li> <li>reference_value \u2014 The value corresponding to the reference value.</li> <li>reference_unit \u2014 The unit corresponding to the reference variable.</li> <li>comment \u2014 A free-text comment on the entry.</li> <li>source \u2014 A valid BibTeX identifier from the <code>sources.bib</code> file(s).</li> <li>source_detail \u2014 Detailed information on where exactly in the source this entry can be found.</li> </ul> PythonR <p>The base columns in Python are defined here.</p> <p>The base columns in R are defined here.</p> <p>Columns that are not found in a CSV file will be added by POSTED and set to the default value of the column type.</p> <p>If one wants to specify additional columns, these need to be defined as fields in one of the databases.</p> <p>By placing an asterisk (*) in a period, source, or any field column, POSTED expands these rows across all possible values for these columns in the harmonise method of the NHS framework.</p>"},{"location":"methodology/#fields","title":"Fields","text":"<p>Fields can create additional columns for specific variables. Fields can currently be one of three:</p> <ul> <li>A case field: POSTED treats these columns as different competing options for data. When data is aggregated in the select method of the NHS framework, case fields are averaged over.</li> <li>A component field: POSTED treats these columns as components of the same data that require summing up. When data is aggregated in the select method of the NHS framework, component fields are simply added up.</li> </ul>"},{"location":"methodology/#masks","title":"Masks","text":"<p>To be written.</p>"},{"location":"methodology/#variable-definitions","title":"Variable definitions","text":"<p>To be written.</p> <p>See IAMC format: https://github.com/IAMconsortium/common-definitions</p>"},{"location":"methodology/#units","title":"Units","text":"<p>To be written.</p> <p>See pint: https://pint.readthedocs.io/en/stable/ </p> <p>See IAMC units: https://github.com/IAMconsortium/units/ </p>"},{"location":"methodology/#the-normalise-select-aggregate-noslag-framework","title":"The Normalise-Select-Aggregate (NOSLAG) framework","text":"<p>To be written.</p>"},{"location":"methodology/#the-techno-economic-assessment-and-manipulation-team-framework","title":"The Techno-economic Assessment and Manipulation (TEAM) framework","text":""},{"location":"methodology/#levelised-cost-of-x-lcox","title":"Levelised Cost of X (LCOX)","text":"<p>The levelised cost of activity X can be calculated via POSTED based on the following convention: $$ \\mathrm{LCOX} = \\frac{\\mathrm{Capital~Cost} + \\mathrm{OM~Fixed} + \\mathrm{OM~Variable} + \\sum_f \\mathrm{Input~Cost}_f - \\sum_f \\mathrm{Output~Revenue}_f}{\\mathrm{Activity}_X} $$</p> <p>This is based on the following cost components:</p> \\[ \\mathrm{Capital~Cost} = \\frac{\\mathrm{ANF} \\times \\mathrm{CAPEX}}{\\mathrm{OCF} \\times \\mathrm{Reference~Capacity}} \\times \\mathrm{Reference~Flow} \\] \\[ \\mathrm{OM~Fixed} = \\frac{\\mathrm{OPEX~Fixed}}{\\mathrm{OCF} \\times \\mathrm{Reference~Capacity}} \\times \\mathrm{Reference~Flow} \\] \\[ \\mathrm{OM~Variable} = \\mathrm{OPEX~Variable} \\] \\[ \\mathrm{Input~Cost}_f = \\mathrm{Price}_f \\times \\mathrm{Input}_f \\] \\[ \\mathrm{Output~Revenue}_f = \\mathrm{Price}_f \\times \\mathrm{Output}_f \\] <p>with \\(\\mathrm{ANF} = \\frac{\\mathrm{IR} * (1 + \\mathrm{IR})^\\mathrm{BL} / ((1 + \\mathrm{IR})^\\mathrm{BL} - 1)}{\\mathrm{yr}}\\) based on the <code>Interest Rate</code> (IR) and <code>Book Lifetime</code> (BL). The \\(\\mathrm{Reference~Capacity}\\) is the capacity that the <code>CAPEX</code> and <code>OPEX Fixed</code> variables are defined in reference to (e.g. <code>Input Capacity|Electricity</code> or <code>Output Capacity|Methanol</code>), and the \\(\\mathrm{Reference~Flow}\\) is the associated flow. Moreover, \\(\\mathrm{Activity}_X\\) is one of <code>Output|X</code> (with <code>X</code> being <code>Hydrogen</code>, <code>Methanol</code>, <code>Ammonia</code>, etc), <code>Input|X</code> (with <code>X</code> being e.g. <code>Waste</code>), or <code>Service|X</code> (with <code>X</code> being e.g. <code>Passenger Kilometers</code>).</p>"},{"location":"methodology/#process-chains","title":"Process chains","text":"<p>Process chains, i.e. a combination of processes that feed inputs and outputs into each other, can be define in POSTED before performing LCOX analysis.</p> <p>For a process chain consisting of processes \\(P = \\{p_1, p_2, \\ldots\\}\\) we can define feeds \\(C^\\mathrm{Flow}_{p_1\\rightarrow p_2}\\) for <code>Flow</code> being fed from process \\(p_1\\) to process \\(p_2\\). Moreover, we can define demand \\(\\mathrm{Demand|Flow}_{p_1}\\) for the <code>Flow</code> demanded from process \\(p_1\\). This results in the following linear equation for functional process units \\(\\mathrm{Functional~Unit}_{p_1}\\):</p> \\[ \\mathrm{Output|Flow}_{p_1} \\times \\mathrm{Functional~Unit}_{p_1} = \\sum_{p_2} \\mathrm{Input|Flow}_{p_2} \\times \\mathrm{Functional~Unit}_{p_2} \\times C^\\mathrm{Flow}_{p_1\\rightarrow p_2} + \\mathrm{Demand|Flow}_{p_1} \\]"},{"location":"code/R/functions/AbstractColumnDefinition/","title":"AbstractColumnDefinition","text":""},{"location":"code/R/functions/AbstractColumnDefinition/#abstractcolumndefinition","title":"<code>AbstractColumnDefinition</code>","text":""},{"location":"code/R/functions/AbstractColumnDefinition/#description","title":"Description","text":"<p>Abstract class to store columns</p>"},{"location":"code/R/functions/AbstractColumnDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/AbstractColumnDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>AbstractColumnDefinition$new()</code></li> <li><code>AbstractColumnDefinition$is_allowed()</code></li> <li><code>AbstractColumnDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/AbstractColumnDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the AbstractColumnDefinition class</p> <p>Usage</p> <pre><code>AbstractColumnDefinition$new(col_type, name, description, dtype, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>col_type</code> (<code>data.frame</code>)\\cr Type of the column.</li> <li><code>name</code> (<code>character(1)</code>)\\cr Name of the column.</li> <li><code>description</code> (<code>character(1)</code>)\\cr Description of the column.</li> <li><code>dtype</code> (\\verb{Data type})\\cr Data type of the column.</li> <li><code>required</code> (<code>Logical</code>)\\cr Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/functions/AbstractColumnDefinition/#method-is_allowed","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>AbstractColumnDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/functions/AbstractColumnDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>AbstractColumnDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/AbstractFieldDefinition/","title":"AbstractFieldDefinition","text":""},{"location":"code/R/functions/AbstractFieldDefinition/#abstractfielddefinition","title":"<code>AbstractFieldDefinition</code>","text":""},{"location":"code/R/functions/AbstractFieldDefinition/#description","title":"Description","text":"<p>Abstract class to store fields</p>"},{"location":"code/R/functions/AbstractFieldDefinition/#examples","title":"Examples","text":"<pre><code>### ------------------------------------------------\n### Method `AbstractFieldDefinition$select_and_expand`\n### ------------------------------------------------\n\n## Example usage:\n## select_and_expand(df, \"col_id\", field_vals = NULL)\n</code></pre>"},{"location":"code/R/functions/AbstractFieldDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/AbstractFieldDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>AbstractFieldDefinition$new()</code></li> <li><code>AbstractFieldDefinition$is_allowed()</code></li> <li><code>AbstractFieldDefinition$select_and_expand()</code></li> <li><code>AbstractFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/AbstractFieldDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the AbstractFieldDefinition Class</p> <p>Usage</p> <pre><code>AbstractFieldDefinition$new(\n  field_type,\n  name,\n  description,\n  dtype,\n  coded,\n  codes = NULL\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>field_type</code> Type of the field</li> <li><code>name</code> Name of the field</li> <li><code>description</code> Description of the field</li> <li><code>dtype</code> Data type of the field</li> <li><code>coded</code> If the field is coded</li> <li><code>codes</code> Optional codes for the field (default: NULL)</li> </ul> <p></p>"},{"location":"code/R/functions/AbstractFieldDefinition/#method-is_allowed","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>AbstractFieldDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/functions/AbstractFieldDefinition/#method-select_and_expand","title":"Method <code>select_and_expand()</code>","text":"<p>Select and expand fields which are valid for multiple periods or other field vals</p> <p>Usage</p> <pre><code>AbstractFieldDefinition$select_and_expand(df, col_id, field_vals = NA, ...)\n</code></pre> <p>Arguments:</p> <ul> <li><code>df</code> DataFrame where fields should be selected and expanded</li> <li><code>col_id</code> col_id of the column to be selected and expanded</li> <li><code>field_vals</code> NULL or list of field_vals to select and expand</li> <li><code>...</code> Additional keyword arguments</li> </ul> <p>Example:</p> <pre><code>## Example usage:\n## select_and_expand(df, \"col_id\", field_vals = NULL)\n</code></pre> <p>Returns:</p> <p>DataFrame where fields are selected and expanded</p> <p></p>"},{"location":"code/R/functions/AbstractFieldDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>AbstractFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/CommentDefinition/","title":"CommentDefinition","text":""},{"location":"code/R/functions/CommentDefinition/#commentdefinition","title":"<code>CommentDefinition</code>","text":""},{"location":"code/R/functions/CommentDefinition/#description","title":"Description","text":"<p>Class to store comment columns</p>"},{"location":"code/R/functions/CommentDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/CommentDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>CommentDefinition$new()</code></li> <li><code>CommentDefinition$is_allowed()</code></li> <li><code>CommentDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/CommentDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the CommentDefinition Class</p> <p>Usage</p> <pre><code>CommentDefinition$new(name, description, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the column.</li> <li><code>description</code> Character. Description of the column.</li> <li><code>required</code> Logical. Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/functions/CommentDefinition/#method-is_allowed","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>CommentDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/functions/CommentDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>CommentDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/CustomFieldDefinition/","title":"CustomFieldDefinition","text":""},{"location":"code/R/functions/CustomFieldDefinition/#customfielddefinition","title":"<code>CustomFieldDefinition</code>","text":""},{"location":"code/R/functions/CustomFieldDefinition/#description","title":"Description","text":"<p>Class to store Custom fields</p>"},{"location":"code/R/functions/CustomFieldDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/CustomFieldDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>CustomFieldDefinition$new()</code></li> <li><code>CustomFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/CustomFieldDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the CustomFieldDefinition class</p> <p>Usage</p> <pre><code>CustomFieldDefinition$new(field_specs)\n</code></pre> <p>Arguments:</p> <ul> <li><code>field_specs</code> Specs of the custom field</li> </ul> <p></p>"},{"location":"code/R/functions/CustomFieldDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>CustomFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/DataSet/","title":"DataSet","text":""},{"location":"code/R/functions/DataSet/#dataset","title":"<code>DataSet</code>","text":""},{"location":"code/R/functions/DataSet/#description","title":"Description","text":"<p>This class provides methods to store, normalize, select, and aggregate DataSets.</p>"},{"location":"code/R/functions/DataSet/#examples","title":"Examples","text":"<pre><code>### ------------------------------------------------\n### Method `DataSet$normalise`\n### ------------------------------------------------\n\n## Example usage:\ndataset$normalize(override = list(\"variable1\" = \"value1\"), inplace = FALSE)\n\n\n### ------------------------------------------------\n### Method `DataSet$select`\n### ------------------------------------------------\n\n## Example usage:\ndataset$select(override = list(\"variable1\" = \"value1\"), drop_singular_fields = TRUE, extrapolate_period = FALSE, field1 = \"value1\")\n\n\n### ------------------------------------------------\n### Method `DataSet$aggregate`\n### ------------------------------------------------\n\n## Example usage:\ndataset$aggregate(override = list(\"variable1\" = \"value1\"), drop_singular_fields = TRUE, extrapolate_period = FALSE, agg = \"field\", masks = list(mask1, mask2), masks_database = TRUE)\n</code></pre>"},{"location":"code/R/functions/DataSet/#methods","title":"Methods","text":""},{"location":"code/R/functions/DataSet/#public-methods","title":"Public Methods","text":"<ul> <li><code>DataSet$new()</code></li> <li><code>DataSet$normalise()</code></li> <li><code>DataSet$select()</code></li> <li><code>DataSet$aggregate()</code></li> <li><code>DataSet$clone()</code></li> </ul>"},{"location":"code/R/functions/DataSet/#method-new","title":"Method <code>new()</code>","text":"<p>Create new instance of the DataSet class</p> <p>Usage</p> <pre><code>DataSet$new(\n  parent_variable,\n  include_databases = NULL,\n  file_paths = NULL,\n  check_inconsistencies = FALSE,\n  data = NULL\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>parent_variable</code> Character. Variable to collect Data on.</li> <li><code>include_databases</code> Optional listCharacter | tupleCharacter, optional. Databases to load from.</li> <li><code>file_paths</code> Optional listCharacter, optional. Paths to load data from.</li> <li><code>check_inconsistencies</code> Logical, optional. Whether to check for inconsistencies.</li> <li><code>data</code> Optional DataFrame, optional. Specific data to include in the dataset.</li> </ul> <p></p>"},{"location":"code/R/functions/DataSet/#method-normalise","title":"Method <code>normalise()</code>","text":"<p>Normalize data: default reference units, reference value equal to 1.0, default reported units</p> <p>Usage</p> <pre><code>DataSet$normalise(override = NULL, inplace = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>override</code> Optional listCharacter. Dictionary with key, value pairs of variables to override.</li> <li><code>inplace</code> Logical, optional. Whether to do the normalization in place.</li> </ul> <p>Example:</p> <pre><code>## Example usage:\ndataset$normalize(override = list(\"variable1\" = \"value1\"), inplace = FALSE)\n</code></pre> <p>Returns:</p> <p>DataFrame. If <code>inplace</code> is <code>FALSE</code>, returns normalized dataframe.</p> <p></p>"},{"location":"code/R/functions/DataSet/#method-select","title":"Method <code>select()</code>","text":"<p>Select desired data from the dataframe</p> <p>Usage</p> <pre><code>DataSet$select(\n  override = NULL,\n  drop_singular_fields = TRUE,\n  extrapolate_period = TRUE,\n  ...\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>override</code> Optional listCharacter. Dictionary with key, value pairs of variables to override.</li> <li><code>drop_singular_fields</code> Logical, optional. If <code>TRUE</code>, drop custom fields with only one value.</li> <li><code>extrapolate_period</code> Logical, optional. If <code>TRUE</code>, extrapolate values if no value for this period is given.</li> <li><code>...</code> IDs of values to select.</li> </ul> <p>Example:</p> <pre><code>## Example usage:\ndataset$select(override = list(\"variable1\" = \"value1\"), drop_singular_fields = TRUE, extrapolate_period = FALSE, field1 = \"value1\")\n</code></pre> <p>Returns:</p> <p>DataFrame. DataFrame with selected values.</p> <p></p>"},{"location":"code/R/functions/DataSet/#method-aggregate","title":"Method <code>aggregate()</code>","text":"<p>Aggregates data based on specified parameters, applies masks, and cleans up the resulting DataFrame.</p> <p>Usage</p> <pre><code>DataSet$aggregate(\n  override = NULL,\n  drop_singular_fields = TRUE,\n  extrapolate_period = TRUE,\n  agg = NULL,\n  masks = NULL,\n  masks_database = TRUE,\n  ...\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>override</code> Optional listCharacter. Dictionary with key, value pairs of variables to override.</li> <li><code>drop_singular_fields</code> Logical, optional. If <code>TRUE</code>, drop custom fields with only one value.</li> <li><code>extrapolate_period</code> Logical, optional. If <code>TRUE</code>, extrapolate values if no value for this period is given.</li> <li><code>agg</code> Optional Character | listCharacter | tupleCharacter. Specifies which fields to aggregate over.</li> <li><code>masks</code> Optional listMask. Specifies a list of Mask objects that will be applied to the data during aggregation. These masks can be used to filter or weight the data based on certain conditions defined in the Mask objects.</li> <li><code>masks_database</code> Logical, optional. Determines whether to include masks from databases in the aggregation process. If <code>TRUE</code>, masks from databases will be included along with any masks provided as function arguments. If <code>FALSE</code>, only the masks provided as function arguments will be applied.</li> <li><code>...</code> additional field vals</li> </ul> <p>Example:</p> <pre><code>## Example usage:\ndataset$aggregate(override = list(\"variable1\" = \"value1\"), drop_singular_fields = TRUE, extrapolate_period = FALSE, agg = \"field\", masks = list(mask1, mask2), masks_database = TRUE)\n</code></pre> <p>Returns:</p> <p>DataFrame. The <code>aggregate</code> method returns a pandas DataFrame that has been cleaned up and aggregated based on the specified parameters and input data. The method performs aggregation over component fields and case fields, applies weights based on masks, drops rows with NaN weights, aggregates with weights, inserts reference variables, sorts columns and rows, rounds values, and inserts units before returning the final cleaned and aggregated DataFrame.</p> <p></p>"},{"location":"code/R/functions/DataSet/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>DataSet$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/Mask/","title":"Mask","text":""},{"location":"code/R/functions/Mask/#mask","title":"<code>Mask</code>","text":""},{"location":"code/R/functions/Mask/#description","title":"Description","text":"<p>Class to define masks with conditions and weights to apply to DataFiles</p>"},{"location":"code/R/functions/Mask/#methods","title":"Methods","text":""},{"location":"code/R/functions/Mask/#public-methods","title":"Public Methods","text":"<ul> <li><code>Mask$new()</code></li> <li><code>Mask$matches()</code></li> <li><code>Mask$get_weights()</code></li> <li><code>Mask$clone()</code></li> </ul>"},{"location":"code/R/functions/Mask/#method-new","title":"Method <code>new()</code>","text":"<p>Create a new mask object</p> <p>Usage</p> <pre><code>Mask$new(where = NULL, use = NULL, weight = NULL, other = NaN, comment = \"\")\n</code></pre> <p>Arguments:</p> <ul> <li><code>where</code> MaskCondition | listMaskCondition, optional. Where the mask should be applied.</li> <li><code>use</code> MaskCondition | listMaskCondition, optional. Condition on where to use the masks.</li> <li><code>weight</code> Numeric | Character | listNumeric | Character, optional. Weights to apply.</li> <li><code>other</code> Numeric, optional.</li> <li><code>comment</code> Character, optional. Comment.</li> </ul> <p></p>"},{"location":"code/R/functions/Mask/#method-matches","title":"Method <code>matches()</code>","text":"<p>Check if a mask matches a dataframe by verifying if all 'where' conditions match across all rows.</p> <p>Usage</p> <pre><code>Mask$matches(df)\n</code></pre> <p>Arguments:</p> <ul> <li><code>df</code> DataFrame. Dataframe to check for matches.</li> </ul> <p>Returns:</p> <p>Logical. If the mask matches the dataframe.</p> <p></p>"},{"location":"code/R/functions/Mask/#method-get_weights","title":"Method <code>get_weights()</code>","text":"<p>Apply weights to the dataframe</p> <p>Usage</p> <pre><code>Mask$get_weights(df)\n</code></pre> <p>Arguments:</p> <ul> <li><code>df</code> (<code>Dataframe</code>): Dataframe to apply weights on</li> </ul> <p>Returns:</p> <p>Dataframe. Dataframe with applied weights</p> <p></p>"},{"location":"code/R/functions/Mask/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>Mask$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/PeriodFieldDefinition/","title":"PeriodFieldDefinition","text":""},{"location":"code/R/functions/PeriodFieldDefinition/#periodfielddefinition","title":"<code>PeriodFieldDefinition</code>","text":""},{"location":"code/R/functions/PeriodFieldDefinition/#description","title":"Description","text":"<p>Class to store Period fields</p>"},{"location":"code/R/functions/PeriodFieldDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/PeriodFieldDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>PeriodFieldDefinition$new()</code></li> <li><code>PeriodFieldDefinition$is_allowed()</code></li> <li><code>PeriodFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/PeriodFieldDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the PeriodFieldDefinition Class</p> <p>Usage</p> <pre><code>PeriodFieldDefinition$new(name, description)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the field.</li> <li><code>description</code> Character. Description of the field</li> </ul> <p></p>"},{"location":"code/R/functions/PeriodFieldDefinition/#method-is_allowed","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>PeriodFieldDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/functions/PeriodFieldDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>PeriodFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/RegionFieldDefinition/","title":"RegionFieldDefinition","text":""},{"location":"code/R/functions/RegionFieldDefinition/#regionfielddefinition","title":"<code>RegionFieldDefinition</code>","text":""},{"location":"code/R/functions/RegionFieldDefinition/#description","title":"Description","text":"<p>Class to store Region fields</p>"},{"location":"code/R/functions/RegionFieldDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/RegionFieldDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>RegionFieldDefinition$new()</code></li> <li><code>RegionFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/RegionFieldDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the RegionFieldDefinition class</p> <p>Usage</p> <pre><code>RegionFieldDefinition$new(name, description)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the field.</li> <li><code>description</code> Character. Description of the field.</li> </ul> <p></p>"},{"location":"code/R/functions/RegionFieldDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>RegionFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/SourceFieldDefinition/","title":"SourceFieldDefinition","text":""},{"location":"code/R/functions/SourceFieldDefinition/#sourcefielddefinition","title":"<code>SourceFieldDefinition</code>","text":""},{"location":"code/R/functions/SourceFieldDefinition/#description","title":"Description","text":"<p>Class to store Source fields</p>"},{"location":"code/R/functions/SourceFieldDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/SourceFieldDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>SourceFieldDefinition$new()</code></li> <li><code>SourceFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/SourceFieldDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the SourceFieldDefinition class</p> <p>Usage</p> <pre><code>SourceFieldDefinition$new(name, description)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the field.</li> <li><code>description</code> Character. Description of the field.</li> </ul> <p></p>"},{"location":"code/R/functions/SourceFieldDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>SourceFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/TEBase/","title":"TEBase","text":""},{"location":"code/R/functions/TEBase/#tebase","title":"<code>TEBase</code>","text":""},{"location":"code/R/functions/TEBase/#description","title":"Description","text":"<p>This is the base class for technoeconomic data.</p>"},{"location":"code/R/functions/TEBase/#examples","title":"Examples","text":"<pre><code>## Example usage:\nbase_technoeconomic_data &lt;- TEBase$new(\"variable_name\")\n</code></pre>"},{"location":"code/R/functions/TEBase/#methods","title":"Methods","text":""},{"location":"code/R/functions/TEBase/#public-methods","title":"Public Methods","text":"<ul> <li><code>TEBase$new()</code></li> <li><code>TEBase$clone()</code></li> </ul>"},{"location":"code/R/functions/TEBase/#method-new","title":"Method <code>new()</code>","text":"<p>Create new instance of TEBase class. Set parent variable and technology specifications (var_specs) from input</p> <p>Usage</p> <pre><code>TEBase$new(parent_variable)\n</code></pre> <p>Arguments:</p> <ul> <li><code>parent_variable</code> (<code>character</code>) Name of the parent variable</li> </ul> <p></p>"},{"location":"code/R/functions/TEBase/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>TEBase$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/TEDF/","title":"TEDF","text":""},{"location":"code/R/functions/TEDF/#tedf","title":"<code>TEDF</code>","text":""},{"location":"code/R/functions/TEDF/#description","title":"Description","text":"<p>This class is used to store Technoeconomic DataFiles.</p>"},{"location":"code/R/functions/TEDF/#examples","title":"Examples","text":"<pre><code>## Example usage:\ntedf &lt;- TEDF$new(\"variable_name\")\ntedf$load()\ntedf$read(\"file_path.csv\")\ntedf$write(\"output_file_path.csv\")\ntedf$check()\ntedf$check_row()\n\n\n### ------------------------------------------------\n### Method `TEDF$load`\n### ------------------------------------------------\n\n## Example usage:\ntedf$load()\n\n\n### ------------------------------------------------\n### Method `TEDF$read`\n### ------------------------------------------------\n\n## Example usage:\ntedf$read()\n\n\n### ------------------------------------------------\n### Method `TEDF$write`\n### ------------------------------------------------\n\n## Example usage:\ntedf$write()\n\n\n### ------------------------------------------------\n### Method `TEDF$check`\n### ------------------------------------------------\n\n## Example usage:\ntedf$check(raise_exception = TRUE)\n</code></pre>"},{"location":"code/R/functions/TEDF/#methods","title":"Methods","text":""},{"location":"code/R/functions/TEDF/#public-methods","title":"Public Methods","text":"<ul> <li><code>TEDF$new()</code></li> <li><code>TEDF$load()</code></li> <li><code>TEDF$read()</code></li> <li><code>TEDF$write()</code></li> <li><code>TEDF$check()</code></li> <li><code>TEDF$check_row()</code></li> <li><code>TEDF$clone()</code></li> </ul>"},{"location":"code/R/functions/TEDF/#method-new","title":"Method <code>new()</code>","text":"<p>Create new instance of TEDF class. Initialise parent class and object fields</p> <p>Usage</p> <pre><code>TEDF$new(\n  parent_variable,\n  database_id = \"public\",\n  file_path = NULL,\n  data = NULL\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>parent_variable</code> (<code>Character</code>): Variable from which data should be collected.</li> <li><code>database_id</code> (<code>Character</code>):, default: \"public\". Database from which to load data.</li> <li><code>file_path</code> (<code>Path</code>):, optional. File path from which to load file.</li> <li><code>data</code> (<code>DataFrame</code>):, optional. Specific Technoeconomic data.</li> </ul> <p></p>"},{"location":"code/R/functions/TEDF/#method-load","title":"Method <code>load()</code>","text":"<p>Load TEDataFile (only if it has not been read yet)</p> <p>Usage</p> <pre><code>TEDF$load()\n</code></pre> <p>Example:</p> <pre><code>## Example usage:\ntedf$load()\n</code></pre> <p>Returns:</p> <p>TEDF. Returns the TEDF object it is called on.</p> <p></p>"},{"location":"code/R/functions/TEDF/#method-read","title":"Method <code>read()</code>","text":"<p>This method reads TEDF from a CSV file.</p> <p>Usage</p> <pre><code>TEDF$read()\n</code></pre> <p>Example:</p> <pre><code>## Example usage:\ntedf$read()\n</code></pre> <p></p>"},{"location":"code/R/functions/TEDF/#method-write","title":"Method <code>write()</code>","text":"<p>write TEDF to CSV file.</p> <p>Usage</p> <pre><code>TEDF$write()\n</code></pre> <p>Example:</p> <pre><code>## Example usage:\ntedf$write()\n</code></pre> <p></p>"},{"location":"code/R/functions/TEDF/#method-check","title":"Method <code>check()</code>","text":"<p>Check that TEDF is consistent and add inconsistencies to internal parameter</p> <p>Usage</p> <pre><code>TEDF$check(raise_exception = TRUE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>raise_exception</code> Logical, default: TRUE. If an exception is to be raised.</li> </ul> <p>Example:</p> <pre><code>## Example usage:\ntedf$check(raise_exception = TRUE)\n</code></pre> <p></p>"},{"location":"code/R/functions/TEDF/#method-check_row","title":"Method <code>check_row()</code>","text":"<p>checks if row of dataframe has issues - NOT IMPLEMENTED YET</p> <p>Usage</p> <pre><code>TEDF$check_row(row_id, raise_exception = TRUE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>row_id</code> Id of the row</li> <li><code>raise_exception</code> (<code>logical</code>) If exception is to be raised</li> </ul> <p></p>"},{"location":"code/R/functions/TEDF/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>TEDF$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/TEDFInconsistencyException/","title":"TEDFInconsistencyException","text":""},{"location":"code/R/functions/TEDFInconsistencyException/#tedfinconsistencyexception","title":"<code>TEDFInconsistencyException</code>","text":""},{"location":"code/R/functions/TEDFInconsistencyException/#description","title":"Description","text":"<p>This is a class to store inconsistencies in the TEDFs</p>"},{"location":"code/R/functions/TEDFInconsistencyException/#methods","title":"Methods","text":""},{"location":"code/R/functions/TEDFInconsistencyException/#public-methods","title":"Public Methods","text":"<ul> <li><code>TEDFInconsistencyException$new()</code></li> <li><code>TEDFInconsistencyException$clone()</code></li> </ul>"},{"location":"code/R/functions/TEDFInconsistencyException/#method-new","title":"Method <code>new()</code>","text":"<p>Create instance of TEDFInconsistencyException class</p> <p>Usage</p> <pre><code>TEDFInconsistencyException$new(\n  message = \"Inconsistency detected\",\n  row_id = NULL,\n  col_id = NULL,\n  file_path = NULL\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>message</code> (<code>character</code>) the message of the exception</li> <li><code>row_id</code> Id of the row</li> <li><code>col_id</code> Id of the column</li> <li><code>file_path</code> file path</li> </ul> <p></p>"},{"location":"code/R/functions/TEDFInconsistencyException/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>TEDFInconsistencyException$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/UnitDefinition/","title":"UnitDefinition","text":""},{"location":"code/R/functions/UnitDefinition/#unitdefinition","title":"<code>UnitDefinition</code>","text":""},{"location":"code/R/functions/UnitDefinition/#description","title":"Description","text":"<p>Class to store Unit columns</p>"},{"location":"code/R/functions/UnitDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/UnitDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>UnitDefinition$new()</code></li> <li><code>UnitDefinition$is_allowed()</code></li> <li><code>UnitDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/UnitDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the UnitDefinition class</p> <p>Usage</p> <pre><code>UnitDefinition$new(name, description, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the column.</li> <li><code>description</code> Character. Description of the column.</li> <li><code>required</code> Logical. Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/functions/UnitDefinition/#method-is_allowed","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>UnitDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/functions/UnitDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>UnitDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/ValueDefinition/","title":"ValueDefinition","text":""},{"location":"code/R/functions/ValueDefinition/#valuedefinition","title":"<code>ValueDefinition</code>","text":""},{"location":"code/R/functions/ValueDefinition/#description","title":"Description","text":"<p>Class to store Value columns</p>"},{"location":"code/R/functions/ValueDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/ValueDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>ValueDefinition$new()</code></li> <li><code>ValueDefinition$is_allowed()</code></li> <li><code>ValueDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/ValueDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the ValueDefinition class</p> <p>Usage</p> <pre><code>ValueDefinition$new(name, description, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the column.</li> <li><code>description</code> Character. Description of the column.</li> <li><code>required</code> Logical. Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/functions/ValueDefinition/#method-is_allowed","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>ValueDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/functions/ValueDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>ValueDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/VariableDefinition/","title":"VariableDefinition","text":""},{"location":"code/R/functions/VariableDefinition/#variabledefinition","title":"<code>VariableDefinition</code>","text":""},{"location":"code/R/functions/VariableDefinition/#description","title":"Description","text":"<p>Class to store variable columns</p>"},{"location":"code/R/functions/VariableDefinition/#methods","title":"Methods","text":""},{"location":"code/R/functions/VariableDefinition/#public-methods","title":"Public Methods","text":"<ul> <li><code>VariableDefinition$new()</code></li> <li><code>VariableDefinition$is_allowed()</code></li> <li><code>VariableDefinition$clone()</code></li> </ul>"},{"location":"code/R/functions/VariableDefinition/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the VariableDefinition class</p> <p>Usage</p> <pre><code>VariableDefinition$new(name, description, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> (<code>Character</code>): Name of the column.</li> <li><code>description</code> (<code>Character</code>): Description of the column.</li> <li><code>required</code> (<code>Logical</code>): Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/functions/VariableDefinition/#method-is_allowed","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>VariableDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/functions/VariableDefinition/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>VariableDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/functions/apply_cond/","title":"Apply cond","text":""},{"location":"code/R/functions/apply_cond/#apply_cond","title":"<code>apply_cond</code>","text":"<p>apply_cond</p>"},{"location":"code/R/functions/apply_cond/#description","title":"Description","text":"<p>Takes a pandas DataFrame and a condition, which can be a string, dictionary, or callable, and applies the condition to the DataFrame using <code>eval</code> or <code>apply</code> accordingly.</p>"},{"location":"code/R/functions/apply_cond/#usage","title":"Usage","text":"<pre><code>apply_cond(df, cond)\n</code></pre>"},{"location":"code/R/functions/apply_cond/#arguments","title":"Arguments","text":"Argument Description <code>df</code> DataFrame. A pandas DataFrame containing the data on which the condition will be applied. <code>cond</code> MaskCondition. The condition to be applied on the dataframe. Can be either a string, a dictionary, or a callable function."},{"location":"code/R/functions/apply_cond/#return-value","title":"Return Value","text":"<p>DataFrame. Dataframe evaluated at the mask condition.</p>"},{"location":"code/R/functions/collect_files/","title":"Collect files","text":""},{"location":"code/R/functions/collect_files/#collect_files","title":"<code>collect_files</code>","text":"<p>collect_files</p>"},{"location":"code/R/functions/collect_files/#description","title":"Description","text":"<p>Takes a parent variable and optional list of databases to include, checks for their existence, and collects files and directories based on the parent variable.</p>"},{"location":"code/R/functions/collect_files/#usage","title":"Usage","text":"<pre><code>collect_files(parent_variable, include_databases = NULL)\n</code></pre>"},{"location":"code/R/functions/collect_files/#arguments","title":"Arguments","text":"Argument Description <code>parent_variable</code> Character. Variable to collect files on. <code>include_databases</code> Optional listCharacter. List of Database IDs to collect files from."},{"location":"code/R/functions/collect_files/#return-value","title":"Return Value","text":"<p>List of tuples. List of tuples containing the parent variable and the database ID for each file found in the specified directories.</p>"},{"location":"code/R/functions/collect_files/#examples","title":"Examples","text":"<pre><code>## Example usage:\ncollect_files(\"variable_name\", c(\"db1\", \"db2\"))\n</code></pre>"},{"location":"code/R/functions/combine_units/","title":"Combine units","text":""},{"location":"code/R/functions/combine_units/#combine_units","title":"<code>combine_units</code>","text":"<p>combine_units</p>"},{"location":"code/R/functions/combine_units/#description","title":"Description","text":"<p>Combine fraction of two units into an updated unit string</p>"},{"location":"code/R/functions/combine_units/#usage","title":"Usage","text":"<pre><code>combine_units(numerator, denominator)\n</code></pre>"},{"location":"code/R/functions/combine_units/#arguments","title":"Arguments","text":"Argument Description <code>numerator</code> Character. Numerator of the fraction. <code>denominator</code> Character. Denominator of the fraction."},{"location":"code/R/functions/combine_units/#return-value","title":"Return Value","text":"<p>Character. Updated unit string after simplification.</p>"},{"location":"code/R/functions/combine_units/#examples","title":"Examples","text":"<pre><code>## Example usage:\ncombine_units(\"m\", \"s\")\n</code></pre>"},{"location":"code/R/functions/is_float/","title":"Is float","text":""},{"location":"code/R/functions/is_float/#is_float","title":"<code>is_float</code>","text":"<p>is_float</p>"},{"location":"code/R/functions/is_float/#description","title":"Description","text":"<p>Checks if a given string can be converted to a floating-point number in Python.</p>"},{"location":"code/R/functions/is_float/#usage","title":"Usage","text":"<pre><code>is_float(string)\n</code></pre>"},{"location":"code/R/functions/is_float/#arguments","title":"Arguments","text":"Argument Description <code>string</code> Character. String to check."},{"location":"code/R/functions/is_float/#return-value","title":"Return Value","text":"<p>Logical. <code>TRUE</code> if conversion was successful, <code>FALSE</code> if not.</p>"},{"location":"code/R/functions/is_float/#examples","title":"Examples","text":"<pre><code>## Example usage:\nis_numeric(\"3.14\")\n</code></pre>"},{"location":"code/R/functions/normalise_units/","title":"Normalise units","text":""},{"location":"code/R/functions/normalise_units/#normalise_units","title":"<code>normalise_units</code>","text":"<p>normalise_units</p>"},{"location":"code/R/functions/normalise_units/#description","title":"Description","text":"<p>Takes a DataFrame with reported or reference data, along with dictionaries mapping variable units and flow IDs, and normalizes the units of the variables in the DataFrame based on the provided mappings.</p>"},{"location":"code/R/functions/normalise_units/#usage","title":"Usage","text":"<pre><code>normalise_units(df, level, var_units, var_flow_ids)\n</code></pre>"},{"location":"code/R/functions/normalise_units/#arguments","title":"Arguments","text":"Argument Description <code>df</code> DataFrame. Dataframe to be normalized. <code>level</code> Character. Specifies whether the data should be normalized on the reported or reference values. Possible values are 'reported' or 'reference'. <code>var_units</code> List. Dictionary that maps a combination of parent variable and variable to its corresponding unit. The keys in the dictionary are in the format \"{parent_variable} <code>var_flow_ids</code> List. Dictionary that maps a combination of parent variable and variable to a specific flow ID. This flow ID is used for unit conversion in the <code>normalize_units</code> function."},{"location":"code/R/functions/normalise_units/#return-value","title":"Return Value","text":"<p>DataFrame. Normalized dataframe.</p>"},{"location":"code/R/functions/normalise_units/#examples","title":"Examples","text":"<pre><code>## Example usage:\nnormalize_dataframe(df, \"reported\", var_units, var_flow_ids)\n</code></pre>"},{"location":"code/R/functions/normalise_values/","title":"Normalise values","text":""},{"location":"code/R/functions/normalise_values/#normalise_values","title":"<code>normalise_values</code>","text":"<p>normalise_values</p>"},{"location":"code/R/functions/normalise_values/#description","title":"Description","text":"<p>Takes a DataFrame as input, normalizes the 'value' and 'uncertainty' columns by the reference value, and updates the 'reference_value' column accordingly.</p>"},{"location":"code/R/functions/normalise_values/#usage","title":"Usage","text":"<pre><code>normalise_values(df)\n</code></pre>"},{"location":"code/R/functions/normalise_values/#arguments","title":"Arguments","text":"Argument Description <code>df</code> DataFrame. Dataframe to be normalized."},{"location":"code/R/functions/normalise_values/#return-value","title":"Return Value","text":"<p>DataFrame. Returns a modified DataFrame where the 'value' column has been divided by the 'reference_value' column (or 1.0 if 'reference_value' is null), the 'uncertainty' column has been divided by the 'reference_value' column, and the 'reference_value' column has been replaced with 1.0 if it was not null.</p>"},{"location":"code/R/functions/normalise_values/#examples","title":"Examples","text":"<pre><code>## Example usage:\nnormalized_df &lt;- normalize_values(df)\n</code></pre>"},{"location":"code/R/functions/read_csv_file/","title":"Read csv file","text":""},{"location":"code/R/functions/read_csv_file/#read_csv_file","title":"<code>read_csv_file</code>","text":"<p>read_csv_file</p>"},{"location":"code/R/functions/read_csv_file/#description","title":"Description","text":"<p>Read a csv datafile</p>"},{"location":"code/R/functions/read_csv_file/#usage","title":"Usage","text":"<pre><code>read_csv_file(fpath)\n</code></pre>"},{"location":"code/R/functions/read_csv_file/#arguments","title":"Arguments","text":"Argument Description <code>fpath</code> path of the csv file"},{"location":"code/R/functions/read_definitions/","title":"Read definitions","text":""},{"location":"code/R/functions/read_definitions/#read_definitions","title":"<code>read_definitions</code>","text":"<p>read_definitions</p>"},{"location":"code/R/functions/read_definitions/#description","title":"Description","text":"<p>Reads YAML files from definitions directory, extracts tags, inserts tags into definitions, replaces tokens in definitions, and returns the updated definitions.</p>"},{"location":"code/R/functions/read_definitions/#usage","title":"Usage","text":"<pre><code>read_definitions(definitions_dir, flows, techs)\n</code></pre>"},{"location":"code/R/functions/read_definitions/#arguments","title":"Arguments","text":"Argument Description <code>definitions_dir</code> Character. Path leading to the definitions. <code>flows</code> List. Dictionary containing the different flow types. Each key represents a flow type, the corresponding value is a dictionary containing key value pairs of attributes like density, energy content and their values. <code>techs</code> List. Dictionary containing information about different technologies. Each key in the dictionary represents a unique technology ID, and the corresponding value is a dictionary containing various specifications for that technology, like 'description', 'class', 'primary output' etc."},{"location":"code/R/functions/read_definitions/#return-value","title":"Return Value","text":"<p>List. Dictionary containing the definitions after processing and replacing tags and tokens.</p>"},{"location":"code/R/functions/read_masks/","title":"Read masks","text":""},{"location":"code/R/functions/read_masks/#read_masks","title":"<code>read_masks</code>","text":"<p>read_masks</p>"},{"location":"code/R/functions/read_masks/#description","title":"Description","text":"<p>Reads YAML files containing mask specifications from multiple databases and returns a list of Mask objects.</p>"},{"location":"code/R/functions/read_masks/#usage","title":"Usage","text":"<pre><code>read_masks(variable)\n</code></pre>"},{"location":"code/R/functions/read_masks/#arguments","title":"Arguments","text":"Argument Description <code>variable</code> Character. Variable to be read."},{"location":"code/R/functions/read_masks/#return-value","title":"Return Value","text":"<p>List. List with masks for the variable.</p>"},{"location":"code/R/functions/read_yml_file/","title":"Read yml file","text":""},{"location":"code/R/functions/read_yml_file/#read_yml_file","title":"<code>read_yml_file</code>","text":"<p>read_yaml_file</p>"},{"location":"code/R/functions/read_yml_file/#description","title":"Description","text":"<p>read YAML config file</p>"},{"location":"code/R/functions/read_yml_file/#usage","title":"Usage","text":"<pre><code>read_yml_file(fpath)\n</code></pre>"},{"location":"code/R/functions/read_yml_file/#arguments","title":"Arguments","text":"Argument Description <code>fpath</code> path of the YAML file"},{"location":"code/R/functions/replace_tags/","title":"Replace tags","text":""},{"location":"code/R/functions/replace_tags/#replace_tags","title":"<code>replace_tags</code>","text":"<p>replace_tags</p>"},{"location":"code/R/functions/replace_tags/#description","title":"Description","text":"<p>Replaces specified tags in dictionary keys and values with corresponding items from another dictionary.</p>"},{"location":"code/R/functions/replace_tags/#usage","title":"Usage","text":"<pre><code>replace_tags(definitions, tag, items)\n</code></pre>"},{"location":"code/R/functions/replace_tags/#arguments","title":"Arguments","text":"Argument Description <code>definitions</code> List. Dictionary containing the definitions, where the tags should be replaced by the items. <code>tag</code> Character. String to identify where replacements should be made in the definitions. Specifies the placeholder that needs to be replaced with actual values from the <code>items</code> dictionary. <code>items</code> List. Dictionary containing the items from which to replace the definitions."},{"location":"code/R/functions/replace_tags/#return-value","title":"Return Value","text":"<p>List. Dictionary containing the definitions with replacements based on the provided tag and items.</p>"},{"location":"code/R/functions/unit_convert/","title":"Unit convert","text":""},{"location":"code/R/functions/unit_convert/#unit_convert","title":"<code>unit_convert</code>","text":"<p>unit_convert</p>"},{"location":"code/R/functions/unit_convert/#description","title":"Description","text":"<p>Converts units with optional flow context handling based on specified variants and flow ID. The function checks if the input units are not NaN, then it proceeds to handle different cases based on the presence of a flow context and unit variants.</p>"},{"location":"code/R/functions/unit_convert/#usage","title":"Usage","text":"<pre><code>unit_convert(unit_from, unit_to, flow_id = NULL)\n</code></pre>"},{"location":"code/R/functions/unit_convert/#arguments","title":"Arguments","text":"Argument Description <code>unit_from</code> Character or numeric. Unit to convert from. <code>unit_to</code> Character or numeric. Unit to convert to. <code>flow_id</code> Character or NULL. Identifier for the specific flow or process."},{"location":"code/R/functions/unit_convert/#return-value","title":"Return Value","text":"<p>Numeric. Conversion factor between <code>unit_from</code> and <code>unit_to</code>.</p>"},{"location":"code/R/functions/unit_convert/#examples","title":"Examples","text":"<pre><code>## Example usage:\nunit_convert(\"m\", \"km\", flow_id = NULL)\n</code></pre>"},{"location":"code/R/functions/unit_token_func/","title":"Unit token func","text":""},{"location":"code/R/functions/unit_token_func/#unit_token_func","title":"<code>unit_token_func</code>","text":"<p>unit_token_func</p>"},{"location":"code/R/functions/unit_token_func/#description","title":"Description","text":"<p>Takes a unit component type and a dictionary of flows, and returns a lambda function that extracts the default unit based on the specified component type from the flow dictionary.</p>"},{"location":"code/R/functions/unit_token_func/#usage","title":"Usage","text":"<pre><code>unit_token_func(unit_component, flows)\n</code></pre>"},{"location":"code/R/functions/unit_token_func/#arguments","title":"Arguments","text":"Argument Description <code>unit_component</code> Character. Specifies the type of unit token to be returned. Possible values are 'full', 'raw', 'variant'. <code>flows</code> List. Dictionary containing the flows."},{"location":"code/R/functions/unit_token_func/#return-value","title":"Return Value","text":"<p>Function. Lambda function that takes a dictionary <code>def_specs</code> as input. The lambda function will return different values based on the <code>unit_component</code> parameter.</p>"},{"location":"code/R/modules/columns/","title":"columns","text":""},{"location":"code/R/modules/columns/#is_float","title":"<code>is_float</code>","text":"<p>is_float</p>"},{"location":"code/R/modules/columns/#description","title":"Description","text":"<p>Checks if a given string can be converted to a floating-point number in Python.</p>"},{"location":"code/R/modules/columns/#usage","title":"Usage","text":"<pre><code>is_float(string)\n</code></pre>"},{"location":"code/R/modules/columns/#arguments","title":"Arguments","text":"Argument Description <code>string</code> Character. String to check."},{"location":"code/R/modules/columns/#return-value","title":"Return Value","text":"<p>Logical. <code>TRUE</code> if conversion was successful, <code>FALSE</code> if not.</p>"},{"location":"code/R/modules/columns/#examples","title":"Examples","text":"<pre><code>## Example usage:\nis_numeric(\"3.14\")\n</code></pre>"},{"location":"code/R/modules/columns/#abstractcolumndefinition","title":"<code>AbstractColumnDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_1","title":"Description","text":"<p>Abstract class to store columns</p>"},{"location":"code/R/modules/columns/#methods","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods","title":"Public Methods","text":"<ul> <li><code>AbstractColumnDefinition$new()</code></li> <li><code>AbstractColumnDefinition$is_allowed()</code></li> <li><code>AbstractColumnDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the AbstractColumnDefinition class</p> <p>Usage</p> <pre><code>AbstractColumnDefinition$new(col_type, name, description, dtype, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>col_type</code> (<code>data.frame</code>)\\cr Type of the column.</li> <li><code>name</code> (<code>character(1)</code>)\\cr Name of the column.</li> <li><code>description</code> (<code>character(1)</code>)\\cr Description of the column.</li> <li><code>dtype</code> (\\verb{Data type})\\cr Data type of the column.</li> <li><code>required</code> (<code>Logical</code>)\\cr Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-is_allowed","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>AbstractColumnDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>AbstractColumnDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#variabledefinition","title":"<code>VariableDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_2","title":"Description","text":"<p>Class to store variable columns</p>"},{"location":"code/R/modules/columns/#methods_1","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_1","title":"Public Methods","text":"<ul> <li><code>VariableDefinition$new()</code></li> <li><code>VariableDefinition$is_allowed()</code></li> <li><code>VariableDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_1","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the VariableDefinition class</p> <p>Usage</p> <pre><code>VariableDefinition$new(name, description, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> (<code>Character</code>): Name of the column.</li> <li><code>description</code> (<code>Character</code>): Description of the column.</li> <li><code>required</code> (<code>Logical</code>): Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-is_allowed_1","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>VariableDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone_1","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>VariableDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#unitdefinition","title":"<code>UnitDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_3","title":"Description","text":"<p>Class to store Unit columns</p>"},{"location":"code/R/modules/columns/#methods_2","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_2","title":"Public Methods","text":"<ul> <li><code>UnitDefinition$new()</code></li> <li><code>UnitDefinition$is_allowed()</code></li> <li><code>UnitDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_2","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the UnitDefinition class</p> <p>Usage</p> <pre><code>UnitDefinition$new(name, description, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the column.</li> <li><code>description</code> Character. Description of the column.</li> <li><code>required</code> Logical. Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-is_allowed_2","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>UnitDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone_2","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>UnitDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#valuedefinition","title":"<code>ValueDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_4","title":"Description","text":"<p>Class to store Value columns</p>"},{"location":"code/R/modules/columns/#methods_3","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_3","title":"Public Methods","text":"<ul> <li><code>ValueDefinition$new()</code></li> <li><code>ValueDefinition$is_allowed()</code></li> <li><code>ValueDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_3","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the ValueDefinition class</p> <p>Usage</p> <pre><code>ValueDefinition$new(name, description, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the column.</li> <li><code>description</code> Character. Description of the column.</li> <li><code>required</code> Logical. Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-is_allowed_3","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>ValueDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone_3","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>ValueDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#commentdefinition","title":"<code>CommentDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_5","title":"Description","text":"<p>Class to store comment columns</p>"},{"location":"code/R/modules/columns/#methods_4","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_4","title":"Public Methods","text":"<ul> <li><code>CommentDefinition$new()</code></li> <li><code>CommentDefinition$is_allowed()</code></li> <li><code>CommentDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_4","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the CommentDefinition Class</p> <p>Usage</p> <pre><code>CommentDefinition$new(name, description, required)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the column.</li> <li><code>description</code> Character. Description of the column.</li> <li><code>required</code> Logical. Bool that specifies if the column is required.</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-is_allowed_4","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>CommentDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone_4","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>CommentDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#abstractfielddefinition","title":"<code>AbstractFieldDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_6","title":"Description","text":"<p>Abstract class to store fields</p>"},{"location":"code/R/modules/columns/#examples_1","title":"Examples","text":"<pre><code>### ------------------------------------------------\n### Method `AbstractFieldDefinition$select_and_expand`\n### ------------------------------------------------\n\n## Example usage:\n## select_and_expand(df, \"col_id\", field_vals = NULL)\n</code></pre>"},{"location":"code/R/modules/columns/#methods_5","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_5","title":"Public Methods","text":"<ul> <li><code>AbstractFieldDefinition$new()</code></li> <li><code>AbstractFieldDefinition$is_allowed()</code></li> <li><code>AbstractFieldDefinition$select_and_expand()</code></li> <li><code>AbstractFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_5","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the AbstractFieldDefinition Class</p> <p>Usage</p> <pre><code>AbstractFieldDefinition$new(\n  field_type,\n  name,\n  description,\n  dtype,\n  coded,\n  codes = NULL\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>field_type</code> Type of the field</li> <li><code>name</code> Name of the field</li> <li><code>description</code> Description of the field</li> <li><code>dtype</code> Data type of the field</li> <li><code>coded</code> If the field is coded</li> <li><code>codes</code> Optional codes for the field (default: NULL)</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-is_allowed_5","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>AbstractFieldDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-select_and_expand","title":"Method <code>select_and_expand()</code>","text":"<p>Select and expand fields which are valid for multiple periods or other field vals</p> <p>Usage</p> <pre><code>AbstractFieldDefinition$select_and_expand(df, col_id, field_vals = NA, ...)\n</code></pre> <p>Arguments:</p> <ul> <li><code>df</code> DataFrame where fields should be selected and expanded</li> <li><code>col_id</code> col_id of the column to be selected and expanded</li> <li><code>field_vals</code> NULL or list of field_vals to select and expand</li> <li><code>...</code> Additional keyword arguments</li> </ul> <p>Example:</p> <pre><code>## Example usage:\n## select_and_expand(df, \"col_id\", field_vals = NULL)\n</code></pre> <p>Returns:</p> <p>DataFrame where fields are selected and expanded</p> <p></p>"},{"location":"code/R/modules/columns/#method-clone_5","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>AbstractFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#regionfielddefinition","title":"<code>RegionFieldDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_7","title":"Description","text":"<p>Class to store Region fields</p>"},{"location":"code/R/modules/columns/#methods_6","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_6","title":"Public Methods","text":"<ul> <li><code>RegionFieldDefinition$new()</code></li> <li><code>RegionFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_6","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the RegionFieldDefinition class</p> <p>Usage</p> <pre><code>RegionFieldDefinition$new(name, description)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the field.</li> <li><code>description</code> Character. Description of the field.</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone_6","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>RegionFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#periodfielddefinition","title":"<code>PeriodFieldDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_8","title":"Description","text":"<p>Class to store Period fields</p>"},{"location":"code/R/modules/columns/#methods_7","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_7","title":"Public Methods","text":"<ul> <li><code>PeriodFieldDefinition$new()</code></li> <li><code>PeriodFieldDefinition$is_allowed()</code></li> <li><code>PeriodFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_7","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the PeriodFieldDefinition Class</p> <p>Usage</p> <pre><code>PeriodFieldDefinition$new(name, description)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the field.</li> <li><code>description</code> Character. Description of the field</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-is_allowed_6","title":"Method <code>is_allowed()</code>","text":"<p>Tests if cell is allowed</p> <p>Usage</p> <pre><code>PeriodFieldDefinition$is_allowed(cell)\n</code></pre> <p>Arguments:</p> <ul> <li><code>cell</code> cell to test</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone_7","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>PeriodFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#sourcefielddefinition","title":"<code>SourceFieldDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_9","title":"Description","text":"<p>Class to store Source fields</p>"},{"location":"code/R/modules/columns/#methods_8","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_8","title":"Public Methods","text":"<ul> <li><code>SourceFieldDefinition$new()</code></li> <li><code>SourceFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_8","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the SourceFieldDefinition class</p> <p>Usage</p> <pre><code>SourceFieldDefinition$new(name, description)\n</code></pre> <p>Arguments:</p> <ul> <li><code>name</code> Character. Name of the field.</li> <li><code>description</code> Character. Description of the field.</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone_8","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>SourceFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/columns/#customfielddefinition","title":"<code>CustomFieldDefinition</code>","text":""},{"location":"code/R/modules/columns/#description_10","title":"Description","text":"<p>Class to store Custom fields</p>"},{"location":"code/R/modules/columns/#methods_9","title":"Methods","text":""},{"location":"code/R/modules/columns/#public-methods_9","title":"Public Methods","text":"<ul> <li><code>CustomFieldDefinition$new()</code></li> <li><code>CustomFieldDefinition$clone()</code></li> </ul>"},{"location":"code/R/modules/columns/#method-new_9","title":"Method <code>new()</code>","text":"<p>Creates a new instance of the CustomFieldDefinition class</p> <p>Usage</p> <pre><code>CustomFieldDefinition$new(field_specs)\n</code></pre> <p>Arguments:</p> <ul> <li><code>field_specs</code> Specs of the custom field</li> </ul> <p></p>"},{"location":"code/R/modules/columns/#method-clone_9","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>CustomFieldDefinition$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/definitions/","title":"definitions","text":""},{"location":"code/R/modules/definitions/#read_definitions","title":"<code>read_definitions</code>","text":"<p>read_definitions</p>"},{"location":"code/R/modules/definitions/#description","title":"Description","text":"<p>Reads YAML files from definitions directory, extracts tags, inserts tags into definitions, replaces tokens in definitions, and returns the updated definitions.</p>"},{"location":"code/R/modules/definitions/#usage","title":"Usage","text":"<pre><code>read_definitions(definitions_dir, flows, techs)\n</code></pre>"},{"location":"code/R/modules/definitions/#arguments","title":"Arguments","text":"Argument Description <code>definitions_dir</code> Character. Path leading to the definitions. <code>flows</code> List. Dictionary containing the different flow types. Each key represents a flow type, the corresponding value is a dictionary containing key value pairs of attributes like density, energy content and their values. <code>techs</code> List. Dictionary containing information about different technologies. Each key in the dictionary represents a unique technology ID, and the corresponding value is a dictionary containing various specifications for that technology, like 'description', 'class', 'primary output' etc."},{"location":"code/R/modules/definitions/#return-value","title":"Return Value","text":"<p>List. Dictionary containing the definitions after processing and replacing tags and tokens.</p>"},{"location":"code/R/modules/definitions/#replace_tags","title":"<code>replace_tags</code>","text":"<p>replace_tags</p>"},{"location":"code/R/modules/definitions/#description_1","title":"Description","text":"<p>Replaces specified tags in dictionary keys and values with corresponding items from another dictionary.</p>"},{"location":"code/R/modules/definitions/#usage_1","title":"Usage","text":"<pre><code>replace_tags(definitions, tag, items)\n</code></pre>"},{"location":"code/R/modules/definitions/#arguments_1","title":"Arguments","text":"Argument Description <code>definitions</code> List. Dictionary containing the definitions, where the tags should be replaced by the items. <code>tag</code> Character. String to identify where replacements should be made in the definitions. Specifies the placeholder that needs to be replaced with actual values from the <code>items</code> dictionary. <code>items</code> List. Dictionary containing the items from which to replace the definitions."},{"location":"code/R/modules/definitions/#return-value_1","title":"Return Value","text":"<p>List. Dictionary containing the definitions with replacements based on the provided tag and items.</p>"},{"location":"code/R/modules/definitions/#unit_token_func","title":"<code>unit_token_func</code>","text":"<p>unit_token_func</p>"},{"location":"code/R/modules/definitions/#description_2","title":"Description","text":"<p>Takes a unit component type and a dictionary of flows, and returns a lambda function that extracts the default unit based on the specified component type from the flow dictionary.</p>"},{"location":"code/R/modules/definitions/#usage_2","title":"Usage","text":"<pre><code>unit_token_func(unit_component, flows)\n</code></pre>"},{"location":"code/R/modules/definitions/#arguments_2","title":"Arguments","text":"Argument Description <code>unit_component</code> Character. Specifies the type of unit token to be returned. Possible values are 'full', 'raw', 'variant'. <code>flows</code> List. Dictionary containing the flows."},{"location":"code/R/modules/definitions/#return-value_2","title":"Return Value","text":"<p>Function. Lambda function that takes a dictionary <code>def_specs</code> as input. The lambda function will return different values based on the <code>unit_component</code> parameter.</p>"},{"location":"code/R/modules/masking/","title":"masking","text":""},{"location":"code/R/modules/masking/#apply_cond","title":"<code>apply_cond</code>","text":"<p>apply_cond</p>"},{"location":"code/R/modules/masking/#description","title":"Description","text":"<p>Takes a pandas DataFrame and a condition, which can be a string, dictionary, or callable, and applies the condition to the DataFrame using <code>eval</code> or <code>apply</code> accordingly.</p>"},{"location":"code/R/modules/masking/#usage","title":"Usage","text":"<pre><code>apply_cond(df, cond)\n</code></pre>"},{"location":"code/R/modules/masking/#arguments","title":"Arguments","text":"Argument Description <code>df</code> DataFrame. A pandas DataFrame containing the data on which the condition will be applied. <code>cond</code> MaskCondition. The condition to be applied on the dataframe. Can be either a string, a dictionary, or a callable function."},{"location":"code/R/modules/masking/#return-value","title":"Return Value","text":"<p>DataFrame. Dataframe evaluated at the mask condition.</p>"},{"location":"code/R/modules/masking/#mask","title":"<code>Mask</code>","text":""},{"location":"code/R/modules/masking/#description_1","title":"Description","text":"<p>Class to define masks with conditions and weights to apply to DataFiles</p>"},{"location":"code/R/modules/masking/#methods","title":"Methods","text":""},{"location":"code/R/modules/masking/#public-methods","title":"Public Methods","text":"<ul> <li><code>Mask$new()</code></li> <li><code>Mask$matches()</code></li> <li><code>Mask$get_weights()</code></li> <li><code>Mask$clone()</code></li> </ul>"},{"location":"code/R/modules/masking/#method-new","title":"Method <code>new()</code>","text":"<p>Create a new mask object</p> <p>Usage</p> <pre><code>Mask$new(where = NULL, use = NULL, weight = NULL, other = NaN, comment = \"\")\n</code></pre> <p>Arguments:</p> <ul> <li><code>where</code> MaskCondition | listMaskCondition, optional. Where the mask should be applied.</li> <li><code>use</code> MaskCondition | listMaskCondition, optional. Condition on where to use the masks.</li> <li><code>weight</code> Numeric | Character | listNumeric | Character, optional. Weights to apply.</li> <li><code>other</code> Numeric, optional.</li> <li><code>comment</code> Character, optional. Comment.</li> </ul> <p></p>"},{"location":"code/R/modules/masking/#method-matches","title":"Method <code>matches()</code>","text":"<p>Check if a mask matches a dataframe by verifying if all 'where' conditions match across all rows.</p> <p>Usage</p> <pre><code>Mask$matches(df)\n</code></pre> <p>Arguments:</p> <ul> <li><code>df</code> DataFrame. Dataframe to check for matches.</li> </ul> <p>Returns:</p> <p>Logical. If the mask matches the dataframe.</p> <p></p>"},{"location":"code/R/modules/masking/#method-get_weights","title":"Method <code>get_weights()</code>","text":"<p>Apply weights to the dataframe</p> <p>Usage</p> <pre><code>Mask$get_weights(df)\n</code></pre> <p>Arguments:</p> <ul> <li><code>df</code> (<code>Dataframe</code>): Dataframe to apply weights on</li> </ul> <p>Returns:</p> <p>Dataframe. Dataframe with applied weights</p> <p></p>"},{"location":"code/R/modules/masking/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>Mask$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/masking/#read_masks","title":"<code>read_masks</code>","text":"<p>read_masks</p>"},{"location":"code/R/modules/masking/#description_2","title":"Description","text":"<p>Reads YAML files containing mask specifications from multiple databases and returns a list of Mask objects.</p>"},{"location":"code/R/modules/masking/#usage_1","title":"Usage","text":"<pre><code>read_masks(variable)\n</code></pre>"},{"location":"code/R/modules/masking/#arguments_1","title":"Arguments","text":"Argument Description <code>variable</code> Character. Variable to be read."},{"location":"code/R/modules/masking/#return-value_1","title":"Return Value","text":"<p>List. List with masks for the variable.</p>"},{"location":"code/R/modules/noslag/","title":"noslag","text":""},{"location":"code/R/modules/noslag/#collect_files","title":"<code>collect_files</code>","text":"<p>collect_files</p>"},{"location":"code/R/modules/noslag/#description","title":"Description","text":"<p>Takes a parent variable and optional list of databases to include, checks for their existence, and collects files and directories based on the parent variable.</p>"},{"location":"code/R/modules/noslag/#usage","title":"Usage","text":"<pre><code>collect_files(parent_variable, include_databases = NULL)\n</code></pre>"},{"location":"code/R/modules/noslag/#arguments","title":"Arguments","text":"Argument Description <code>parent_variable</code> Character. Variable to collect files on. <code>include_databases</code> Optional listCharacter. List of Database IDs to collect files from."},{"location":"code/R/modules/noslag/#return-value","title":"Return Value","text":"<p>List of tuples. List of tuples containing the parent variable and the database ID for each file found in the specified directories.</p>"},{"location":"code/R/modules/noslag/#examples","title":"Examples","text":"<pre><code>## Example usage:\ncollect_files(\"variable_name\", c(\"db1\", \"db2\"))\n</code></pre>"},{"location":"code/R/modules/noslag/#normalise_units","title":"<code>normalise_units</code>","text":"<p>normalise_units</p>"},{"location":"code/R/modules/noslag/#description_1","title":"Description","text":"<p>Takes a DataFrame with reported or reference data, along with dictionaries mapping variable units and flow IDs, and normalizes the units of the variables in the DataFrame based on the provided mappings.</p>"},{"location":"code/R/modules/noslag/#usage_1","title":"Usage","text":"<pre><code>normalise_units(df, level, var_units, var_flow_ids)\n</code></pre>"},{"location":"code/R/modules/noslag/#arguments_1","title":"Arguments","text":"Argument Description <code>df</code> DataFrame. Dataframe to be normalized. <code>level</code> Character. Specifies whether the data should be normalized on the reported or reference values. Possible values are 'reported' or 'reference'. <code>var_units</code> List. Dictionary that maps a combination of parent variable and variable to its corresponding unit. The keys in the dictionary are in the format \"{parent_variable} <code>var_flow_ids</code> List. Dictionary that maps a combination of parent variable and variable to a specific flow ID. This flow ID is used for unit conversion in the <code>normalize_units</code> function."},{"location":"code/R/modules/noslag/#return-value_1","title":"Return Value","text":"<p>DataFrame. Normalized dataframe.</p>"},{"location":"code/R/modules/noslag/#examples_1","title":"Examples","text":"<pre><code>## Example usage:\nnormalize_dataframe(df, \"reported\", var_units, var_flow_ids)\n</code></pre>"},{"location":"code/R/modules/noslag/#normalise_values","title":"<code>normalise_values</code>","text":"<p>normalise_values</p>"},{"location":"code/R/modules/noslag/#description_2","title":"Description","text":"<p>Takes a DataFrame as input, normalizes the 'value' and 'uncertainty' columns by the reference value, and updates the 'reference_value' column accordingly.</p>"},{"location":"code/R/modules/noslag/#usage_2","title":"Usage","text":"<pre><code>normalise_values(df)\n</code></pre>"},{"location":"code/R/modules/noslag/#arguments_2","title":"Arguments","text":"Argument Description <code>df</code> DataFrame. Dataframe to be normalized."},{"location":"code/R/modules/noslag/#return-value_2","title":"Return Value","text":"<p>DataFrame. Returns a modified DataFrame where the 'value' column has been divided by the 'reference_value' column (or 1.0 if 'reference_value' is null), the 'uncertainty' column has been divided by the 'reference_value' column, and the 'reference_value' column has been replaced with 1.0 if it was not null.</p>"},{"location":"code/R/modules/noslag/#examples_2","title":"Examples","text":"<pre><code>## Example usage:\nnormalized_df &lt;- normalize_values(df)\n</code></pre>"},{"location":"code/R/modules/noslag/#dataset","title":"<code>DataSet</code>","text":""},{"location":"code/R/modules/noslag/#description_3","title":"Description","text":"<p>This class provides methods to store, normalize, select, and aggregate DataSets.</p>"},{"location":"code/R/modules/noslag/#examples_3","title":"Examples","text":"<pre><code>### ------------------------------------------------\n### Method `DataSet$normalise`\n### ------------------------------------------------\n\n## Example usage:\ndataset$normalize(override = list(\"variable1\" = \"value1\"), inplace = FALSE)\n\n\n### ------------------------------------------------\n### Method `DataSet$select`\n### ------------------------------------------------\n\n## Example usage:\ndataset$select(override = list(\"variable1\" = \"value1\"), drop_singular_fields = TRUE, extrapolate_period = FALSE, field1 = \"value1\")\n\n\n### ------------------------------------------------\n### Method `DataSet$aggregate`\n### ------------------------------------------------\n\n## Example usage:\ndataset$aggregate(override = list(\"variable1\" = \"value1\"), drop_singular_fields = TRUE, extrapolate_period = FALSE, agg = \"field\", masks = list(mask1, mask2), masks_database = TRUE)\n</code></pre>"},{"location":"code/R/modules/noslag/#methods","title":"Methods","text":""},{"location":"code/R/modules/noslag/#public-methods","title":"Public Methods","text":"<ul> <li><code>DataSet$new()</code></li> <li><code>DataSet$normalise()</code></li> <li><code>DataSet$select()</code></li> <li><code>DataSet$aggregate()</code></li> <li><code>DataSet$clone()</code></li> </ul>"},{"location":"code/R/modules/noslag/#method-new","title":"Method <code>new()</code>","text":"<p>Create new instance of the DataSet class</p> <p>Usage</p> <pre><code>DataSet$new(\n  parent_variable,\n  include_databases = NULL,\n  file_paths = NULL,\n  check_inconsistencies = FALSE,\n  data = NULL\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>parent_variable</code> Character. Variable to collect Data on.</li> <li><code>include_databases</code> Optional listCharacter | tupleCharacter, optional. Databases to load from.</li> <li><code>file_paths</code> Optional listCharacter, optional. Paths to load data from.</li> <li><code>check_inconsistencies</code> Logical, optional. Whether to check for inconsistencies.</li> <li><code>data</code> Optional DataFrame, optional. Specific data to include in the dataset.</li> </ul> <p></p>"},{"location":"code/R/modules/noslag/#method-normalise","title":"Method <code>normalise()</code>","text":"<p>Normalize data: default reference units, reference value equal to 1.0, default reported units</p> <p>Usage</p> <pre><code>DataSet$normalise(override = NULL, inplace = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>override</code> Optional listCharacter. Dictionary with key, value pairs of variables to override.</li> <li><code>inplace</code> Logical, optional. Whether to do the normalization in place.</li> </ul> <p>Example:</p> <pre><code>## Example usage:\ndataset$normalize(override = list(\"variable1\" = \"value1\"), inplace = FALSE)\n</code></pre> <p>Returns:</p> <p>DataFrame. If <code>inplace</code> is <code>FALSE</code>, returns normalized dataframe.</p> <p></p>"},{"location":"code/R/modules/noslag/#method-select","title":"Method <code>select()</code>","text":"<p>Select desired data from the dataframe</p> <p>Usage</p> <pre><code>DataSet$select(\n  override = NULL,\n  drop_singular_fields = TRUE,\n  extrapolate_period = TRUE,\n  ...\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>override</code> Optional listCharacter. Dictionary with key, value pairs of variables to override.</li> <li><code>drop_singular_fields</code> Logical, optional. If <code>TRUE</code>, drop custom fields with only one value.</li> <li><code>extrapolate_period</code> Logical, optional. If <code>TRUE</code>, extrapolate values if no value for this period is given.</li> <li><code>...</code> IDs of values to select.</li> </ul> <p>Example:</p> <pre><code>## Example usage:\ndataset$select(override = list(\"variable1\" = \"value1\"), drop_singular_fields = TRUE, extrapolate_period = FALSE, field1 = \"value1\")\n</code></pre> <p>Returns:</p> <p>DataFrame. DataFrame with selected values.</p> <p></p>"},{"location":"code/R/modules/noslag/#method-aggregate","title":"Method <code>aggregate()</code>","text":"<p>Aggregates data based on specified parameters, applies masks, and cleans up the resulting DataFrame.</p> <p>Usage</p> <pre><code>DataSet$aggregate(\n  override = NULL,\n  drop_singular_fields = TRUE,\n  extrapolate_period = TRUE,\n  agg = NULL,\n  masks = NULL,\n  masks_database = TRUE,\n  ...\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>override</code> Optional listCharacter. Dictionary with key, value pairs of variables to override.</li> <li><code>drop_singular_fields</code> Logical, optional. If <code>TRUE</code>, drop custom fields with only one value.</li> <li><code>extrapolate_period</code> Logical, optional. If <code>TRUE</code>, extrapolate values if no value for this period is given.</li> <li><code>agg</code> Optional Character | listCharacter | tupleCharacter. Specifies which fields to aggregate over.</li> <li><code>masks</code> Optional listMask. Specifies a list of Mask objects that will be applied to the data during aggregation. These masks can be used to filter or weight the data based on certain conditions defined in the Mask objects.</li> <li><code>masks_database</code> Logical, optional. Determines whether to include masks from databases in the aggregation process. If <code>TRUE</code>, masks from databases will be included along with any masks provided as function arguments. If <code>FALSE</code>, only the masks provided as function arguments will be applied.</li> <li><code>...</code> additional field vals</li> </ul> <p>Example:</p> <pre><code>## Example usage:\ndataset$aggregate(override = list(\"variable1\" = \"value1\"), drop_singular_fields = TRUE, extrapolate_period = FALSE, agg = \"field\", masks = list(mask1, mask2), masks_database = TRUE)\n</code></pre> <p>Returns:</p> <p>DataFrame. The <code>aggregate</code> method returns a pandas DataFrame that has been cleaned up and aggregated based on the specified parameters and input data. The method performs aggregation over component fields and case fields, applies weights based on masks, drops rows with NaN weights, aggregates with weights, inserts reference variables, sorts columns and rows, rounds values, and inserts units before returning the final cleaned and aggregated DataFrame.</p> <p></p>"},{"location":"code/R/modules/noslag/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>DataSet$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/read/","title":"read","text":""},{"location":"code/R/modules/read/#read_csv_file","title":"<code>read_csv_file</code>","text":"<p>read_csv_file</p>"},{"location":"code/R/modules/read/#description","title":"Description","text":"<p>Read a csv datafile</p>"},{"location":"code/R/modules/read/#usage","title":"Usage","text":"<pre><code>read_csv_file(fpath)\n</code></pre>"},{"location":"code/R/modules/read/#arguments","title":"Arguments","text":"Argument Description <code>fpath</code> path of the csv file"},{"location":"code/R/modules/tedf/","title":"tedf","text":""},{"location":"code/R/modules/tedf/#tedfinconsistencyexception","title":"<code>TEDFInconsistencyException</code>","text":""},{"location":"code/R/modules/tedf/#description","title":"Description","text":"<p>This is a class to store inconsistencies in the TEDFs</p>"},{"location":"code/R/modules/tedf/#methods","title":"Methods","text":""},{"location":"code/R/modules/tedf/#public-methods","title":"Public Methods","text":"<ul> <li><code>TEDFInconsistencyException$new()</code></li> <li><code>TEDFInconsistencyException$clone()</code></li> </ul>"},{"location":"code/R/modules/tedf/#method-new","title":"Method <code>new()</code>","text":"<p>Create instance of TEDFInconsistencyException class</p> <p>Usage</p> <pre><code>TEDFInconsistencyException$new(\n  message = \"Inconsistency detected\",\n  row_id = NULL,\n  col_id = NULL,\n  file_path = NULL\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>message</code> (<code>character</code>) the message of the exception</li> <li><code>row_id</code> Id of the row</li> <li><code>col_id</code> Id of the column</li> <li><code>file_path</code> file path</li> </ul> <p></p>"},{"location":"code/R/modules/tedf/#method-clone","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>TEDFInconsistencyException$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/tedf/#tebase","title":"<code>TEBase</code>","text":""},{"location":"code/R/modules/tedf/#description_1","title":"Description","text":"<p>This is the base class for technoeconomic data.</p>"},{"location":"code/R/modules/tedf/#examples","title":"Examples","text":"<pre><code>## Example usage:\nbase_technoeconomic_data &lt;- TEBase$new(\"variable_name\")\n</code></pre>"},{"location":"code/R/modules/tedf/#methods_1","title":"Methods","text":""},{"location":"code/R/modules/tedf/#public-methods_1","title":"Public Methods","text":"<ul> <li><code>TEBase$new()</code></li> <li><code>TEBase$clone()</code></li> </ul>"},{"location":"code/R/modules/tedf/#method-new_1","title":"Method <code>new()</code>","text":"<p>Create new instance of TEBase class. Set parent variable and technology specifications (var_specs) from input</p> <p>Usage</p> <pre><code>TEBase$new(parent_variable)\n</code></pre> <p>Arguments:</p> <ul> <li><code>parent_variable</code> (<code>character</code>) Name of the parent variable</li> </ul> <p></p>"},{"location":"code/R/modules/tedf/#method-clone_1","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>TEBase$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/tedf/#tedf","title":"<code>TEDF</code>","text":""},{"location":"code/R/modules/tedf/#description_2","title":"Description","text":"<p>This class is used to store Technoeconomic DataFiles.</p>"},{"location":"code/R/modules/tedf/#examples_1","title":"Examples","text":"<pre><code>## Example usage:\ntedf &lt;- TEDF$new(\"variable_name\")\ntedf$load()\ntedf$read(\"file_path.csv\")\ntedf$write(\"output_file_path.csv\")\ntedf$check()\ntedf$check_row()\n\n\n### ------------------------------------------------\n### Method `TEDF$load`\n### ------------------------------------------------\n\n## Example usage:\ntedf$load()\n\n\n### ------------------------------------------------\n### Method `TEDF$read`\n### ------------------------------------------------\n\n## Example usage:\ntedf$read()\n\n\n### ------------------------------------------------\n### Method `TEDF$write`\n### ------------------------------------------------\n\n## Example usage:\ntedf$write()\n\n\n### ------------------------------------------------\n### Method `TEDF$check`\n### ------------------------------------------------\n\n## Example usage:\ntedf$check(raise_exception = TRUE)\n</code></pre>"},{"location":"code/R/modules/tedf/#methods_2","title":"Methods","text":""},{"location":"code/R/modules/tedf/#public-methods_2","title":"Public Methods","text":"<ul> <li><code>TEDF$new()</code></li> <li><code>TEDF$load()</code></li> <li><code>TEDF$read()</code></li> <li><code>TEDF$write()</code></li> <li><code>TEDF$check()</code></li> <li><code>TEDF$check_row()</code></li> <li><code>TEDF$clone()</code></li> </ul>"},{"location":"code/R/modules/tedf/#method-new_2","title":"Method <code>new()</code>","text":"<p>Create new instance of TEDF class. Initialise parent class and object fields</p> <p>Usage</p> <pre><code>TEDF$new(\n  parent_variable,\n  database_id = \"public\",\n  file_path = NULL,\n  data = NULL\n)\n</code></pre> <p>Arguments:</p> <ul> <li><code>parent_variable</code> (<code>Character</code>): Variable from which data should be collected.</li> <li><code>database_id</code> (<code>Character</code>):, default: \"public\". Database from which to load data.</li> <li><code>file_path</code> (<code>Path</code>):, optional. File path from which to load file.</li> <li><code>data</code> (<code>DataFrame</code>):, optional. Specific Technoeconomic data.</li> </ul> <p></p>"},{"location":"code/R/modules/tedf/#method-load","title":"Method <code>load()</code>","text":"<p>Load TEDataFile (only if it has not been read yet)</p> <p>Usage</p> <pre><code>TEDF$load()\n</code></pre> <p>Example:</p> <pre><code>## Example usage:\ntedf$load()\n</code></pre> <p>Returns:</p> <p>TEDF. Returns the TEDF object it is called on.</p> <p></p>"},{"location":"code/R/modules/tedf/#method-read","title":"Method <code>read()</code>","text":"<p>This method reads TEDF from a CSV file.</p> <p>Usage</p> <pre><code>TEDF$read()\n</code></pre> <p>Example:</p> <pre><code>## Example usage:\ntedf$read()\n</code></pre> <p></p>"},{"location":"code/R/modules/tedf/#method-write","title":"Method <code>write()</code>","text":"<p>write TEDF to CSV file.</p> <p>Usage</p> <pre><code>TEDF$write()\n</code></pre> <p>Example:</p> <pre><code>## Example usage:\ntedf$write()\n</code></pre> <p></p>"},{"location":"code/R/modules/tedf/#method-check","title":"Method <code>check()</code>","text":"<p>Check that TEDF is consistent and add inconsistencies to internal parameter</p> <p>Usage</p> <pre><code>TEDF$check(raise_exception = TRUE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>raise_exception</code> Logical, default: TRUE. If an exception is to be raised.</li> </ul> <p>Example:</p> <pre><code>## Example usage:\ntedf$check(raise_exception = TRUE)\n</code></pre> <p></p>"},{"location":"code/R/modules/tedf/#method-check_row","title":"Method <code>check_row()</code>","text":"<p>checks if row of dataframe has issues - NOT IMPLEMENTED YET</p> <p>Usage</p> <pre><code>TEDF$check_row(row_id, raise_exception = TRUE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>row_id</code> Id of the row</li> <li><code>raise_exception</code> (<code>logical</code>) If exception is to be raised</li> </ul> <p></p>"},{"location":"code/R/modules/tedf/#method-clone_2","title":"Method <code>clone()</code>","text":"<p>The objects of this class are cloneable with this method.</p> <p>Usage</p> <pre><code>TEDF$clone(deep = FALSE)\n</code></pre> <p>Arguments:</p> <ul> <li><code>deep</code> Whether to make a deep clone.</li> </ul>"},{"location":"code/R/modules/units/","title":"units","text":""},{"location":"code/R/modules/units/#unit_convert","title":"<code>unit_convert</code>","text":"<p>unit_convert</p>"},{"location":"code/R/modules/units/#description","title":"Description","text":"<p>Converts units with optional flow context handling based on specified variants and flow ID. The function checks if the input units are not NaN, then it proceeds to handle different cases based on the presence of a flow context and unit variants.</p>"},{"location":"code/R/modules/units/#usage","title":"Usage","text":"<pre><code>unit_convert(unit_from, unit_to, flow_id = NULL)\n</code></pre>"},{"location":"code/R/modules/units/#arguments","title":"Arguments","text":"Argument Description <code>unit_from</code> Character or numeric. Unit to convert from. <code>unit_to</code> Character or numeric. Unit to convert to. <code>flow_id</code> Character or NULL. Identifier for the specific flow or process."},{"location":"code/R/modules/units/#return-value","title":"Return Value","text":"<p>Numeric. Conversion factor between <code>unit_from</code> and <code>unit_to</code>.</p>"},{"location":"code/R/modules/units/#examples","title":"Examples","text":"<pre><code>## Example usage:\nunit_convert(\"m\", \"km\", flow_id = NULL)\n</code></pre>"},{"location":"code/python/columns/","title":"columns","text":""},{"location":"code/python/columns/#python.posted.columns.AbstractColumnDefinition","title":"<code>AbstractColumnDefinition</code>","text":"<p>Abstract class to store columns</p> <p>Parameters:</p> Name Type Description Default <code>col_type</code> <code>str</code> <p>Type of the column</p> required <code>name</code> <code>str</code> <p>Name of the column</p> required <code>description</code> <code>str</code> <p>Description of the column</p> required <code>dtype</code> <code>str</code> <p>Data type of the column</p> required <code>required</code> <code>bool</code> <p>Bool that specifies if the column is required</p> required <p>Methods:</p> Name Description <code>    is_allowed</code> <p>Check if cell is allowed</p> Source code in <code>python/posted/columns.py</code> <pre><code>class AbstractColumnDefinition:\n    '''\n    Abstract class to store columns\n\n    Parameters\n    ----------\n    col_type: str\n        Type of the column\n    name: str\n        Name of the column\n    description: str\n        Description of the column\n    dtype:\n        Data type of the column\n    required: bool\n        Bool that specifies if the column is required\n\n    Methods\n    -------\n        is_allowed\n            Check if cell is allowed\n    '''\n    def __init__(self, col_type: str, name: str, description: str, dtype: str, required: bool):\n        if col_type not in ['field', 'variable', 'unit', 'value', 'comment']:\n            raise Exception(f\"Columns must be of type field, variable, unit, value, or comment but found: {col_type}\")\n        if not isinstance(name, str):\n            raise Exception(f\"The 'name' must be a string but found type {type(name)}: {name}\")\n        if not isinstance(description, str):\n            raise Exception(f\"The 'name' must be a string but found type {type(description)}: {description}\")\n        if not (isinstance(dtype, str) and dtype in ['float', 'str', 'category']):\n            raise Exception(f\"The 'dtype' must be a valid data type but found: {dtype}\")\n        if not isinstance(required, bool):\n            raise Exception(f\"The 'required' argument must be a bool but found: {required}\")\n\n        self._col_type: str = col_type\n        self._name: str = name\n        self._description: str = description\n        self._dtype: str = dtype\n        self._required: bool = required\n\n    @property\n    def col_type(self):\n        '''Get col type'''\n        return self._col_type\n\n    @property\n    def name(self):\n        '''Get name of the column'''\n        return self._name\n\n    @property\n    def description(self):\n        '''Get description of the column'''\n        return self._description\n\n    @property\n    def dtype(self):\n        '''Get data type of the column'''\n        return self._dtype\n\n    @property\n    def required(self):\n        '''Return if column is required'''\n        return self._required\n\n    @property\n    def default(self):\n        '''Get default value of the column'''\n        return np.nan\n\n    def is_allowed(self, cell: str | float | int) -&gt; bool:\n        '''Check if Cell is allowed\n\n        Parameters\n        ----------\n            cell: str | float | int\n                Cell to check\n        Returns\n        -------\n            bool\n                If the cell is allowed\n        '''\n        return True\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.AbstractColumnDefinition.col_type","title":"<code>col_type</code>  <code>property</code>","text":"<p>Get col type</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractColumnDefinition.default","title":"<code>default</code>  <code>property</code>","text":"<p>Get default value of the column</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractColumnDefinition.description","title":"<code>description</code>  <code>property</code>","text":"<p>Get description of the column</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractColumnDefinition.dtype","title":"<code>dtype</code>  <code>property</code>","text":"<p>Get data type of the column</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractColumnDefinition.name","title":"<code>name</code>  <code>property</code>","text":"<p>Get name of the column</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractColumnDefinition.required","title":"<code>required</code>  <code>property</code>","text":"<p>Return if column is required</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractColumnDefinition.is_allowed","title":"<code>is_allowed(cell)</code>","text":"<p>Check if Cell is allowed</p> <p>Returns:</p> Type Description <code>    bool</code> <p>If the cell is allowed</p> Source code in <code>python/posted/columns.py</code> <pre><code>def is_allowed(self, cell: str | float | int) -&gt; bool:\n    '''Check if Cell is allowed\n\n    Parameters\n    ----------\n        cell: str | float | int\n            Cell to check\n    Returns\n    -------\n        bool\n            If the cell is allowed\n    '''\n    return True\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.AbstractFieldDefinition","title":"<code>AbstractFieldDefinition</code>","text":"<p>               Bases: <code>AbstractColumnDefinition</code></p> <p>Abstract class to store fields</p> <p>Parameters:</p> Name Type Description Default <code>field_type</code> <code>str</code> <p>Type of the field</p> required <code>name</code> <code>str</code> <p>Name of the field</p> required <code>description</code> <code>str</code> <p>Description of the field</p> required <code>dtype</code> <code>str</code> <p>Data type of the field</p> required <code>coded</code> <code>bool</code> <p>If the field is coded</p> required <code>coded</code> <code>bool</code> <p>Codes for the field</p> required <p>Methods:</p> Name Description <code>is_allowed</code> <p>Check if cell is allowed</p> <code>select_and_expand</code> <p>Select and expand fields</p> Source code in <code>python/posted/columns.py</code> <pre><code>class AbstractFieldDefinition(AbstractColumnDefinition):\n    '''\n    Abstract class to store fields\n\n    Parameters\n    ----------\n    field_type: str\n        Type of the field\n    name: str\n        Name of the field\n    description: str\n        Description of the field\n    dtype: str\n        Data type of the field\n    coded: bool\n        If the field is coded\n    coded: Optional[dict[str,str]], optional\n        Codes for the field\n\n\n    Methods\n    -------\n    is_allowed\n        Check if cell is allowed\n    select_and_expand\n        Select and expand fields\n\n    '''\n    def __init__(self, field_type: str, name: str, description: str, dtype: str, coded: bool,\n                 codes: Optional[dict[str, str]] = None):\n        if field_type not in ['case', 'component']:\n            raise Exception('Fields must be of type case or component.')\n        super().__init__(\n            col_type='field',\n            name=name,\n            description=description,\n            dtype=dtype,\n            required=True,\n        )\n\n        self._field_type: str = field_type\n        self._coded: bool = coded\n        self._codes: None | dict[str, str] = codes\n\n    @property\n    def field_type(self) -&gt; str:\n        '''Get field type'''\n        return self._field_type\n\n    @property\n    def coded(self) -&gt; bool:\n        '''Return if field is coded'''\n        return self._coded\n\n    @property\n    def codes(self) -&gt; None | dict[str, str]:\n        '''Get field codes'''\n        return self._codes\n\n    @property\n    def default(self):\n        '''Get symbol for default value'''\n        return '*' if self._field_type == 'case' else '#'\n\n    def is_allowed(self, cell: str | float | int) -&gt; bool:\n        ''' Chek if cell is allowed'''\n        if pd.isnull(cell):\n            return False\n        if self._coded:\n            return cell in self._codes or cell == '*' or (cell == '#' and self.col_type == 'component')\n        else:\n            return True\n\n    def _expand(self, df: pd.DataFrame, col_id: str, field_vals: list, **kwargs) -&gt; pd.DataFrame:\n        # Expand fields\n        return pd.concat([\n            df[df[col_id].isin(field_vals)],\n            df[df[col_id] == '*']\n            .drop(columns=[col_id])\n            .merge(pd.DataFrame.from_dict({col_id: field_vals}), how='cross'),\n        ])\n\n    def _select(self, df: pd.DataFrame, col_id: str, field_vals: list, **kwargs):\n        # Select fields\n        return df.query(f\"{col_id}.isin({field_vals})\").reset_index(drop=True)\n\n\n    def select_and_expand(self, df: pd.DataFrame, col_id: str, field_vals: None | list, **kwargs) -&gt; pd.DataFrame:\n        '''\n        Select and expand fields which are valid for multiple periods or other field vals\n\n        Parameters\n        ----------\n        df: pd.DataFrame\n            DataFrame where fields should be selected and expanded\n        col_id: str\n            col_id of the column to be selected and expanded\n        field_vals: None | list\n            field_vals to select and expand\n        **kwargs\n            Additional keyword arguments\n\n        Returns\n        -------\n        pd.DataFrame\n            Dataframe where fields are selected and expanded\n\n        '''\n        # get list of selected field values\n        if field_vals is None:\n            if col_id == 'period':\n                field_vals = default_periods\n            elif self._coded:\n                field_vals = list(self._codes.keys())\n            else:\n                field_vals = [v for v in df[col_id].unique() if v != '*' and not pd.isnull(v)]\n        else:\n            # ensure that field values is a list of elements (not tuple, not single value)\n            if isinstance(field_vals, tuple):\n                field_vals = list(field_vals)\n            elif not isinstance(field_vals, list):\n                field_vals = [field_vals]\n            # check that every element is of allowed type\n            for val in field_vals:\n                if not self.is_allowed(val):\n                    raise Exception(f\"Invalid type selected for field '{col_id}': {val}\")\n            if '*' in field_vals:\n                raise Exception(f\"Selected values for field '{col_id}' must not contain the asterisk.\"\n                                f\"Omit the '{col_id}' argument to select all entries.\")\n\n\n        df = self._expand(df, col_id, field_vals, **kwargs)\n        df = self._select(df, col_id, field_vals, **kwargs)\n\n        return df\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.AbstractFieldDefinition.coded","title":"<code>coded: bool</code>  <code>property</code>","text":"<p>Return if field is coded</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractFieldDefinition.codes","title":"<code>codes: None | dict[str, str]</code>  <code>property</code>","text":"<p>Get field codes</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractFieldDefinition.default","title":"<code>default</code>  <code>property</code>","text":"<p>Get symbol for default value</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractFieldDefinition.field_type","title":"<code>field_type: str</code>  <code>property</code>","text":"<p>Get field type</p>"},{"location":"code/python/columns/#python.posted.columns.AbstractFieldDefinition.is_allowed","title":"<code>is_allowed(cell)</code>","text":"<p>Chek if cell is allowed</p> Source code in <code>python/posted/columns.py</code> <pre><code>def is_allowed(self, cell: str | float | int) -&gt; bool:\n    ''' Chek if cell is allowed'''\n    if pd.isnull(cell):\n        return False\n    if self._coded:\n        return cell in self._codes or cell == '*' or (cell == '#' and self.col_type == 'component')\n    else:\n        return True\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.AbstractFieldDefinition.select_and_expand","title":"<code>select_and_expand(df, col_id, field_vals, **kwargs)</code>","text":"<p>Select and expand fields which are valid for multiple periods or other field vals</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame where fields should be selected and expanded</p> required <code>col_id</code> <code>str</code> <p>col_id of the column to be selected and expanded</p> required <code>field_vals</code> <code>None | list</code> <p>field_vals to select and expand</p> required <code>**kwargs</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe where fields are selected and expanded</p> Source code in <code>python/posted/columns.py</code> <pre><code>def select_and_expand(self, df: pd.DataFrame, col_id: str, field_vals: None | list, **kwargs) -&gt; pd.DataFrame:\n    '''\n    Select and expand fields which are valid for multiple periods or other field vals\n\n    Parameters\n    ----------\n    df: pd.DataFrame\n        DataFrame where fields should be selected and expanded\n    col_id: str\n        col_id of the column to be selected and expanded\n    field_vals: None | list\n        field_vals to select and expand\n    **kwargs\n        Additional keyword arguments\n\n    Returns\n    -------\n    pd.DataFrame\n        Dataframe where fields are selected and expanded\n\n    '''\n    # get list of selected field values\n    if field_vals is None:\n        if col_id == 'period':\n            field_vals = default_periods\n        elif self._coded:\n            field_vals = list(self._codes.keys())\n        else:\n            field_vals = [v for v in df[col_id].unique() if v != '*' and not pd.isnull(v)]\n    else:\n        # ensure that field values is a list of elements (not tuple, not single value)\n        if isinstance(field_vals, tuple):\n            field_vals = list(field_vals)\n        elif not isinstance(field_vals, list):\n            field_vals = [field_vals]\n        # check that every element is of allowed type\n        for val in field_vals:\n            if not self.is_allowed(val):\n                raise Exception(f\"Invalid type selected for field '{col_id}': {val}\")\n        if '*' in field_vals:\n            raise Exception(f\"Selected values for field '{col_id}' must not contain the asterisk.\"\n                            f\"Omit the '{col_id}' argument to select all entries.\")\n\n\n    df = self._expand(df, col_id, field_vals, **kwargs)\n    df = self._select(df, col_id, field_vals, **kwargs)\n\n    return df\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.CommentDefinition","title":"<code>CommentDefinition</code>","text":"<p>               Bases: <code>AbstractColumnDefinition</code></p> <p>Class to store comment columns</p> <p>Parameters:</p> Name Type Description Default <code>col_type</code> <p>Type of the column</p> required <code>name</code> <code>str</code> <p>Name of the column</p> required <code>description</code> <code>str</code> <p>Description of the column</p> required <code>required</code> <code>bool</code> <p>Bool that specifies if the column is required</p> required <p>Methods:</p> Name Description <code>is_allowed</code> <p>Check if cell is allowed</p> Source code in <code>python/posted/columns.py</code> <pre><code>class CommentDefinition(AbstractColumnDefinition):\n    '''\n    Class to store comment columns\n\n    Parameters\n    ----------\n    col_type: str\n        Type of the column\n    name: str\n        Name of the column\n    description: str\n        Description of the column\n    required: bool\n        Bool that specifies if the column is required\n\n    Methods\n    -------\n    is_allowed\n        Check if cell is allowed\n    '''\n    def __init__(self, name: str, description: str, required: bool):\n        super().__init__(\n            col_type='comment',\n            name=name,\n            description=description,\n            dtype='str',\n            required=required,\n        )\n\n    def is_allowed(self, cell: str | float | int) -&gt; bool:\n        return True\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.CustomFieldDefinition","title":"<code>CustomFieldDefinition</code>","text":"<p>               Bases: <code>AbstractFieldDefinition</code></p> <p>Class to store Custom fields</p> <p>Parameters:</p> Name Type Description Default <code>**field_specs</code> <p>Specs of the custom fields</p> <code>{}</code> Source code in <code>python/posted/columns.py</code> <pre><code>class CustomFieldDefinition(AbstractFieldDefinition):\n    '''\n    Class to store Custom fields\n\n    Parameters\n    ----------\n    **field_specs:\n        Specs of the custom fields\n    '''\n    def __init__(self, **field_specs):\n        '''Check if the field specs are of the required type and format,\n        initialize parent class'''\n        if not ('type' in field_specs and isinstance(field_specs['type'], str) and\n                field_specs['type'] in ['case', 'component']):\n            raise Exception(\"Field type must be provided and equal to 'case' or 'component'.\")\n        if not ('name' in field_specs and isinstance(field_specs['name'], str)):\n            raise Exception('Field name must be provided and of type string.')\n        if not ('description' in field_specs and isinstance(field_specs['description'], str)):\n            raise Exception('Field description must be provided and of type string.')\n        if not ('coded' in field_specs and isinstance(field_specs['coded'], bool)):\n            raise Exception('Field coded must be provided and of type bool.')\n        if field_specs['coded'] and not ('codes' in field_specs and isinstance(field_specs['codes'], dict)):\n            raise Exception('Field codes must be provided and contain a dict of possible codes.')\n\n        super().__init__(\n            field_type=field_specs['type'],\n            name=field_specs['name'],\n            description=field_specs['description'],\n            dtype='category',\n            coded=field_specs['coded'],\n            codes=field_specs['codes'] if 'codes' in field_specs else None,\n        )\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.CustomFieldDefinition.__init__","title":"<code>__init__(**field_specs)</code>","text":"<p>Check if the field specs are of the required type and format, initialize parent class</p> Source code in <code>python/posted/columns.py</code> <pre><code>def __init__(self, **field_specs):\n    '''Check if the field specs are of the required type and format,\n    initialize parent class'''\n    if not ('type' in field_specs and isinstance(field_specs['type'], str) and\n            field_specs['type'] in ['case', 'component']):\n        raise Exception(\"Field type must be provided and equal to 'case' or 'component'.\")\n    if not ('name' in field_specs and isinstance(field_specs['name'], str)):\n        raise Exception('Field name must be provided and of type string.')\n    if not ('description' in field_specs and isinstance(field_specs['description'], str)):\n        raise Exception('Field description must be provided and of type string.')\n    if not ('coded' in field_specs and isinstance(field_specs['coded'], bool)):\n        raise Exception('Field coded must be provided and of type bool.')\n    if field_specs['coded'] and not ('codes' in field_specs and isinstance(field_specs['codes'], dict)):\n        raise Exception('Field codes must be provided and contain a dict of possible codes.')\n\n    super().__init__(\n        field_type=field_specs['type'],\n        name=field_specs['name'],\n        description=field_specs['description'],\n        dtype='category',\n        coded=field_specs['coded'],\n        codes=field_specs['codes'] if 'codes' in field_specs else None,\n    )\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.PeriodFieldDefinition","title":"<code>PeriodFieldDefinition</code>","text":"<p>               Bases: <code>AbstractFieldDefinition</code></p> <p>Class to store Period fields</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the field</p> required <code>description</code> <code>str</code> <p>Description of the field</p> required <p>Methods:</p> Name Description <code>is_allowed</code> <p>Checks if cell is allowed</p> Source code in <code>python/posted/columns.py</code> <pre><code>class PeriodFieldDefinition(AbstractFieldDefinition):\n    '''\n    Class to store Period fields\n\n    Parameters\n    ----------\n    name: str\n        Name of the field\n    description: str\n        Description of the field\n\n    Methods\n    -------\n    is_allowed\n        Checks if cell is allowed\n    '''\n    def __init__(self, name: str, description: str):\n        '''Initialize parent class'''\n        super().__init__(\n            field_type='case',\n            name=name,\n            description=description,\n            dtype='float',\n            coded=False,\n        )\n\n    def is_allowed(self, cell: str | float | int) -&gt; bool:\n        '''Check if cell is a flowat or *'''\n        return is_float(cell) or cell == '*'\n\n    def _expand(self, df: pd.DataFrame, col_id: str, field_vals: list, **kwargs) -&gt; pd.DataFrame:\n        return pd.concat([\n            df[df[col_id] != '*'],\n            df[df[col_id] == '*']\n            .drop(columns=[col_id])\n            .merge(pd.DataFrame.from_dict({col_id: field_vals}), how='cross'),\n        ]).astype({'period': 'float'})\n\n\n    def _select(self, df: pd.DataFrame, col_id: str, field_vals: list[int | float], **kwargs) -&gt; pd.DataFrame:\n        # group by identifying columns and select periods/generate time series\n        # get list of groupable columns\n        group_cols = [\n            c for c in df.columns\n            if c not in [col_id, 'value']\n        ]\n\n        # perform groupby and do not drop NA values\n        grouped = df.groupby(group_cols, dropna=False)\n\n        # create return list\n        ret = []\n\n        # loop over groups\n        for keys, rows in grouped:\n            # get rows in group\n            rows = rows[[col_id, 'value']]\n\n            # get a list of periods that exist\n            periods_exist = rows[col_id].unique()\n\n            # create dataframe containing rows for all requested periods\n            req_rows = pd.DataFrame.from_dict({\n                f\"{col_id}\": field_vals,\n                f\"{col_id}_upper\": [min([ip for ip in periods_exist if ip &gt;= p], default=np.nan) for p in field_vals],\n                f\"{col_id}_lower\": [max([ip for ip in periods_exist if ip &lt;= p], default=np.nan) for p in field_vals],\n            })\n\n            # set missing columns from group\n            req_rows[group_cols] = keys\n\n            # check case\n            cond_match = req_rows[col_id].isin(periods_exist)\n            cond_extrapolate = (req_rows[f\"{col_id}_upper\"].isna() | req_rows[f\"{col_id}_lower\"].isna())\n\n            # match\n            rows_match = req_rows.loc[cond_match] \\\n                .merge(rows, on=col_id)\n\n            # extrapolate\n            rows_extrapolate = (\n                req_rows.loc[~cond_match &amp; cond_extrapolate]\n                    .assign(\n                        period_combined=lambda x: np.where(\n                            x.notna()[f\"{col_id}_upper\"],\n                            x[f\"{col_id}_upper\"],\n                            x[f\"{col_id}_lower\"],\n                        )\n                    )\n                    .merge(rows.rename(columns={col_id: f\"{col_id}_combined\"}), on=f\"{col_id}_combined\")\n                if 'extrapolate_period' not in kwargs or kwargs['extrapolate_period'] else\n                pd.DataFrame()\n            )\n\n            # interpolate\n            rows_interpolate = req_rows.loc[~cond_match &amp; ~cond_extrapolate] \\\n                .merge(rows.rename(columns={c: f\"{c}_upper\" for c in rows.columns}), on=f\"{col_id}_upper\") \\\n                .merge(rows.rename(columns={c: f\"{c}_lower\" for c in rows.columns}), on=f\"{col_id}_lower\") \\\n                .assign(value=lambda row: row['value_lower'] + (row[f\"{col_id}_upper\"] - row[col_id]) /\n                                          (row[f\"{col_id}_upper\"] - row[f\"{col_id}_lower\"]) * (row['value_upper'] - row['value_lower']))\n\n            # combine into one dataframe and drop unused columns\n            rows_to_concat = [df for df in [rows_match, rows_extrapolate, rows_interpolate] if not df.empty]\n            if rows_to_concat:\n                rows_append = pd.concat(rows_to_concat)\n                rows_append.drop(columns=[\n                        c for c in [f\"{col_id}_upper\", f\"{col_id}_lower\", f\"{col_id}_combined\", 'value_upper', 'value_lower']\n                        if c in rows_append.columns\n                    ], inplace=True)\n\n                # add to return list\n                ret.append(rows_append)\n\n        # convert return list to dataframe and return\n        return pd.concat(ret).reset_index(drop=True) if ret else df.iloc[[]]\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.PeriodFieldDefinition.__init__","title":"<code>__init__(name, description)</code>","text":"<p>Initialize parent class</p> Source code in <code>python/posted/columns.py</code> <pre><code>def __init__(self, name: str, description: str):\n    '''Initialize parent class'''\n    super().__init__(\n        field_type='case',\n        name=name,\n        description=description,\n        dtype='float',\n        coded=False,\n    )\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.PeriodFieldDefinition.is_allowed","title":"<code>is_allowed(cell)</code>","text":"<p>Check if cell is a flowat or *</p> Source code in <code>python/posted/columns.py</code> <pre><code>def is_allowed(self, cell: str | float | int) -&gt; bool:\n    '''Check if cell is a flowat or *'''\n    return is_float(cell) or cell == '*'\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.RegionFieldDefinition","title":"<code>RegionFieldDefinition</code>","text":"<p>               Bases: <code>AbstractFieldDefinition</code></p> <p>Class to store Region fields</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the field</p> required <code>description</code> <code>str</code> <p>Description of the field</p> required Source code in <code>python/posted/columns.py</code> <pre><code>class RegionFieldDefinition(AbstractFieldDefinition):\n    '''\n    Class to store Region fields\n\n    Parameters\n    ----------\n    name: str\n        Name of the field\n    description: str\n        Description of the field\n    '''\n    def __init__(self, name: str, description: str):\n        '''Initialize parent class'''\n        super().__init__(\n            field_type='case',\n            name=name,\n            description=description,\n            dtype='category',\n            coded=True,\n            codes={'World': 'World'},  # TODO: Insert list of country names here.\n        )\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.RegionFieldDefinition.__init__","title":"<code>__init__(name, description)</code>","text":"<p>Initialize parent class</p> Source code in <code>python/posted/columns.py</code> <pre><code>def __init__(self, name: str, description: str):\n    '''Initialize parent class'''\n    super().__init__(\n        field_type='case',\n        name=name,\n        description=description,\n        dtype='category',\n        coded=True,\n        codes={'World': 'World'},  # TODO: Insert list of country names here.\n    )\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.SourceFieldDefinition","title":"<code>SourceFieldDefinition</code>","text":"<p>               Bases: <code>AbstractFieldDefinition</code></p> <p>Class to store Source fields</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the field</p> required <code>description</code> <code>str</code> <p>Description of the field</p> required Source code in <code>python/posted/columns.py</code> <pre><code>class SourceFieldDefinition(AbstractFieldDefinition):\n    '''\n    Class to store Source fields\n\n    Parameters\n    ----------\n    name: str\n        Name of the field\n    description: str\n        Description of the field\n    '''\n    def __init__(self, name: str, description: str):\n        '''Initialize parent class'''\n        super().__init__(\n            field_type='case',\n            name=name,\n            description=description,\n            dtype='category',\n            coded=False,  # TODO: Insert list of BibTeX identifiers here.\n        )\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.SourceFieldDefinition.__init__","title":"<code>__init__(name, description)</code>","text":"<p>Initialize parent class</p> Source code in <code>python/posted/columns.py</code> <pre><code>def __init__(self, name: str, description: str):\n    '''Initialize parent class'''\n    super().__init__(\n        field_type='case',\n        name=name,\n        description=description,\n        dtype='category',\n        coded=False,  # TODO: Insert list of BibTeX identifiers here.\n    )\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.UnitDefinition","title":"<code>UnitDefinition</code>","text":"<p>               Bases: <code>AbstractColumnDefinition</code></p> <p>Class to store Unit columns</p> <p>Parameters:</p> Name Type Description Default <code>col_type</code> <p>Type of the column</p> required <code>name</code> <code>str</code> <p>Name of the column</p> required <code>description</code> <code>str</code> <p>Description of the column</p> required <code>required</code> <code>bool</code> <p>Bool that specifies if the column is required</p> required <p>Methods:</p> Name Description <code>is_allowed</code> <p>Check if cell is allowed</p> Source code in <code>python/posted/columns.py</code> <pre><code>class UnitDefinition(AbstractColumnDefinition):\n    '''\n    Class to store Unit columns\n\n    Parameters\n    ----------\n    col_type: str\n        Type of the column\n    name: str\n        Name of the column\n    description: str\n        Description of the column\n    required: bool\n        Bool that specifies if the column is required\n\n    Methods\n    -------\n    is_allowed\n        Check if cell is allowed\n    '''\n    def __init__(self, name: str, description: str, required: bool):\n        super().__init__(\n            col_type='unit',\n            name=name,\n            description=description,\n            dtype='category',\n            required=required,\n        )\n\n    def is_allowed(self, cell: str | float | int) -&gt; bool:\n        if pd.isnull(cell):\n            return not self._required\n        if not isinstance(cell, str):\n            return False\n        tokens = cell.split(';')\n        if len(tokens) == 1:\n            return cell in ureg\n        elif len(tokens) == 2:\n            return tokens[0] in ureg and tokens[1] in unit_variants\n        else:\n            return False\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.ValueDefinition","title":"<code>ValueDefinition</code>","text":"<p>               Bases: <code>AbstractColumnDefinition</code></p> <p>Class to store Value columns</p> <p>Parameters:</p> Name Type Description Default <code>col_type</code> <p>Type of the column</p> required <code>name</code> <code>str</code> <p>Name of the column</p> required <code>description</code> <code>str</code> <p>Description of the column</p> required <code>required</code> <code>bool</code> <p>Bool that specifies if the column is required</p> required <p>Methods:</p> Name Description <code>is_allowed</code> <p>Check if cell is allowed</p> Source code in <code>python/posted/columns.py</code> <pre><code>class ValueDefinition(AbstractColumnDefinition):\n    '''\n    Class to store Value columns\n\n    Parameters\n    ----------\n    col_type: str\n        Type of the column\n    name: str\n        Name of the column\n    description: str\n        Description of the column\n    required: bool\n        Bool that specifies if the column is required\n\n    Methods\n    -------\n    is_allowed\n        Check if cell is allowed\n    '''\n    def __init__(self, name: str, description: str, required: bool):\n        super().__init__(\n            col_type='value',\n            name=name,\n            description=description,\n            dtype='float',\n            required=required,\n        )\n\n    def is_allowed(self, cell: str | float | int) -&gt; bool:\n        if pd.isnull(cell):\n            return not self._required\n        return isinstance(cell, float | int)\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.VariableDefinition","title":"<code>VariableDefinition</code>","text":"<p>               Bases: <code>AbstractColumnDefinition</code></p> <p>Class to store variable columns</p> <p>Parameters:</p> Name Type Description Default <code>col_type</code> <p>Type of the column</p> required <code>name</code> <code>str</code> <p>Name of the column</p> required <code>description</code> <code>str</code> <p>Description of the column</p> required <code>required</code> <code>bool</code> <p>Bool that specifies if the column is required</p> required <p>Methods:</p> Name Description <code>is_allowed</code> <p>Check if cell is allowed</p> Source code in <code>python/posted/columns.py</code> <pre><code>class VariableDefinition(AbstractColumnDefinition):\n    '''\n    Class to store variable columns\n\n    Parameters\n    ----------\n    col_type: str\n        Type of the column\n    name: str\n        Name of the column\n    description: str\n        Description of the column\n    required: bool\n        Bool that specifies if the column is required\n\n    Methods\n    -------\n    is_allowed\n        Check if cell is allowed\n    '''\n    def __init__(self, name: str, description: str, required: bool):\n        super().__init__(\n            col_type='variable',\n            name=name,\n            description=description,\n            dtype='category',\n            required=required,\n        )\n\n    def is_allowed(self, cell: str | float | int) -&gt; bool:\n        if pd.isnull(cell):\n            return not self._required\n        return isinstance(cell, str) and cell in variables\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.is_float","title":"<code>is_float(string)</code>","text":"<p>Checks if a given string can be converted to a floating-point number in Python.</p> <p>Parameters:</p> Name Type Description Default <code>string</code> <code>str</code> <p>String to check</p> required <p>Returns:</p> Type Description <code>    bool</code> <p>True if conversion was successful, False if not</p> Source code in <code>python/posted/columns.py</code> <pre><code>def is_float(string: str) -&gt; bool:\n    '''Checks if a given string can be converted to a floating-point number in\n    Python.\n\n    Parameters\n    ----------\n    string : str\n        String to check\n\n    Returns\n    -------\n        bool\n            True if conversion was successful, False if not\n    '''\n    try:\n        float(string)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"code/python/columns/#python.posted.columns.read_fields","title":"<code>read_fields(variable)</code>","text":"<p>Read the fields of a variable</p> <p>Returns:</p> Type Description <code>    dict</code> <pre><code>Dictionary containing the fields\n</code></pre> <p>comments     Dictionary containing the comments</p> Source code in <code>python/posted/columns.py</code> <pre><code>def read_fields(variable: str):\n    '''\n    Read the fields of a variable\n\n    Parameters\n    ----------\n        variable: str\n            Variable to read\n\n    Returns\n    -------\n        dict\n            Dictionary containing the fields\n        comments\n            Dictionary containing the comments\n\n    '''\n    fields: dict[str, CustomFieldDefinition] = {}\n    comments: dict[str, CommentDefinition] = {}\n\n    for database_id in databases:\n        fpath = databases[database_id] / 'fields' / ('/'.join(variable.split('|')) + '.yml')\n        if fpath.exists():\n            if not fpath.is_file():\n                raise Exception(f\"Expected YAML file, but not a file: {fpath}\")\n\n            for col_id, field_specs in read_yml_file(fpath).items():\n                if field_specs['type'] in ('case', 'component'):\n                    fields[col_id] = CustomFieldDefinition(**field_specs)\n                elif field_specs['type'] == 'comment':\n                    comments[col_id] = CommentDefinition(\n                        **{k: v for k, v in field_specs.items() if k != 'type'},\n                        required=False,\n                    )\n                else:\n                    raise Exception(f\"Unkown field type: {col_id}\")\n\n    # make sure the field ID is not the same as for a base column\n    for col_id in fields:\n        if col_id in base_columns:\n            raise Exception(f\"Field ID cannot be equal to a base column ID: {col_id}\")\n\n    return fields, comments\n</code></pre>"},{"location":"code/python/definitions/","title":"definitions","text":""},{"location":"code/python/definitions/#python.posted.definitions.read_definitions","title":"<code>read_definitions(definitions_dir, flows, techs)</code>","text":"<p>Reads YAML files from definitions directory, extracts tags, inserts tags into definitions, replaces tokens in definitions, and returns the updated definitions.</p> <p>Parameters:</p> Name Type Description Default <code>definitions_dir</code> <code>Path</code> <p>Path leading to the definitions</p> required <code>flows</code> <code>dict</code> <p>Dictionary containng the different flow types. Each key represents a flow type, the corresponding value is a dictionary containing key value pairs of attributes like denisty, energycontent and their values.</p> required <code>techs</code> <code>dict</code> <p>Dictionary containing information about different technologies. Each key in the dictionary represents a unique technology ID, and the corresponding value is a dictionary containing various specifications for that technology, like 'description', 'class', 'primary output' etc.</p> required <p>Returns:</p> Type Description <code>    dict</code> <p>Dictionary containing the definitions after processing and replacing tags and tokens</p> Source code in <code>python/posted/definitions.py</code> <pre><code>def read_definitions(definitions_dir: Path, flows: dict, techs: dict):\n    '''\n    Reads YAML files from definitions directory, extracts tags, inserts tags into\n    definitions, replaces tokens in definitions, and returns the updated definitions.\n\n    Parameters\n    ----------\n    definitions_dir : Path\n        Path leading to the definitions\n    flows : dict\n        Dictionary containng the different flow types. Each key represents a flow type, the corresponding\n        value is a dictionary containing key value pairs of attributes like denisty, energycontent and their\n        values.\n    techs : dict\n        Dictionary containing information about different technologies. Each key in the\n        dictionary represents a unique technology ID, and the corresponding value is a dictionary containing\n        various specifications for that technology, like 'description', 'class', 'primary output' etc.\n\n    Returns\n    -------\n        dict\n            Dictionary containing the definitions after processing and replacing tags and tokens\n    '''\n    # check that variables exists and is a directory\n    if not definitions_dir.exists():\n        return {}\n    if not definitions_dir.is_dir():\n        raise Exception(f\"Should be a directory but is not: {definitions_dir}\")\n\n    # read all definitions and tags\n    definitions = {}\n    tags = {}\n    for file_path in definitions_dir.rglob('*.yml'):\n        if file_path.name.startswith('tag_'):\n            tags |= read_yml_file(file_path)\n        else:\n            definitions |= read_yml_file(file_path)\n\n    # read tags from flows and techs\n    tags['Flow IDs'] = {\n        flow_id: {}\n        for flow_id, flow_specs in flows.items()\n    }\n    tags['Tech IDs'] = {\n        tech_id: {\n            k: v\n            for k, v in tech_specs.items()\n            if k in ['primary_output']\n        }\n        for tech_id, tech_specs in techs.items()\n    }\n\n    # insert tags\n    for tag, items in tags.items():\n        definitions = replace_tags(definitions, tag, items)\n\n    # remove definitions where tags could not been replaced\n    if any('{' in key for key in definitions):\n        warnings.warn('Tokens could not be replaced correctly.')\n        definitions = {k: v for k, v in definitions.items() if '{' not in k}\n\n    # insert tokens\n    tokens = {\n        'default currency': lambda def_specs: default_currency,\n        'primary output': lambda def_specs: def_specs['primary_output'],\n    } | {\n        f\"default flow unit {unit_component}\": unit_token_func(unit_component, flows)\n        for unit_component in ('full', 'raw', 'variant')\n    }\n    for def_key, def_specs in definitions.items():\n        for def_property, def_value in def_specs.items():\n            for token_key, token_func in tokens.items():\n                if isinstance(def_value, str) and f\"{{{token_key}}}\" in def_value:\n                    def_specs[def_property] = def_specs[def_property].replace(f\"{{{token_key}}}\", token_func(def_specs))\n\n    return definitions\n</code></pre>"},{"location":"code/python/definitions/#python.posted.definitions.replace_tags","title":"<code>replace_tags(definitions, tag, items)</code>","text":"<p>Replaces specified tags in dictionary keys and values with corresponding items from another dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>definitions</code> <code>dict</code> <p>Dictionary containing the definitions, where the tags should be replaced by the items</p> required <code>tag</code> <code>str</code> <p>String to identify where replacements should be made in the definitions. Specifies the placeholder that needs to be replaced with actual values from the <code>items</code> dictionary.</p> required <code>items</code> <code>dict[str, dict]</code> <p>Dictionary containing the items from whith to replace the definitions</p> required <p>Returns:</p> Type Description <code>    dict</code> <p>Dictionary containing the definitions with replacements based on the provided tag and items.</p> Source code in <code>python/posted/definitions.py</code> <pre><code>def replace_tags(definitions: dict, tag: str, items: dict[str, dict]):\n    '''\n    Replaces specified tags in dictionary keys and values with corresponding\n    items from another dictionary.\n\n    Parameters\n    ----------\n    definitions : dict\n        Dictionary containing the definitions, where the tags should be replaced by the items\n    tag : str\n        String to identify where replacements should be made in the definitions. Specifies\n        the placeholder that needs to be replaced with actual values from the `items` dictionary.\n    items : dict[str, dict]\n        Dictionary containing the items from whith to replace the definitions\n\n    Returns\n    -------\n        dict\n            Dictionary containing the definitions with replacements based on the provided tag and items.\n    '''\n\n    definitions_with_replacements = {}\n    for def_name, def_specs in definitions.items():\n        if f\"{{{tag}}}\" not in def_name:\n            definitions_with_replacements[def_name] = def_specs\n        else:\n            for item_name, item_specs in items.items():\n                item_desc = item_specs['description'] if 'description' in item_specs else item_name\n                def_name_new = def_name.replace(f\"{{{tag}}}\", item_name)\n                def_specs_new = copy.deepcopy(def_specs)\n                def_specs_new |= item_specs\n\n                # replace tags in description\n                def_specs_new['description'] = def_specs['description'].replace(f\"{{{tag}}}\", item_desc)\n\n                # replace tags in other specs\n                for k, v in def_specs_new.items():\n                    if k == 'description' or not isinstance(v, str):\n                        continue\n                    def_specs_new[k] = def_specs_new[k].replace(f\"{{{tag}}}\", item_name)\n                    def_specs_new[k] = def_specs_new[k].replace('{parent variable}', def_name[:def_name.find(f\"{{{tag}}}\")-1])\n                definitions_with_replacements[def_name_new] = def_specs_new\n\n    return definitions_with_replacements\n</code></pre>"},{"location":"code/python/definitions/#python.posted.definitions.unit_token_func","title":"<code>unit_token_func(unit_component, flows)</code>","text":"<p>Takes a unit component type and a dictionary of flows, and returns a lambda function that extracts the default unit based on the specified component type from the flow dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>unit_component</code> <code>Literal['full', 'raw', 'variant']</code> <p>Specifies the type of unit token to be returned.</p> required <code>flows</code> <code>dict</code> <p>Dictionary containg the flows</p> required <p>Returns:</p> Type Description <code>    lambda function</code> <p>lambda function that takes a dictionary <code>def_specs</code> as input. The lambda function will return different values based on the <code>unit_component</code> parameter and the contents of the <code>flows</code> dictionary.</p> Source code in <code>python/posted/definitions.py</code> <pre><code>def unit_token_func(unit_component: Literal['full', 'raw', 'variant'], flows: dict):\n    '''\n    Takes a unit component type and a dictionary of flows, and returns a lambda function\n    that extracts the default unit based on the specified component type from the flow\n    dictionary.\n\n    Parameters\n    ----------\n    unit_component : Literal['full', 'raw', 'variant']\n        Specifies the type of unit token to be returned.\n    flows : dict\n        Dictionary containg the flows\n\n\n    Returns\n    -------\n        lambda function\n            lambda function that takes a dictionary `def_specs` as input. The lambda function\n            will return different values based on the `unit_component` parameter and\n            the contents of the `flows` dictionary.\n    '''\n    return lambda def_specs: (\n        'ERROR'\n        if 'flow_id' not in def_specs or def_specs['flow_id'] not in flows else\n        (\n            flows[def_specs['flow_id']]['default_unit']\n            if unit_component == 'full' else\n            flows[def_specs['flow_id']]['default_unit'].split(';')[0]\n            if unit_component == 'raw' else\n            ';'.join([''] + flows[def_specs['flow_id']]['default_unit'].split(';')[1:2])\n            if unit_component == 'variant' else\n            'UNKNOWN'\n        )\n    )\n</code></pre>"},{"location":"code/python/masking/","title":"masking","text":""},{"location":"code/python/masking/#python.posted.masking.Mask","title":"<code>Mask</code>","text":"<p>Class to define masks with conditions and weights to apply to DataFiles</p> <p>Parameters:</p> Name Type Description Default <code>where</code> <code>MaskCondition | list[MaskCondition]</code> <p>Where the mask should be applied</p> <code>None</code> <code>use</code> <code>MaskCondition | list[MaskCondition]</code> <p>Condition on where to use the masks</p> <code>None</code> <code>weight</code> <code>None | float | str | list[float | str]</code> <p>Weights to apply</p> <code>None</code> <code>other</code> <code>float</code> <code>nan</code> <code>comment</code> <code>str</code> <pre><code>Comment\n</code></pre> <code>''</code> Source code in <code>python/posted/masking.py</code> <pre><code>class Mask:\n    '''Class to define masks with conditions and weights to apply to DataFiles\n\n    Parameters\n    ----------\n    where: MaskCondition | list[MaskCondition], optional\n        Where the mask should be applied\n    use:  MaskCondition | list[MaskCondition], optional\n        Condition on where to use the masks\n    weight: None | float | str | list[float | str], optional\n        Weights to apply\n    other: float, optional\n\n    comment: str, optional\n            Comment\n    '''\n    def __init__(self,\n                 where: MaskCondition | list[MaskCondition] = None,\n                 use: MaskCondition | list[MaskCondition] = None,\n                 weight: None | float | str | list[float | str] = None,\n                 other: float = np.nan,\n                 comment: str = ''):\n        '''set fields from constructor arguments, perform consistency checks on fields,\n        set default weight to 1 if not set otherwise'''\n        self._where: list[MaskCondition] = [] if where is None else where if isinstance(where, list) else [where]\n        self._use: list[MaskCondition] = [] if use is None else use if isinstance(use, list) else [use]\n        self._weight: list[float] = (\n            None\n            if weight is None else\n            [float(w) for w in weight]\n            if isinstance(weight, list) else\n            [float(weight)]\n        )\n        self._other: float = other\n        self._comment: str = comment\n\n        # perform consistency checks on fields\n        if self._use and self._weight and len(self._use) != len(self._weight):\n            raise Exception(f\"Must provide same length of 'use' conditions as 'weight' values.\")\n\n        # set default weight to 1 if not set otherwise\n        if not self._weight:\n            self._weight = len(self._use) * [1.0]\n\n\n    def matches(self, df: pd.DataFrame):\n        '''Check if a mask matches a dataframe (all 'where' conditions match across all rows)\n\n        Parameters\n        ----------\n        df: pd.Dataframe\n            Dataframe to check for matches\n        Returns\n        -------\n            bool\n                If the mask matches the dataframe'''\n        for w in self._where:\n            if not apply_cond(df, w).all():\n                return False\n        return True\n\n\n    def get_weights(self, df: pd.DataFrame):\n        '''Apply weights to the dataframe\n\n        Parameters\n        ----------\n        df: pd.Dataframe\n            Dataframe to apply weights on\n\n        Returns\n        -------\n            pd.DataFrame\n                Dataframe with applied weights'''\n        ret = pd.Series(index=df.index, data=np.nan)\n\n        # apply weights where the use condition matches\n        for u, w in zip(self._use, self._weight):\n            ret.loc[apply_cond(df, u)] = w\n\n        return ret\n</code></pre>"},{"location":"code/python/masking/#python.posted.masking.Mask.__init__","title":"<code>__init__(where=None, use=None, weight=None, other=np.nan, comment='')</code>","text":"<p>set fields from constructor arguments, perform consistency checks on fields, set default weight to 1 if not set otherwise</p> Source code in <code>python/posted/masking.py</code> <pre><code>def __init__(self,\n             where: MaskCondition | list[MaskCondition] = None,\n             use: MaskCondition | list[MaskCondition] = None,\n             weight: None | float | str | list[float | str] = None,\n             other: float = np.nan,\n             comment: str = ''):\n    '''set fields from constructor arguments, perform consistency checks on fields,\n    set default weight to 1 if not set otherwise'''\n    self._where: list[MaskCondition] = [] if where is None else where if isinstance(where, list) else [where]\n    self._use: list[MaskCondition] = [] if use is None else use if isinstance(use, list) else [use]\n    self._weight: list[float] = (\n        None\n        if weight is None else\n        [float(w) for w in weight]\n        if isinstance(weight, list) else\n        [float(weight)]\n    )\n    self._other: float = other\n    self._comment: str = comment\n\n    # perform consistency checks on fields\n    if self._use and self._weight and len(self._use) != len(self._weight):\n        raise Exception(f\"Must provide same length of 'use' conditions as 'weight' values.\")\n\n    # set default weight to 1 if not set otherwise\n    if not self._weight:\n        self._weight = len(self._use) * [1.0]\n</code></pre>"},{"location":"code/python/masking/#python.posted.masking.Mask.get_weights","title":"<code>get_weights(df)</code>","text":"<p>Apply weights to the dataframe</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to apply weights on</p> required <p>Returns:</p> Type Description <code>    pd.DataFrame</code> <p>Dataframe with applied weights</p> Source code in <code>python/posted/masking.py</code> <pre><code>def get_weights(self, df: pd.DataFrame):\n    '''Apply weights to the dataframe\n\n    Parameters\n    ----------\n    df: pd.Dataframe\n        Dataframe to apply weights on\n\n    Returns\n    -------\n        pd.DataFrame\n            Dataframe with applied weights'''\n    ret = pd.Series(index=df.index, data=np.nan)\n\n    # apply weights where the use condition matches\n    for u, w in zip(self._use, self._weight):\n        ret.loc[apply_cond(df, u)] = w\n\n    return ret\n</code></pre>"},{"location":"code/python/masking/#python.posted.masking.Mask.matches","title":"<code>matches(df)</code>","text":"<p>Check if a mask matches a dataframe (all 'where' conditions match across all rows)</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to check for matches</p> required <p>Returns:</p> Type Description <code>    bool</code> <p>If the mask matches the dataframe</p> Source code in <code>python/posted/masking.py</code> <pre><code>def matches(self, df: pd.DataFrame):\n    '''Check if a mask matches a dataframe (all 'where' conditions match across all rows)\n\n    Parameters\n    ----------\n    df: pd.Dataframe\n        Dataframe to check for matches\n    Returns\n    -------\n        bool\n            If the mask matches the dataframe'''\n    for w in self._where:\n        if not apply_cond(df, w).all():\n            return False\n    return True\n</code></pre>"},{"location":"code/python/masking/#python.posted.masking.apply_cond","title":"<code>apply_cond(df, cond)</code>","text":"<p>Takes a pandas DataFrame and a condition, which can be a string, dictionary, or callable, and applies the condition to the DataFrame using <code>eval</code> or <code>apply</code> accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame containing the data on which the condition will be applied.</p> required <code>cond</code> <code>MaskCondition</code> <p>The condition to be applied on the dataframe. Can be either a string, a dictionary, or a callable function.</p> required <p>Returns:</p> Type Description <code>    pd.DataFrame</code> <p>Dataframe evaluated at the mask condition</p> Source code in <code>python/posted/masking.py</code> <pre><code>def apply_cond(df: pd.DataFrame, cond: MaskCondition):\n    '''Takes a pandas DataFrame and a condition, which can be a string, dictionary,\n    or callable, and applies the condition to the DataFrame using `eval` or `apply`\n    accordingly.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        A pandas DataFrame containing the data on which the condition will be applied.\n    cond : MaskCondition\n        The condition to be applied on the dataframe. Can be either a string, a dictionary, or a\n        callable function.\n\n    Returns\n    -------\n        pd.DataFrame\n            Dataframe evaluated at the mask condition\n\n    '''\n    if isinstance(cond, str):\n        return df.eval(cond)\n    elif isinstance(cond, dict):\n        cond = ' &amp; '.join([f\"{key}=='{val}'\" for key, val in cond.items()])\n        return df.eval(cond)\n    elif isinstance(cond, Callable):\n        return df.apply(cond)\n</code></pre>"},{"location":"code/python/masking/#python.posted.masking.read_masks","title":"<code>read_masks(variable)</code>","text":"<p>Reads YAML files containing mask specifications from multiple databases and returns a list of Mask objects.</p> <p>Parameters:</p> Name Type Description Default <code>variable</code> <code>str</code> <p>Variable to be read</p> required <p>Returns:</p> Type Description <code>    list</code> <p>List with masks for the variable</p> Source code in <code>python/posted/masking.py</code> <pre><code>def read_masks(variable: str):\n    '''Reads YAML files containing mask specifications from multiple databases\n    and returns a list of Mask objects.\n\n    Parameters\n    ----------\n    variable : str\n        Variable to be read\n\n    Returns\n    -------\n        list\n            List with masks for the variable\n\n    '''\n    ret: list[Mask] = []\n\n    for database_id in databases:\n        fpath = databases[database_id] / 'masks' / ('/'.join(variable.split('|')) + '.yml')\n        if fpath.exists():\n            if not fpath.is_file():\n                raise Exception(f\"Expected YAML file, but not a file: {fpath}\")\n\n            ret += [\n                Mask(**mask_specs)\n                for mask_specs in read_yml_file(fpath)\n            ]\n\n    return ret\n</code></pre>"},{"location":"code/python/noslag/","title":"noslag","text":""},{"location":"code/python/noslag/#python.posted.noslag.DataSet","title":"<code>DataSet</code>","text":"<p>               Bases: <code>TEBase</code></p> <p>Class to store, normalise, select and aggregate DataSets</p> Parameters <p>parent_variable: str     Variable to collect Data on include_databases: Optional[list|str] | tuple[str]], optional     Databases to load from file_paths: Optional[list[path]], optional     Paths to load data from check_inconsistencies: bool, optional     Wether to check for inconsistencies data: Optional[pd.DataFrame], optional     Specific data to include in the dataset</p> Source code in <code>python/posted/noslag.py</code> <pre><code>class DataSet(TEBase):\n    '''Class to store, normalise, select and aggregate DataSets\n     Parameters\n    ----------\n    parent_variable: str\n        Variable to collect Data on\n    include_databases: Optional[list|str] | tuple[str]], optional\n        Databases to load from\n    file_paths: Optional[list[path]], optional\n        Paths to load data from\n    check_inconsistencies: bool, optional\n        Wether to check for inconsistencies\n    data: Optional[pd.DataFrame], optional\n        Specific data to include in the dataset\n\n\n    '''\n    _df: None | pd.DataFrame\n    _columns: dict[str, AbstractColumnDefinition]\n    _fields: dict[str, AbstractFieldDefinition]\n    _masks: list[Mask]\n\n    # initialise\n    def __init__(self,\n                 parent_variable: str,\n                 include_databases: Optional[list[str] | tuple[str]] = None,\n                 file_paths: Optional[list[Path]] = None,\n                 check_inconsistencies: bool = False,\n                 data: Optional[pd.DataFrame] = None,\n                 ):\n        '''Initialise parent class and fields, load data from specified databases and files\n\n\n        '''\n        TEBase.__init__(self, parent_variable)\n\n        # initialise fields\n        self._df = None\n        self._columns = base_columns\n        self._fields = {\n            col_id: field\n            for col_id, field in self._columns.items()\n            if isinstance(field, AbstractFieldDefinition)\n        }\n        self._masks = []\n\n        # Load data if provided, otherwise load from TEDataFiles\n        if data is not None:\n            self._df = data\n        else:\n            # read TEDataFiles and combine into dataset\n            include_databases = list(include_databases) if include_databases is not None else list(databases.keys())\n            self._df = self._load_files(include_databases, file_paths or [], check_inconsistencies)\n\n\n    @property\n    def data(self):\n        '''str: Get or set dataframe'''\n        return self._df\n\n    def set_data(self, df: pd.DataFrame):\n        self._df = df\n\n\n    def _load_files(self, include_databases: list[str], file_paths: list[Path], check_inconsistencies: bool):\n        # Load TEDFs and compile into NSHADataSet\n\n        files: list[TEDF] = []\n\n        # collect TEDF and append to list\n        collected_files = collect_files(parent_variable=self._parent_variable, include_databases=include_databases)\n        for file_variable, file_database_id in collected_files:\n            files.append(TEDF(parent_variable=file_variable, database_id=file_database_id))\n        for file_path in file_paths:\n            files.append(TEDF(parent_variable=self._parent_variable, file_path=file_path))\n\n        # raise exception if no TEDF can be loaded\n        if not files:\n            raise Exception(f\"No TEDF to load for variable '{self._parent_variable}'.\")\n\n        # get fields and masks from databases\n        files_vars: set[str] = {f.parent_variable for f in files}\n        for v in files_vars:\n            new_fields, new_comments = read_fields(v)\n            for col_id in new_fields | new_comments:\n                if col_id in self._columns:\n                    raise Exception(f\"Cannot load TEDFs due to multiple columns with same ID defined: {col_id}\")\n            self._fields = new_fields | self._fields\n            self._columns = new_fields | self._columns | new_comments\n            self._masks += read_masks(v)\n\n        # load all TEDFs: load from file, check for inconsistencies (if requested), expand cases and variables\n        file_dfs: list[pd.DataFrame] = []\n        for f in files:\n            # load\n            f.load()\n\n            # check for inconsistencies\n            if check_inconsistencies:\n                f.check()\n\n            # obtain dataframe and insert column parent_variable\n            df_tmp = f.data.copy()\n            df_tmp.insert(0, 'parent_variable', f.parent_variable)\n\n            # append to dataframe list\n            file_dfs.append(df_tmp)\n\n        # compile dataset from the dataframes loaded from the individual files\n        data = pd.concat(file_dfs)\n\n        # query relevant variables\n        data = data.query(f\"parent_variable=='{self._parent_variable}'\")\n\n        # drop entries with unknown variables and warn\n        for var_type in ('variable', 'reference_variable'):\n            cond = (data[var_type].notnull() &amp;\n                    data.apply(lambda row: f\"{row['parent_variable']}|{row[var_type]}\" not in self._var_specs, axis=1))\n            if cond.any():\n                warnings.warn(f\"Unknown {var_type}, so dropping rows:\\n{data.loc[cond, var_type]}\")\n                data = data.loc[~cond].reset_index(drop=True)\n\n        # return\n        return data\n\n\n    def normalise(self, override: Optional[dict[str, str]] = None, inplace: bool = False) -&gt; pd.DataFrame | None:\n        '''\n        normalise data: default reference units, reference value equal to 1.0, default reported units\n\n        Parameters\n        ----------\n        override: Optional[dict[str,str]], optional\n            Dictionary with key, value pairs of variables to override\n        inplace: bool, optional\n            Wether to do the normalisation in place\n\n        Returns\n        -------\n        pd.DataFrame\n            if inplace is false, returns normalised dataframe'''\n        normalised, _ = self._normalise(override)\n        if inplace:\n            self._df = normalised\n            return\n        else:\n            return normalised\n\n    def _normalise(self, override: Optional[dict[str, str]]) -&gt; tuple[pd.DataFrame, dict[str, str]]:\n        if override is None:\n            override = {}\n\n        # get overridden var specs\n        var_flow_ids = {\n            var_name: var_specs['flow_id'] if 'flow_id' in var_specs else np.nan\n            for var_name, var_specs in self._var_specs.items()\n        }\n        var_units = {\n            var_name: var_specs['default_unit']\n            for var_name, var_specs in self._var_specs.items()\n        } | override\n\n        # normalise reference units, normalise reference values, and normalise reported units\n        normalised = self._df \\\n            .pipe(normalise_units, level='reference', var_units=var_units, var_flow_ids=var_flow_ids) \\\n            .pipe(normalise_values) \\\n            .pipe(normalise_units, level='reported', var_units=var_units, var_flow_ids=var_flow_ids)\n\n        # return normalised data and variable units\n        return normalised, var_units\n\n    # prepare data for selection\n    def select(self,\n               override: Optional[dict[str, str]] = None,\n               drop_singular_fields: bool = True,\n               extrapolate_period: bool = True,\n               **field_vals_select) -&gt; pd.DataFrame:\n        '''Select desired data from the dataframe\n\n        Parameters\n        ----------\n        override: Optional[dict[str, str]]\n            Dictionary with key, value paris of variables to override\n        drop_singular_fields: bool, optional\n            If True, drop custom fields with only one value\n        extrapolate_period: bool, optional\n            If True, extrapolate values by extrapolation, if no value for this period is given\n        **field_vals_select\n            IDs of values to select\n\n        Returns\n        -------\n        pd.DataFrame\n            DataFrame with selected Values\n            '''\n        selected, var_units, var_references = self._select(\n            override,\n            drop_singular_fields,\n            extrapolate_period,\n            **field_vals_select,\n        )\n        selected.insert(selected.columns.tolist().index('variable'), 'reference_variable', np.nan)\n        selected['reference_variable'] = selected['variable'].map(var_references)\n        return self._cleanup(selected, var_units)\n\n    def _select(self,\n                override: Optional[dict[str, str]],\n                drop_singular_fields: bool,\n                extrapolate_period: bool,\n                **field_vals_select) -&gt; tuple[pd.DataFrame, dict[str, str], dict[str, str]]:\n        # start from normalised data\n        normalised, var_units = self._normalise(override)\n        selected = normalised\n\n        # drop unit columns and reference value column\n        selected.drop(columns=['unit', 'reference_unit', 'reference_value'], inplace=True)\n\n        # drop columns containing comments and uncertainty field (which is currently unsupported)\n        selected.drop(\n            columns=['uncertainty'] + [\n                col_id for col_id, field in self._columns.items()\n                if field.col_type == 'comment'\n            ],\n            inplace=True,\n        )\n\n        # add parent variable as prefix to other variable columns\n        selected['variable'] = selected['parent_variable'] + '|' + selected['variable']\n        selected['reference_variable'] = selected['parent_variable'] + '|' + selected['reference_variable']\n        selected.drop(columns=['parent_variable'], inplace=True)\n\n        # raise exception if fields listed in arguments that are unknown\n        for field_id in field_vals_select:\n            if not any(field_id == col_id for col_id in self._fields):\n                raise Exception(f\"Field '{field_id}' does not exist and cannot be used for selection.\")\n\n        # order fields for selection: period must be expanded last due to the interpolation\n        fields_select = ({col_id: self._fields[col_id] for col_id in field_vals_select} |\n                         {col_id: field for col_id, field in self._fields.items() if col_id != 'period' and col_id not in field_vals_select} |\n                         {'period': self._fields['period']})\n\n        # select and expand fields\n        for col_id, field in fields_select.items():\n            field_vals = field_vals_select[col_id] if col_id in field_vals_select else None\n            selected = field.select_and_expand(selected, col_id, field_vals, extrapolate_period=extrapolate_period)\n\n        # drop custom fields with only one value if specified in method argument\n        if drop_singular_fields:\n            selected.drop(columns=[\n                col_id for col_id, field in self._fields.items()\n                if isinstance(field, CustomFieldDefinition) and selected[col_id].nunique() &lt; 2\n            ], inplace=True)\n\n        # apply mappings\n        selected = self._apply_mappings(selected, var_units)\n\n        # drop rows with failed mappings\n        selected.dropna(subset='value', inplace=True)\n\n        # get map of variable references\n        var_references = selected \\\n            .filter(['variable', 'reference_variable']) \\\n            .drop_duplicates() \\\n            .set_index('variable')['reference_variable']\n\n        # Check for multiple reference variables per reported variable\n        if not var_references.index.is_unique:\n            raise Exception(f\"Multiple reference variables per reported variable found: {var_references}\")\n        var_references = var_references.to_dict()\n\n        # Remove 'reference_variable column\n        selected.drop(columns=['reference_variable'], inplace=True)\n\n        # strip off unit variants\n        var_units = {\n            variable: unit.split(';')[0]\n            for variable, unit in var_units.items()\n        }\n\n        # return\n        return selected, var_units, var_references\n\n\n    def _apply_mappings(self, expanded: pd.DataFrame, var_units: dict) -&gt; pd.DataFrame:\n        # apply mappings between entry types\n        # list of columns to group by\n        group_cols = [\n            c for c in expanded.columns\n            if c not in ['variable', 'reference_variable', 'value']\n        ]\n\n        # perform groupby and do not drop NA values\n        grouped = expanded.groupby(group_cols, dropna=False)\n\n        # create return list\n        ret = []\n\n        # loop over groups\n        for keys, ids in grouped.groups.items():\n            # get rows in group\n            rows = expanded.loc[ids, [c for c in expanded if c not in group_cols]].copy()\n\n            # 1. convert FLH to OCF\n            cond = rows['variable'].str.endswith('|FLH')\n            if cond.any():\n\n                # Multiply 'value' by conversion factor\n                rows.loc[cond, 'value'] *= rows.loc[cond].apply(\n                    lambda row: unit_convert(\n                        var_units[row['variable']] + '/a',\n                        var_units[row['variable'].replace('|FLH', '|OCF')],\n                    ),\n                    axis=1,\n                )\n\n                # Replace '|FLH' with '|OCF\u2018 in 'variable'\n                rows.loc[cond, 'variable'] = rows.loc[cond, 'variable'] \\\n                    .str.replace('|FLH', '|OCF', regex=False)\n\n            # 2. convert OPEX Fixed Relative to OPEX Fixed\n            cond = rows['variable'].str.endswith('|OPEX Fixed Relative')\n            if cond.any():\n\n                # Define a function to calculate the conversion factor\n                def calculate_conversion(row):\n                    conversion_factor = unit_convert(var_units[row['variable']], 'dimensionless') * unit_convert(\n                        var_units[row['variable'].replace('|OPEX Fixed Relative', '|CAPEX')] + '/a',\n                        var_units[row['variable'].replace('|OPEX Fixed Relative', '|OPEX Fixed')]\n                    ) * (rows.query(\n                        f\"variable=='{row['variable'].replace('|OPEX Fixed Relative', '|CAPEX')}'\"\n                    ).pipe(\n                        lambda df: df['value'].iloc[0] if not df.empty else np.nan,\n                    ))\n                    return conversion_factor\n\n                # Calcualte the conversion factor and update 'value' for rows satisfying the condition\n                rows.loc[cond, 'value'] *= rows.loc[cond].apply(\n                    lambda row: calculate_conversion(row),\n                    axis=1,\n                )\n\n                # Replace '|OPEX Fixed Relative' with '|OPEX FIXED' in 'variable'\n                rows.loc[cond, 'variable'] = rows.loc[cond, 'variable'] \\\n                    .str.replace('|OPEX Fixed Relative', '|OPEX Fixed')\n\n                # Assign 'reference_variable' based on modified 'variable'\n                rows.loc[cond, 'reference_variable'] = rows.loc[cond].apply(\n                    lambda row: rows.query(\n                        f\"variable=='{row['variable'].replace('|OPEX Fixed', '|CAPEX')}'\"\n                    ).pipe(\n                        lambda df: df['reference_variable'].iloc[0] if not df.empty else np.nan,\n                    ),\n                    axis=1,\n                )\n\n                # Check if there are rows with null 'value' after the operation\n                if (cond &amp; rows['value'].isnull()).any():\n                    warnings.warn(HarmoniseMappingFailure(\n                        expanded.loc[ids].loc[cond &amp; rows['value'].isnull()],\n                        'No CAPEX value matching a OPEX Fixed Relative value found.',\n                    ))\n\n            # 3. convert OPEX Fixed Specific to OPEX Fixed\n            cond = rows['variable'].str.endswith('|OPEX Fixed Specific')\n            if cond.any():\n\n                # Define a function to calculate the conversion factor\n                def calculate_conversion(row):\n                    conversion_factor = unit_convert(\n                        var_units[row['variable']] + '/a',\n                        var_units[row['variable'].replace('|OPEX Fixed Specific', '|OPEX Fixed')]\n                    ) / unit_convert(\n                        var_units[row['reference_variable']] + '/a',\n                        var_units[re.sub(r'(Input|Output)', r'\\1 Capacity', row['reference_variable'])],\n                        self._var_specs[row['reference_variable']]['flow_id'] if 'flow_id' in self._var_specs[row['reference_variable']] else np.nan,\n                    ) * unit_convert(\n                        var_units[row['variable'].replace('|OPEX Fixed Specific', '|OCF')],\n                        'dimensionless'\n                    ) * (rows.query(\n                        f\"variable=='{row['variable'].replace('|OPEX Fixed Specific', '|OCF')}'\"\n                    ).pipe(\n                        lambda df: df['value'].iloc[0] if not df.empty else np.nan,\n                    ))\n                    return conversion_factor\n\n                # Calculate the conversion factor and update 'value' for rows satisfying the condition\n                rows.loc[cond, 'value'] *= rows.loc[cond].apply(\n                    lambda row: calculate_conversion(row),\n                    axis=1,\n                )\n\n                # replace '|OPEX Fixed Specific' with '|OPEX Fixed' in 'variable'\n                rows.loc[cond, 'variable'] = rows.loc[cond, 'variable'] \\\n                    .str.replace('|OPEX Fixed Specific', '|OPEX Fixed')\n\n                # Assign 'reference_variable by replacing 'Input' or 'Output' with 'Input Capacity' or 'Output Capacity'\n                rows.loc[cond, 'reference_variable'] = rows.loc[cond, 'reference_variable'].apply(\n                    lambda cell: re.sub(r'(Input|Output)', r'\\1 Capacity', cell),\n                )\n\n                # Check if there are any rows with null 'value' after the opera\n                if (cond &amp; rows['value'].isnull()).any():\n                    warnings.warn(HarmoniseMappingFailure(\n                        expanded.loc[ids].loc[cond &amp; rows['value'].isnull()],\n                        'No OCF value matching a OPEX Fixed Specific value found.',\n                    ))\n\n            # 4. convert efficiencies (Output over Input) to demands (Input over Output)\n            cond = (rows['variable'].str.contains(r'\\|Output(?: Capacity)?\\|') &amp;\n                    (rows['reference_variable'].str.contains(r'\\|Input(?: Capacity)?\\|')\n                    if rows['reference_variable'].notnull().any() else False))\n            if cond.any():\n                rows.loc[cond, 'value'] = 1.0 / rows.loc[cond, 'value']\n                rows.loc[cond, 'variable_new'] = rows.loc[cond, 'reference_variable']\n                rows.loc[cond, 'reference_variable'] = rows.loc[cond, 'variable']\n                rows.loc[cond, 'variable'] = rows.loc[cond, 'variable_new']\n                rows.drop(columns=['variable_new'], inplace=True)\n\n            # 5. convert all references to primary output\n            cond = (((rows['reference_variable'].str.contains(r'\\|Output(?: Capacity)?\\|') |\n                    rows['reference_variable'].str.contains(r'\\|Input(?: Capacity)?\\|'))\n                    if rows['reference_variable'].notnull().any() else False) &amp;\n                    rows['variable'].map(lambda var: 'default_reference' in self._var_specs[var]) &amp;\n                    (rows['variable'].map(\n                        lambda var: self._var_specs[var]['default_reference']\n                        if 'default_reference' in self._var_specs[var] else np.nan\n                    ) != rows['reference_variable']))\n            if cond.any():\n                regex_find = r'\\|(Input|Output)(?: Capacity)?\\|'\n                regex_repl = r'|\\1|'\n                rows.loc[cond, 'reference_variable_new'] = rows.loc[cond, 'variable'].map(\n                    lambda var: self._var_specs[var]['default_reference'],\n                )\n\n                # Define function to calculate the conversion factor\n                def calculate_conversion(row):\n                    conversion_factor =  unit_convert(\n                        ('a*' if 'Capacity' in row['reference_variable'] else '') + var_units[row['reference_variable_new']],\n                        var_units[re.sub(regex_find, regex_repl, row['reference_variable_new'])],\n                        row['reference_variable_new'].split('|')[-1]\n                    ) / unit_convert(\n                        ('a*' if 'Capacity' in row['reference_variable'] else '') + var_units[row['reference_variable']],\n                        var_units[re.sub(regex_find, regex_repl, row['reference_variable'])],\n                        row['reference_variable'].split('|')[-1]\n                    ) * rows.query(\n                        f\"variable=='{re.sub(regex_find, regex_repl, row['reference_variable'])}' &amp; \"\n                        f\"reference_variable=='{re.sub(regex_find, regex_repl, row['reference_variable_new'])}'\"\n                    ).pipe(\n                        lambda df: df['value'].iloc[0] if not df.empty else np.nan,\n                    )\n                    return conversion_factor\n\n                # Calculate the conversion factor and update 'value' for rows satisfying the condition\n                rows.loc[cond, 'value'] *= rows.loc[cond].apply(\n                    lambda row: calculate_conversion(row),\n                    axis=1,\n                )\n                rows.loc[cond, 'reference_variable'] = rows.loc[cond, 'reference_variable_new']\n                rows.drop(columns=['reference_variable_new'], inplace=True)\n                if (cond &amp; rows['value'].isnull()).any():\n                    warnings.warn(HarmoniseMappingFailure(\n                        expanded.loc[ids].loc[cond &amp; rows['value'].isnull()],\n                        'No appropriate mapping found to convert row reference to primary output.',\n                    ))\n\n            # set missing columns from group\n            rows[group_cols] = keys\n\n            # add to return list\n            ret.append(rows)\n\n        # convert return list to dataframe and return\n        return pd.concat(ret).reset_index(drop=True) if ret else expanded.iloc[[]]\n\n    # select data\n    def aggregate(self, override: Optional[dict[str, str]] = None,\n                  drop_singular_fields: bool = True,\n                  extrapolate_period: bool = True,\n                  agg: Optional[str | list[str] | tuple[str]] = None,\n                  masks: Optional[list[Mask]] = None,\n                  masks_database: bool = True,\n                  **field_vals_select) -&gt; pd.DataFrame:\n        '''Aggregates data based on specified parameters, applies masks,\n        and cleans up the resulting DataFrame.\n\n        Parameters\n        ----------\n        override: Optional[dict[str, str]]\n            Dictionary with key, value paris of variables to override\n        drop_singular_fields: bool, optional\n            If True, drop custom fields with only one value\n        extrapolate_period: bool, optional\n            If True, extrapolate values by extrapolation, if no value for this period is given\n        agg : Optional[str | list[str] | tuple[str]]\n            Specifies which fields to aggregate over.\n        masks : Optional[list[Mask]]\n            Specifies a list of Mask objects that will be applied to the data during aggregation.\n            These masks can be used to filter or weight the\n            data based on certain conditions defined in the Mask objects.\n        masks_database : bool, optional\n            Determines whether to include masks from databases in the aggregation process.\n            If set to `True`, masks from databases will be included along with any masks provided as function arguments.\n            If set to `False`, only the masks provided as function argruments will be applied\n\n        Returns\n        -------\n        pd.DataFrame\n            The `aggregate` method returns a pandas DataFrame that has been cleaned up and aggregated based\n            on the specified parameters and input data. The method performs aggregation over component\n            fields and cases fields, applies weights based on masks, drops rows with NaN weights, aggregates\n            with weights, inserts reference variables, sorts columns and rows, rounds values, and inserts\n            units before returning the final cleaned and aggregated DataFrame.\n\n        '''\n\n        # get selection\n        selected, var_units, var_references = self._select(override,\n                                                           extrapolate_period,\n                                                           drop_singular_fields,\n                                                           **field_vals_select)\n\n        # compile masks from databases and function argument into one list\n        if masks is not None and any(not isinstance(m, Mask) for m in masks):\n            raise Exception(\"Function argument 'masks' must contain a list of posted.masking.Mask objects.\")\n        masks = (self._masks if masks_database else []) + (masks or [])\n\n        # aggregation\n        component_fields = [\n            col_id for col_id, field in self._fields.items()\n            if field.field_type == 'component'\n        ]\n        if agg is None:\n            agg = component_fields + ['source']\n        else:\n            if isinstance(agg, tuple):\n                agg = list(agg)\n            elif not isinstance(agg, list):\n                agg = [agg]\n            for a in agg:\n                if not isinstance(a, str):\n                    raise Exception(f\"Field ID in argument 'agg' must be a string but found: {a}\")\n                if not any(a == col_id for col_id in self._fields):\n                    raise Exception(f\"Field ID in argument 'agg' is not a valid field: {a}\")\n\n        # aggregate over component fields\n        group_cols = [\n            c for c in selected.columns\n            if not (c == 'value' or (c in agg and c in component_fields))\n        ]\n        aggregated = selected \\\n            .groupby(group_cols, dropna=False) \\\n            .agg({'value': 'sum'}) \\\n            .reset_index()\n\n        # aggregate over cases fields\n        group_cols = [\n            c for c in aggregated.columns\n            if not (c == 'value' or c in agg)\n        ]\n        ret = []\n        for keys, rows in aggregated.groupby(group_cols, dropna=False):\n            # set default weights to 1.0\n            rows = rows.assign(weight=1.0)\n\n            # update weights by applying masks\n            for mask in masks:\n                if mask.matches(rows):\n                    rows['weight'] *= mask.get_weights(rows)\n\n            # drop all rows with weights equal to nan\n            rows.dropna(subset='weight', inplace=True)\n\n            if not rows.empty:\n                # aggregate with weights\n                out = rows \\\n                    .groupby(group_cols, dropna=False)[['value', 'weight']] \\\n                    .apply(lambda cols: pd.Series({\n                        'value': np.average(cols['value'], weights=cols['weight']),\n                    }))\n\n                # add to return list\n                ret.append(out)\n        aggregated = pd.concat(ret).reset_index()\n\n        # insert reference variables\n        var_ref_unique = {\n            var_references[var]\n            for var in aggregated['variable'].unique()\n            if not pd.isnull(var_references[var])\n        }\n        agg_append = []\n        for ref_var in var_ref_unique:\n            agg_append.append(pd.DataFrame({\n                'variable': [ref_var],\n                'value': [1.0],\n            } | {\n                col_id: ['*']\n                for col_id, field in self._fields.items() if col_id in aggregated\n            }))\n        if agg_append:\n            agg_append = pd.concat(agg_append).reset_index(drop=True)\n            for col_id, field in self._fields.items():\n                if col_id not in aggregated:\n                    continue\n                agg_append = field.select_and_expand(agg_append, col_id, aggregated[col_id].unique().tolist())\n        else:\n            agg_append = None\n\n        # convert return list to dataframe, reset index, and clean up\n        return self._cleanup(pd.concat([aggregated, agg_append]), var_units)\n\n    # clean up: sort columns and rows, round values, insert units\n    def _cleanup(self, df: pd.DataFrame, var_units: dict[str, str]) -&gt; pd.DataFrame:\n        # sort columns and rows\n        cols_sorted = (\n            [col_id for col_id, field in self._fields.items() if isinstance(field, CustomFieldDefinition)] +\n            ['source', 'variable', 'reference_variable', 'region', 'period', 'value']\n        )\n        cols_sorted = [c for c in cols_sorted if c in df.columns]\n        df = df[cols_sorted]\n        df = df \\\n            .sort_values(by=[c for c in cols_sorted if c in df and c != 'value']) \\\n            .reset_index(drop=True)\n\n        # round values\n        df['value'] = df['value'].apply(\n            lambda cell: cell if pd.isnull(cell) else round(cell, sigfigs=4, warn=False)\n        )\n\n        # insert column containing units\n        df.insert(df.columns.tolist().index('value'), 'unit', np.nan)\n        if 'reference_variable' in df:\n            df['unit'] = df.apply(\n                lambda row: combine_units(var_units[row['variable']], var_units[row['reference_variable']])\n                            if not pd.isnull(row['reference_variable']) else\n                            var_units[row['variable']],\n                axis=1,\n            )\n        else:\n            df['unit'] = df['variable'].map(var_units)\n\n        return df\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.DataSet.data","title":"<code>data</code>  <code>property</code>","text":"<p>str: Get or set dataframe</p>"},{"location":"code/python/noslag/#python.posted.noslag.DataSet.__init__","title":"<code>__init__(parent_variable, include_databases=None, file_paths=None, check_inconsistencies=False, data=None)</code>","text":"<p>Initialise parent class and fields, load data from specified databases and files</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def __init__(self,\n             parent_variable: str,\n             include_databases: Optional[list[str] | tuple[str]] = None,\n             file_paths: Optional[list[Path]] = None,\n             check_inconsistencies: bool = False,\n             data: Optional[pd.DataFrame] = None,\n             ):\n    '''Initialise parent class and fields, load data from specified databases and files\n\n\n    '''\n    TEBase.__init__(self, parent_variable)\n\n    # initialise fields\n    self._df = None\n    self._columns = base_columns\n    self._fields = {\n        col_id: field\n        for col_id, field in self._columns.items()\n        if isinstance(field, AbstractFieldDefinition)\n    }\n    self._masks = []\n\n    # Load data if provided, otherwise load from TEDataFiles\n    if data is not None:\n        self._df = data\n    else:\n        # read TEDataFiles and combine into dataset\n        include_databases = list(include_databases) if include_databases is not None else list(databases.keys())\n        self._df = self._load_files(include_databases, file_paths or [], check_inconsistencies)\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.DataSet.aggregate","title":"<code>aggregate(override=None, drop_singular_fields=True, extrapolate_period=True, agg=None, masks=None, masks_database=True, **field_vals_select)</code>","text":"<p>Aggregates data based on specified parameters, applies masks, and cleans up the resulting DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>override</code> <code>Optional[dict[str, str]]</code> <p>Dictionary with key, value paris of variables to override</p> <code>None</code> <code>drop_singular_fields</code> <code>bool</code> <p>If True, drop custom fields with only one value</p> <code>True</code> <code>extrapolate_period</code> <code>bool</code> <p>If True, extrapolate values by extrapolation, if no value for this period is given</p> <code>True</code> <code>agg</code> <code>Optional[str | list[str] | tuple[str]]</code> <p>Specifies which fields to aggregate over.</p> <code>None</code> <code>masks</code> <code>Optional[list[Mask]]</code> <p>Specifies a list of Mask objects that will be applied to the data during aggregation. These masks can be used to filter or weight the data based on certain conditions defined in the Mask objects.</p> <code>None</code> <code>masks_database</code> <code>bool</code> <p>Determines whether to include masks from databases in the aggregation process. If set to <code>True</code>, masks from databases will be included along with any masks provided as function arguments. If set to <code>False</code>, only the masks provided as function argruments will be applied</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>The <code>aggregate</code> method returns a pandas DataFrame that has been cleaned up and aggregated based on the specified parameters and input data. The method performs aggregation over component fields and cases fields, applies weights based on masks, drops rows with NaN weights, aggregates with weights, inserts reference variables, sorts columns and rows, rounds values, and inserts units before returning the final cleaned and aggregated DataFrame.</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def aggregate(self, override: Optional[dict[str, str]] = None,\n              drop_singular_fields: bool = True,\n              extrapolate_period: bool = True,\n              agg: Optional[str | list[str] | tuple[str]] = None,\n              masks: Optional[list[Mask]] = None,\n              masks_database: bool = True,\n              **field_vals_select) -&gt; pd.DataFrame:\n    '''Aggregates data based on specified parameters, applies masks,\n    and cleans up the resulting DataFrame.\n\n    Parameters\n    ----------\n    override: Optional[dict[str, str]]\n        Dictionary with key, value paris of variables to override\n    drop_singular_fields: bool, optional\n        If True, drop custom fields with only one value\n    extrapolate_period: bool, optional\n        If True, extrapolate values by extrapolation, if no value for this period is given\n    agg : Optional[str | list[str] | tuple[str]]\n        Specifies which fields to aggregate over.\n    masks : Optional[list[Mask]]\n        Specifies a list of Mask objects that will be applied to the data during aggregation.\n        These masks can be used to filter or weight the\n        data based on certain conditions defined in the Mask objects.\n    masks_database : bool, optional\n        Determines whether to include masks from databases in the aggregation process.\n        If set to `True`, masks from databases will be included along with any masks provided as function arguments.\n        If set to `False`, only the masks provided as function argruments will be applied\n\n    Returns\n    -------\n    pd.DataFrame\n        The `aggregate` method returns a pandas DataFrame that has been cleaned up and aggregated based\n        on the specified parameters and input data. The method performs aggregation over component\n        fields and cases fields, applies weights based on masks, drops rows with NaN weights, aggregates\n        with weights, inserts reference variables, sorts columns and rows, rounds values, and inserts\n        units before returning the final cleaned and aggregated DataFrame.\n\n    '''\n\n    # get selection\n    selected, var_units, var_references = self._select(override,\n                                                       extrapolate_period,\n                                                       drop_singular_fields,\n                                                       **field_vals_select)\n\n    # compile masks from databases and function argument into one list\n    if masks is not None and any(not isinstance(m, Mask) for m in masks):\n        raise Exception(\"Function argument 'masks' must contain a list of posted.masking.Mask objects.\")\n    masks = (self._masks if masks_database else []) + (masks or [])\n\n    # aggregation\n    component_fields = [\n        col_id for col_id, field in self._fields.items()\n        if field.field_type == 'component'\n    ]\n    if agg is None:\n        agg = component_fields + ['source']\n    else:\n        if isinstance(agg, tuple):\n            agg = list(agg)\n        elif not isinstance(agg, list):\n            agg = [agg]\n        for a in agg:\n            if not isinstance(a, str):\n                raise Exception(f\"Field ID in argument 'agg' must be a string but found: {a}\")\n            if not any(a == col_id for col_id in self._fields):\n                raise Exception(f\"Field ID in argument 'agg' is not a valid field: {a}\")\n\n    # aggregate over component fields\n    group_cols = [\n        c for c in selected.columns\n        if not (c == 'value' or (c in agg and c in component_fields))\n    ]\n    aggregated = selected \\\n        .groupby(group_cols, dropna=False) \\\n        .agg({'value': 'sum'}) \\\n        .reset_index()\n\n    # aggregate over cases fields\n    group_cols = [\n        c for c in aggregated.columns\n        if not (c == 'value' or c in agg)\n    ]\n    ret = []\n    for keys, rows in aggregated.groupby(group_cols, dropna=False):\n        # set default weights to 1.0\n        rows = rows.assign(weight=1.0)\n\n        # update weights by applying masks\n        for mask in masks:\n            if mask.matches(rows):\n                rows['weight'] *= mask.get_weights(rows)\n\n        # drop all rows with weights equal to nan\n        rows.dropna(subset='weight', inplace=True)\n\n        if not rows.empty:\n            # aggregate with weights\n            out = rows \\\n                .groupby(group_cols, dropna=False)[['value', 'weight']] \\\n                .apply(lambda cols: pd.Series({\n                    'value': np.average(cols['value'], weights=cols['weight']),\n                }))\n\n            # add to return list\n            ret.append(out)\n    aggregated = pd.concat(ret).reset_index()\n\n    # insert reference variables\n    var_ref_unique = {\n        var_references[var]\n        for var in aggregated['variable'].unique()\n        if not pd.isnull(var_references[var])\n    }\n    agg_append = []\n    for ref_var in var_ref_unique:\n        agg_append.append(pd.DataFrame({\n            'variable': [ref_var],\n            'value': [1.0],\n        } | {\n            col_id: ['*']\n            for col_id, field in self._fields.items() if col_id in aggregated\n        }))\n    if agg_append:\n        agg_append = pd.concat(agg_append).reset_index(drop=True)\n        for col_id, field in self._fields.items():\n            if col_id not in aggregated:\n                continue\n            agg_append = field.select_and_expand(agg_append, col_id, aggregated[col_id].unique().tolist())\n    else:\n        agg_append = None\n\n    # convert return list to dataframe, reset index, and clean up\n    return self._cleanup(pd.concat([aggregated, agg_append]), var_units)\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.DataSet.normalise","title":"<code>normalise(override=None, inplace=False)</code>","text":"<p>normalise data: default reference units, reference value equal to 1.0, default reported units</p> <p>Parameters:</p> Name Type Description Default <code>override</code> <code>Optional[dict[str, str]]</code> <p>Dictionary with key, value pairs of variables to override</p> <code>None</code> <code>inplace</code> <code>bool</code> <p>Wether to do the normalisation in place</p> <code>False</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>if inplace is false, returns normalised dataframe</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def normalise(self, override: Optional[dict[str, str]] = None, inplace: bool = False) -&gt; pd.DataFrame | None:\n    '''\n    normalise data: default reference units, reference value equal to 1.0, default reported units\n\n    Parameters\n    ----------\n    override: Optional[dict[str,str]], optional\n        Dictionary with key, value pairs of variables to override\n    inplace: bool, optional\n        Wether to do the normalisation in place\n\n    Returns\n    -------\n    pd.DataFrame\n        if inplace is false, returns normalised dataframe'''\n    normalised, _ = self._normalise(override)\n    if inplace:\n        self._df = normalised\n        return\n    else:\n        return normalised\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.DataSet.select","title":"<code>select(override=None, drop_singular_fields=True, extrapolate_period=True, **field_vals_select)</code>","text":"<p>Select desired data from the dataframe</p> <p>Parameters:</p> Name Type Description Default <code>override</code> <code>Optional[dict[str, str]]</code> <p>Dictionary with key, value paris of variables to override</p> <code>None</code> <code>drop_singular_fields</code> <code>bool</code> <p>If True, drop custom fields with only one value</p> <code>True</code> <code>extrapolate_period</code> <code>bool</code> <p>If True, extrapolate values by extrapolation, if no value for this period is given</p> <code>True</code> <code>**field_vals_select</code> <p>IDs of values to select</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with selected Values</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def select(self,\n           override: Optional[dict[str, str]] = None,\n           drop_singular_fields: bool = True,\n           extrapolate_period: bool = True,\n           **field_vals_select) -&gt; pd.DataFrame:\n    '''Select desired data from the dataframe\n\n    Parameters\n    ----------\n    override: Optional[dict[str, str]]\n        Dictionary with key, value paris of variables to override\n    drop_singular_fields: bool, optional\n        If True, drop custom fields with only one value\n    extrapolate_period: bool, optional\n        If True, extrapolate values by extrapolation, if no value for this period is given\n    **field_vals_select\n        IDs of values to select\n\n    Returns\n    -------\n    pd.DataFrame\n        DataFrame with selected Values\n        '''\n    selected, var_units, var_references = self._select(\n        override,\n        drop_singular_fields,\n        extrapolate_period,\n        **field_vals_select,\n    )\n    selected.insert(selected.columns.tolist().index('variable'), 'reference_variable', np.nan)\n    selected['reference_variable'] = selected['variable'].map(var_references)\n    return self._cleanup(selected, var_units)\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.HarmoniseMappingFailure","title":"<code>HarmoniseMappingFailure</code>","text":"<p>               Bases: <code>Warning</code></p> <p>Warning raised for rows in TEDataSets where mappings fail.</p> <p>Parameters:</p> Name Type Description Default <code>row_data</code> <code>DataFrame</code> <p>Contains the Data on the rows to map</p> required <code>message</code> <code>str</code> <p>Contains the message of the failure</p> <code>'Failure when selecting from dataset.'</code> <p>Attributes:</p> Name Type Description <code>row_data</code> <code>DataFrame</code> <p>the data of the row that causes the failure</p> <code>message</code> <code>str</code> <p>explanation of the error</p> Source code in <code>python/posted/noslag.py</code> <pre><code>class HarmoniseMappingFailure(Warning):\n    \"\"\"Warning raised for rows in TEDataSets where mappings fail.\n\n    Parameters\n    ----------\n    row_data: pd.DataFrame\n        Contains the Data on the rows to map\n    message: str, optional\n        Contains the message of the failure\n\n    Attributes\n    ----------\n    row_data\n        the data of the row that causes the failure\n    message\n        explanation of the error\n    \"\"\"\n    def __init__(self, row_data: pd.DataFrame, message: str = \"Failure when selecting from dataset.\"):\n        '''Save constructor arguments as public fields, compose warning message, call super constructor'''\n        # save constructor arguments as public fields\n        self.row_data: pd.DataFrame = row_data\n        self.message: str = message\n\n        # compose warning message\n        warning_message: str = message + f\"\\n{row_data}\"\n\n        # call super constructor\n        super().__init__(warning_message)\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.HarmoniseMappingFailure.__init__","title":"<code>__init__(row_data, message='Failure when selecting from dataset.')</code>","text":"<p>Save constructor arguments as public fields, compose warning message, call super constructor</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def __init__(self, row_data: pd.DataFrame, message: str = \"Failure when selecting from dataset.\"):\n    '''Save constructor arguments as public fields, compose warning message, call super constructor'''\n    # save constructor arguments as public fields\n    self.row_data: pd.DataFrame = row_data\n    self.message: str = message\n\n    # compose warning message\n    warning_message: str = message + f\"\\n{row_data}\"\n\n    # call super constructor\n    super().__init__(warning_message)\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.collect_files","title":"<code>collect_files(parent_variable, include_databases=None)</code>","text":"<p>Takes a parent variable and optional list of databases to include, checks for their existence, and collects files and directories based on the parent variable.</p> <p>Parameters:</p> Name Type Description Default <code>parent_variable</code> <code>str</code> <p>Variable to collect files on</p> required <code>include_databases</code> <code>Optional[list[str]]</code> <p>List of Database IDs to collect files from</p> <code>None</code> <p>Returns:</p> Type Description <code>    list[tuple]</code> <pre><code>List of tuples containing the parent variable and the\n</code></pre> <p>database ID for each file found in the specified directories.</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def collect_files(parent_variable: str, include_databases: Optional[list[str]] = None):\n    '''Takes a parent variable and optional list of databases to include,\n    checks for their existence, and collects files and directories based on the parent variable.\n\n    Parameters\n    ----------\n    parent_variable : str\n        Variable to collect files on\n    include_databases : Optional[list[str]]\n        List of Database IDs to collect files from\n\n    Returns\n    -------\n        list[tuple]\n            List of tuples containing the parent variable and the\n        database ID for each file found in the specified directories.\n\n    '''\n    if not parent_variable:\n        raise Exception('Variable may not me empty.')\n\n    # check that the requested database to include can be found\n    if include_databases is not None:\n        for database_id in include_databases:\n            if not (database_id in databases and databases[database_id].exists()):\n                raise Exception(f\"Could not find database '{database_id}'.\")\n\n    ret = []\n    for database_id, database_path in databases.items():\n        # skip ted paths not requested to include\n        if include_databases is not None and database_id not in include_databases: continue\n\n        # find top-level file and directory\n        top_path = '/'.join(parent_variable.split('|'))\n        top_file = database_path / 'tedfs' / (top_path + '.csv')\n        top_directory = database_path / 'tedfs' / top_path\n\n        # add top-level file if it exists\n        if top_file.exists() and top_file.is_file():\n            ret.append((parent_variable, database_id))\n\n        # add all files contained in top-level directory\n        if top_directory.exists() and top_directory.is_dir():\n            for sub_file in top_directory.rglob('*.csv'):\n                sub_variable = parent_variable + '|' + sub_file.relative_to(top_directory).name.rstrip('.csv')\n                ret.append((sub_variable, database_id))\n\n        # loop over levels\n        levels = parent_variable.split('|')\n        for l in range(0, len(levels)):\n            # find top-level file and directory\n            top_path = '/'.join(levels[:l])\n            parent_file = database_path / 'tedfs' / (top_path + '.csv')\n\n            # add parent file if it exists\n            if parent_file.exists() and parent_file.is_file():\n                parent_variable = '|'.join(levels[:l])\n                ret.append((parent_variable, database_id))\n\n    return ret\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.combine_units","title":"<code>combine_units(numerator, denominator)</code>","text":"<p>Combine fraction of two units into updated unit string</p> <p>Parameters:</p> Name Type Description Default <code>numerator</code> <code>str</code> <p>numerator of the fraction</p> required <code>denominator</code> <code>str</code> <p>denominator of the fraction</p> required <p>Returns:</p> Type Description <code>    str</code> <p>updated unit string after simplification</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def combine_units(numerator: str, denominator: str):\n    '''Combine fraction of two units into updated unit string\n\n    Parameters\n    ----------\n    numerator: str\n        numerator of the fraction\n    denominator: str\n        denominator of the fraction\n\n    Returns\n    -------\n        str\n            updated unit string after simplification\n    '''\n\n\n    ret = ureg(f\"{numerator}/({denominator})\").u\n    # chekc if ret is dimensionless, if not return ret, else return the explicit quotient\n    if not ret.dimensionless:\n        return str(ret)\n    else:\n        return (f\"{numerator}/({denominator})\"\n                if '/' in denominator else\n                f\"{numerator}/{denominator}\")\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.normalise_units","title":"<code>normalise_units(df, level, var_units, var_flow_ids)</code>","text":"<p>Takes a DataFrame with reported or reference data, along with dictionaries mapping variable units and flow IDs, and normalizes the units of the variables in the DataFrame based on the provided mappings.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to be normalised</p> required <code>level</code> <code>Literal['reported', 'reference']</code> <p>Specifies whether the data should be normalised on the reported or reference values</p> required <code>var_units</code> <code>dict[str, str]</code> <p>Dictionary that maps a combination of parent variable and variable to its corresponding unit. The keys in the dictionary are in the format \"{parent_variable}|{variable}\", and the values are the units associated with that variable.</p> required <code>var_flow_ids</code> <code>dict[str, str]</code> <p>Dictionary that maps a combination of parent variable and variable to a specific flow ID. This flow ID is used for unit conversion in the <code>normalise_units</code> function.</p> required <p>Returns:</p> Type Description <code>    pd.DataFrame</code> <p>Normalised dataframe</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def normalise_units(df: pd.DataFrame, level: Literal['reported', 'reference'], var_units: dict[str, str],\n                       var_flow_ids: dict[str, str]):\n    '''\n    Takes a DataFrame with reported or reference data, along with\n    dictionaries mapping variable units and flow IDs, and normalizes the units of the variables in the\n    DataFrame based on the provided mappings.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Dataframe to be normalised\n    level : Literal['reported', 'reference']\n        Specifies whether the data should be normalised on the reported or reference values\n    var_units : dict[str, str]\n        Dictionary that maps a combination of parent variable and variable\n        to its corresponding unit. The keys in the dictionary are in the format \"{parent_variable}|{variable}\",\n        and the values are the units associated with that variable.\n    var_flow_ids : dict[str, str]\n        Dictionary that maps a combination of parent variable and variable to a\n        specific flow ID. This flow ID is used for unit conversion in the `normalise_units` function.\n\n    Returns\n    -------\n        pd.DataFrame\n            Normalised dataframe\n\n    '''\n\n    prefix = '' if level == 'reported' else 'reference_'\n    var_col_id = prefix + 'variable'\n    value_col_id = prefix + 'value'\n    unit_col_id = prefix + 'unit'\n    df_tmp = pd.concat([\n        df,\n        df.apply(\n            lambda row: var_units[f\"{row['parent_variable']}|{row[var_col_id]}\"]\n            if isinstance(row[var_col_id], str) else np.nan,\n            axis=1,\n        )\n        .to_frame('target_unit'),\n        df.apply(\n            lambda row: var_flow_ids[f\"{row['parent_variable']}|{row[var_col_id]}\"]\n            if isinstance(row[var_col_id], str) else np.nan,\n            axis=1,\n        )\n        .to_frame('target_flow_id'),\n    ], axis=1)\n\n    # Apply unit conversion\n    conv_factor = df_tmp.apply(\n        lambda row: unit_convert(row[unit_col_id], row['target_unit'], row['target_flow_id'])\n        if not np.isnan(row[value_col_id]) else 1.0,\n        axis=1,\n    )\n\n    # Update value column with conversion factor\n    df_tmp[value_col_id] *= conv_factor\n\n    # If level is 'reported', update uncertainty column with conversion factor\n    if level == 'reported':\n        df_tmp['uncertainty'] *= conv_factor\n\n    # Uupdate unit columns\n    df_tmp[unit_col_id] = df_tmp['target_unit']\n\n    # Drop unneccessary columns and return\n    return df_tmp.drop(columns=['target_unit', 'target_flow_id'])\n</code></pre>"},{"location":"code/python/noslag/#python.posted.noslag.normalise_values","title":"<code>normalise_values(df)</code>","text":"<p>Takes a DataFrame as input, normalizes the 'value' and 'uncertainty' columns by the reference value, and updates the 'reference_value' column accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>Dataframe to be normalised</p> required <p>Returns:</p> Type Description <code>    pd.DataFrame</code> <p>Returns a modified DataFrame where the 'value' column has been divided by the 'reference_value' column (or 1.0 if 'reference_value' is null), the 'uncertainty' column has been divided by the 'reference_value' column, and the 'reference_value' column has been replaced with 1.0 if it was not null, otherwise</p> Source code in <code>python/posted/noslag.py</code> <pre><code>def normalise_values(df: pd.DataFrame):\n    '''Takes a DataFrame as input, normalizes the 'value' and 'uncertainty'\n    columns by the reference value, and updates the 'reference_value' column accordingly.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        Dataframe to be normalised\n\n    Returns\n    -------\n        pd.DataFrame\n            Returns a modified DataFrame where the 'value' column has been\n            divided by the 'reference_value' column (or 1.0 if 'reference_value' is null), the 'uncertainty'\n            column has been divided by the 'reference_value' column, and the 'reference_value' column has been\n            replaced with 1.0 if it was not null, otherwise\n\n    '''\n    # Calculate reference value\n    reference_value =  df.apply(\n        lambda row:\n            row['reference_value']\n            if not pd.isnull(row['reference_value']) else\n            1.0,\n        axis=1,\n    )\n    # Calculate new value, reference value and uncertainty\n    value_new = df['value'] / reference_value\n    uncertainty_new = df['uncertainty'] / reference_value\n    reference_value_new = df.apply(\n        lambda row:\n            1.0\n            if not pd.isnull(row['reference_value']) else\n            np.nan,\n        axis=1,\n    )\n    # Assign new values to dataframe and return\n    return df.assign(value=value_new, uncertainty=uncertainty_new, reference_value=reference_value_new)\n</code></pre>"},{"location":"code/python/read/","title":"read","text":""},{"location":"code/python/read/#python.posted.read.read_csv_file","title":"<code>read_csv_file(fpath)</code>","text":"<p>Read CSV data file</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>str</code> <p>Path of the file to read</p> required <p>Returns:</p> Type Description <code>    pd.DataFrame</code> <p>DataFrame containg the data of the CSV</p> Source code in <code>python/posted/read.py</code> <pre><code>def read_csv_file(fpath: str):\n    \"\"\"\n    Read CSV data file\n\n    Parameters\n    ----------\n    fpath: str\n        Path of the file to read\n    Returns\n    -------\n        pd.DataFrame\n            DataFrame containg the data of the CSV\n    \"\"\"\n    return pd.read_csv(fpath)\n</code></pre>"},{"location":"code/python/read/#python.posted.read.read_yml_file","title":"<code>read_yml_file(fpath)</code>","text":"<p>Read YAML config file</p> <p>Parameters:</p> Name Type Description Default <code>fpath</code> <code>Path</code> <p>Path of the file to read</p> required <p>Returns:</p> Type Description <code>    dict</code> <p>Dictionary containing config info</p> Source code in <code>python/posted/read.py</code> <pre><code>def read_yml_file(fpath: Path):\n    \"\"\"\n    Read YAML config file\n\n    Parameters\n    ----------\n    fpath: str\n        Path of the file to read\n    Returns\n    -------\n        dict\n            Dictionary containing config info\n    \"\"\"\n    fhandle = open(fpath, 'r', encoding='utf-8')\n    ret = yaml.load(stream=fhandle, Loader=yaml.FullLoader)\n    fhandle.close()\n    return ret\n</code></pre>"},{"location":"code/python/sources/","title":"sources","text":""},{"location":"code/python/sources/#python.posted.sources.dump_sources","title":"<code>dump_sources(file_path)</code>","text":"<p>Parses BibTeX files, formats the data, and exports it into a CSV or Excel file using pandas.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str | Path</code> <p>Path to the file where the formatted sources should be exported to.  It can be either a string representing the file path or a <code>Path</code> object from the <code>pathlib</code> module.</p> required Source code in <code>python/posted/sources.py</code> <pre><code>def dump_sources(file_path: str | Path):\n    '''Parses BibTeX files, formats the data, and exports it into a CSV or Excel\n    file using pandas.\n\n    Parameters\n    ----------\n    file_path : str | Path\n        Path to the file where the formatted sources should be exported to.\n         It can be either a string representing the file path or a `Path` object\n        from the `pathlib` module.\n\n    '''\n    # convert string to pathlib.Path if necessary\n    if isinstance(file_path, str):\n        file_path = Path(file_path)\n\n    # define styles and formats\n    style = find_plugin('pybtex.style.formatting', 'apa')()\n    # format_html = find_plugin('pybtex.backends', 'html')()\n    format_plain = find_plugin('pybtex.backends', 'plaintext')()\n\n    # parse bibtex file\n    parser = bibtex.Parser()\n\n    # loop over databases\n    formatted = []\n    for database_path in databases.values():\n        bib_data = parser.parse_file(database_path / 'sources.bib')\n        formatted += format_sources(bib_data, style, format_plain)\n\n    # convert to dataframe\n    df = pd.DataFrame.from_records(formatted)\n\n    # dump dataframe with pandas to CSV or Excel spreadsheet\n    if file_path.suffix == '.csv':\n        df.to_csv(Path(file_path))\n    elif file_path.suffix in ['.xls', '.xlsx']:\n        df.to_excel(Path(file_path))\n    else:\n        raise Exception('Unknown file suffix!')\n</code></pre>"},{"location":"code/python/sources/#python.posted.sources.format_sources","title":"<code>format_sources(bib_data, style, form, exclude_fields=None)</code>","text":"<p>Takes bibliographic data, a citation style, a citation form, and optional excluded fields, and returns a formatted list of sources based on the specified style and form.</p> <p>Parameters:</p> Name Type Description Default <code>bib_data</code> <p>Contains bibliographic information, such as author, title, references or citations.</p> required <code>style</code> <p>Specifies the formatting style for the bibliography entries.</p> required <code>form</code> <p>Specifies the format in which the citation should be rendered. It determines how the citation information will be displayed or structured in the final output.</p> required <code>exclude_fields</code> <p>Specifies a list of fields that should be excluded from the final output. These fields will be removed from the entries before</p> <code>None</code> <code>formatting</code> required <p>Returns:</p> Type Description <code>    list[dict]</code> <p>A list of dictionaries containing the identifier, citation, DOI, and URL information for each entry in the bibliography data, formatted according to the specified style and form, with any excluded fields removed.</p> Source code in <code>python/posted/sources.py</code> <pre><code>def format_sources(bib_data, style, form, exclude_fields = None):\n    '''\n    Takes bibliographic data, a citation style, a citation form, and\n    optional excluded fields, and returns a formatted list of sources based on the specified style and\n    form.\n\n    Parameters\n    ----------\n    bib_data\n        Contains bibliographic information, such as author, title, references or citations.\n    style\n        Specifies the formatting style for the bibliography entries.\n    form\n        Specifies the format in which the citation should be rendered. It determines how the citation information will be displayed or\n        structured in the final output.\n    exclude_fields\n        Specifies a list of fields that should be excluded from the final output. These fields will be removed from the entries before\n    formatting and returning the citation data.\n\n    Returns\n    -------\n        list[dict]\n            A list of dictionaries containing the identifier, citation, DOI, and URL information for each entry\n            in the bibliography data, formatted according to the specified style and form, with any excluded\n            fields removed.\n\n    '''\n    exclude_fields = exclude_fields or []\n\n    if exclude_fields:\n        for entry in bib_data.entries.values():\n            for ef in exclude_fields:\n                if ef in entry.fields.__dict__['_dict']:\n                    del entry.fields.__dict__['_dict'][ef]\n\n    ret = []\n    for identifier in bib_data.entries:\n        entry = bib_data.entries[identifier]\n        fields = entry.fields.__dict__['_dict']\n        ret.append({\n            'identifier': identifier,\n            'citation': next(style.format_entries([entry])).text.render(form),\n            'doi': fields['doi'] if 'doi' in fields else '',\n            'url': fields['url'] if 'url' in fields else '',\n        })\n\n    return ret\n</code></pre>"},{"location":"code/python/tedf/","title":"tedf","text":""},{"location":"code/python/tedf/#python.posted.tedf.TEBase","title":"<code>TEBase</code>","text":"<p>Base Class for Technoeconomic Data</p> <p>Parameters:</p> Name Type Description Default <code>parent_variable</code> <code>str</code> <p>Variable from which Data should be collected</p> required Source code in <code>python/posted/tedf.py</code> <pre><code>class TEBase:\n    \"\"\"\n    Base Class for Technoeconomic Data\n\n    Parameters\n    ----------\n    parent_variable: str\n        Variable from which Data should be collected\n    \"\"\"\n    # initialise\n    def __init__(self, parent_variable: str):\n        \"\"\" Set parent variable and technology specifications (var_specs) from input\"\"\"\n        self._parent_variable: str = parent_variable\n        self._var_specs: dict = {key: val for key, val in variables.items() if key.startswith(self._parent_variable)}\n\n    @property\n    def parent_variable(self) -&gt; str:\n        \"\"\" Get parent variable\"\"\"\n        return self._parent_variable\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEBase.parent_variable","title":"<code>parent_variable: str</code>  <code>property</code>","text":"<p>Get parent variable</p>"},{"location":"code/python/tedf/#python.posted.tedf.TEBase.__init__","title":"<code>__init__(parent_variable)</code>","text":"<p>Set parent variable and technology specifications (var_specs) from input</p> Source code in <code>python/posted/tedf.py</code> <pre><code>def __init__(self, parent_variable: str):\n    \"\"\" Set parent variable and technology specifications (var_specs) from input\"\"\"\n    self._parent_variable: str = parent_variable\n    self._var_specs: dict = {key: val for key, val in variables.items() if key.startswith(self._parent_variable)}\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF","title":"<code>TEDF</code>","text":"<p>               Bases: <code>TEBase</code></p> <p>Class to store Technoeconomic DataFiles</p> <p>Parameters:</p> Name Type Description Default <code>parent_variable</code> <code>str</code> <p>Variable from which Data should be collected</p> required <code>database_id</code> <code>str</code> <p>Database from which to load data</p> <code>'public'</code> <code>file_path</code> <code>Optional[Path]</code> <p>File Path from which to load file</p> <code>None</code> <code>data</code> <code>Optional[DataFrame]</code> <p>Specific Technoeconomic data</p> <code>None</code> <p>Methods:</p> Name Description <code>load</code> <p>Load TEDataFile if it has not been read yet</p> <code>read</code> <p>Read TEDF from CSV file</p> <code>write</code> <p>Write TEDF to CSV file</p> <code>check</code> <p>Check if TEDF is consistent</p> <code>check_row</code> <p>Check that row in TEDF is consistent and return all inconsistencies found for row</p> Source code in <code>python/posted/tedf.py</code> <pre><code>class TEDF(TEBase):\n    \"\"\"\n    Class to store Technoeconomic DataFiles\n\n    Parameters\n    ----------\n    parent_variable: str\n        Variable from which Data should be collected\n    database_id: str, default: public\n        Database from which to load data\n    file_path: Path, optional\n        File Path from which to load file\n    data: pd.DataFrame, optional\n        Specific Technoeconomic data\n\n    Methods\n    ----------\n    load\n        Load TEDataFile if it has not been read yet\n    read\n        Read TEDF from CSV file\n    write\n        Write TEDF to CSV file\n    check\n        Check if TEDF is consistent\n    check_row\n        Check that row in TEDF is consistent and return all inconsistencies found for row\n    \"\"\"\n\n    # typed delcarations\n    _df: None | pd.DataFrame\n    _inconsistencies: dict\n    _file_path: None | Path\n    _fields: dict[str, AbstractFieldDefinition]\n    _columns: dict[str, AbstractColumnDefinition]\n\n\n    def __init__(self,\n                 parent_variable: str,\n                 database_id: str = 'public',\n                 file_path: Optional[Path] = None,\n                 data: Optional[pd.DataFrame] = None,\n                 ):\n        \"\"\" Initialise parent class and object fields\"\"\"\n        TEBase.__init__(self, parent_variable)\n\n        self._df = data\n        self._inconsistencies = {}\n        self._file_path = (\n            None if data is not None else\n            file_path if file_path is not None else\n            databases[database_id] / 'tedfs' / ('/'.join(self._parent_variable.split('|')) + '.csv')\n        )\n        self._fields, comments = read_fields(self._parent_variable)\n        self._columns = self._fields | base_columns | comments\n\n    @property\n    def file_path(self) -&gt; Path:\n        \"\"\" Get or set the file File Path\"\"\"\n        return self._file_path\n\n    @file_path.setter\n    def file_path(self, file_path: Path):\n        self._file_path = file_path\n\n\n    def load(self):\n        \"\"\"\n        load TEDataFile (only if it has not been read yet)\n\n        Warns\n        ----------\n        warning\n            Warns if TEDF is already loaded\n        Returns\n        --------\n            TEDF\n                Returns the TEDF object it is called on\n        \"\"\"\n        if self._df is None:\n            self.read()\n        else:\n            warnings.warn('TEDF is already loaded. Please execute .read() if you want to load from file again.')\n\n        return self\n\n    def read(self):\n        \"\"\"\n        read TEDF from CSV file\n\n        Raises\n        ------\n        Exception\n            If there is no file path from which to read\n        \"\"\"\n\n        if self._file_path is None:\n            raise Exception('Cannot read from file, as this TEDF object has been created from a dataframe.')\n\n        # read CSV file\n        self._df = pd.read_csv(\n            self._file_path,\n            sep=',',\n            quotechar='\"',\n            encoding='utf-8',\n        )\n\n        # check column IDs match base columns and fields\n        if not all(c in self._columns for c in self._df.columns):\n            raise Exception(f\"Column IDs used in CSV file do not match columns definition: {self._df.columns.tolist()}\")\n\n        # adjust row index to start at 1 instead of 0\n        self._df.index += 1\n\n        # insert missing columns and reorder via reindexing, then update dtypes\n        df_new = self._df.reindex(columns=list(self._columns.keys()))\n        for col_id, col in self._columns.items():\n            if col_id in self._df:\n                continue\n            df_new[col_id] = df_new[col_id].astype(col.dtype)\n            df_new[col_id] = col.default\n        self._df = df_new\n\n    def write(self):\n        \"\"\"\n        Write TEDF to CSV file\n\n        Raises\n        ------\n        Exception\n            If there is no file path that specifies where to write\n        \"\"\"\n        if self._file_path is None:\n            raise Exception('Cannot write to file, as this TEDataFile object has been created from a dataframe. Please '\n                            'first set a file path on this object.')\n\n        self._df.to_csv(\n            self._file_path,\n            index=False,\n            sep=',',\n            quotechar='\"',\n            encoding='utf-8',\n            na_rep='',\n        )\n\n\n    @property\n    def data(self) -&gt; pd.DataFrame:\n        \"\"\"Get data, i.e. access dataframe\"\"\"\n        return self._df\n\n    @property\n    def inconsistencies(self) -&gt; dict[int, TEDFInconsistencyException]:\n        \"\"\"Get inconsistencies\"\"\"\n        return self._inconsistencies\n\n    def check(self, raise_exception: bool = True):\n        \"\"\"\n        Check that TEDF is consistent and add inconsistencies to internal parameter\n\n        Parameters\n        ----------\n        raise_exception: bool, default: True\n            If exception is to be raised\n        \"\"\"\n        self._inconsistencies = {}\n\n        # check row consistency for each row individually\n        for row_id in self._df.index:\n            self._inconsistencies[row_id] = self.check_row(row_id, raise_exception=raise_exception)\n\n    def check_row(self, row_id: int, raise_exception: bool) -&gt; list[TEDFInconsistencyException]:\n        \"\"\"\n        Check that row in TEDF is consistent and return all inconsistencies found for row\n\n        Parameters\n        ----------\n        row_id: int\n            Index of the row to check\n        raise_exception: bool\n            If exception is to be raised\n\n        Returns\n        -------\n            list\n                List of inconsistencies\n        \"\"\"\n        row = self._df.loc[row_id]\n        ikwargs = {'row_id': row_id, 'file_path': self._file_path, 'raise_exception': raise_exception}\n        ret = []\n\n        # check whether fields are among those defined in the technology specs\n        for col_id, col in self._columns.items():\n            cell = row[col_id]\n            if col.col_type == 'variable':\n                cell = cell if pd.isnull(cell) else self.parent_variable + '|' + cell\n            if not col.is_allowed(cell):\n                ret.append(new_inconsistency(\n                    message=f\"Invalid cell for column of type '{col.col_type}': {cell}\", col_id=col_id, **ikwargs,\n                ))\n\n        # check that reported and reference units match variable definition\n        for col_prefix in ['', 'reference_']:\n            raw_variable = row[col_prefix + 'variable']\n            col_id = col_prefix + 'unit'\n            unit = row[col_id]\n            if pd.isnull(raw_variable) and pd.isnull(unit):\n                continue\n            if pd.isnull(raw_variable) or pd.isnull(unit):\n                ret.append(new_inconsistency(\n                    message=f\"Variable and unit must either both be set or both be unset': {raw_variable} -- {unit}\",\n                    col_id=col_id, **ikwargs,\n                ))\n            variable = self.parent_variable + '|' + raw_variable\n            var_specs = variables[variable]\n            if 'dimension' not in var_specs:\n                if unit is not np.nan:\n                    ret.append(new_inconsistency(\n                        message=f\"Unexpected unit '{unit}' for {col_id}.\", col_id=col_id, **ikwargs,\n                    ))\n                continue\n            dimension = var_specs['dimension']\n\n            flow_id = var_specs['flow_id'] if 'flow_id' in var_specs else None\n            allowed, message = unit_allowed(unit=unit, flow_id=flow_id, dimension=dimension)\n            if not allowed:\n                ret.append(new_inconsistency(message=message, col_id=col_id, **ikwargs))\n\n        return ret\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.data","title":"<code>data: pd.DataFrame</code>  <code>property</code>","text":"<p>Get data, i.e. access dataframe</p>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.file_path","title":"<code>file_path: Path</code>  <code>property</code> <code>writable</code>","text":"<p>Get or set the file File Path</p>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.inconsistencies","title":"<code>inconsistencies: dict[int, TEDFInconsistencyException]</code>  <code>property</code>","text":"<p>Get inconsistencies</p>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.__init__","title":"<code>__init__(parent_variable, database_id='public', file_path=None, data=None)</code>","text":"<p>Initialise parent class and object fields</p> Source code in <code>python/posted/tedf.py</code> <pre><code>def __init__(self,\n             parent_variable: str,\n             database_id: str = 'public',\n             file_path: Optional[Path] = None,\n             data: Optional[pd.DataFrame] = None,\n             ):\n    \"\"\" Initialise parent class and object fields\"\"\"\n    TEBase.__init__(self, parent_variable)\n\n    self._df = data\n    self._inconsistencies = {}\n    self._file_path = (\n        None if data is not None else\n        file_path if file_path is not None else\n        databases[database_id] / 'tedfs' / ('/'.join(self._parent_variable.split('|')) + '.csv')\n    )\n    self._fields, comments = read_fields(self._parent_variable)\n    self._columns = self._fields | base_columns | comments\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.check","title":"<code>check(raise_exception=True)</code>","text":"<p>Check that TEDF is consistent and add inconsistencies to internal parameter</p> <p>Parameters:</p> Name Type Description Default <code>raise_exception</code> <code>bool</code> <p>If exception is to be raised</p> <code>True</code> Source code in <code>python/posted/tedf.py</code> <pre><code>def check(self, raise_exception: bool = True):\n    \"\"\"\n    Check that TEDF is consistent and add inconsistencies to internal parameter\n\n    Parameters\n    ----------\n    raise_exception: bool, default: True\n        If exception is to be raised\n    \"\"\"\n    self._inconsistencies = {}\n\n    # check row consistency for each row individually\n    for row_id in self._df.index:\n        self._inconsistencies[row_id] = self.check_row(row_id, raise_exception=raise_exception)\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.check_row","title":"<code>check_row(row_id, raise_exception)</code>","text":"<p>Check that row in TEDF is consistent and return all inconsistencies found for row</p> <p>Parameters:</p> Name Type Description Default <code>row_id</code> <code>int</code> <p>Index of the row to check</p> required <code>raise_exception</code> <code>bool</code> <p>If exception is to be raised</p> required <p>Returns:</p> Type Description <code>    list</code> <p>List of inconsistencies</p> Source code in <code>python/posted/tedf.py</code> <pre><code>def check_row(self, row_id: int, raise_exception: bool) -&gt; list[TEDFInconsistencyException]:\n    \"\"\"\n    Check that row in TEDF is consistent and return all inconsistencies found for row\n\n    Parameters\n    ----------\n    row_id: int\n        Index of the row to check\n    raise_exception: bool\n        If exception is to be raised\n\n    Returns\n    -------\n        list\n            List of inconsistencies\n    \"\"\"\n    row = self._df.loc[row_id]\n    ikwargs = {'row_id': row_id, 'file_path': self._file_path, 'raise_exception': raise_exception}\n    ret = []\n\n    # check whether fields are among those defined in the technology specs\n    for col_id, col in self._columns.items():\n        cell = row[col_id]\n        if col.col_type == 'variable':\n            cell = cell if pd.isnull(cell) else self.parent_variable + '|' + cell\n        if not col.is_allowed(cell):\n            ret.append(new_inconsistency(\n                message=f\"Invalid cell for column of type '{col.col_type}': {cell}\", col_id=col_id, **ikwargs,\n            ))\n\n    # check that reported and reference units match variable definition\n    for col_prefix in ['', 'reference_']:\n        raw_variable = row[col_prefix + 'variable']\n        col_id = col_prefix + 'unit'\n        unit = row[col_id]\n        if pd.isnull(raw_variable) and pd.isnull(unit):\n            continue\n        if pd.isnull(raw_variable) or pd.isnull(unit):\n            ret.append(new_inconsistency(\n                message=f\"Variable and unit must either both be set or both be unset': {raw_variable} -- {unit}\",\n                col_id=col_id, **ikwargs,\n            ))\n        variable = self.parent_variable + '|' + raw_variable\n        var_specs = variables[variable]\n        if 'dimension' not in var_specs:\n            if unit is not np.nan:\n                ret.append(new_inconsistency(\n                    message=f\"Unexpected unit '{unit}' for {col_id}.\", col_id=col_id, **ikwargs,\n                ))\n            continue\n        dimension = var_specs['dimension']\n\n        flow_id = var_specs['flow_id'] if 'flow_id' in var_specs else None\n        allowed, message = unit_allowed(unit=unit, flow_id=flow_id, dimension=dimension)\n        if not allowed:\n            ret.append(new_inconsistency(message=message, col_id=col_id, **ikwargs))\n\n    return ret\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.load","title":"<code>load()</code>","text":"<p>load TEDataFile (only if it has not been read yet)</p> <p>Warns:</p> Type Description <code>warning</code> <p>Warns if TEDF is already loaded</p> <p>Returns:</p> Type Description <code>    TEDF</code> <p>Returns the TEDF object it is called on</p> Source code in <code>python/posted/tedf.py</code> <pre><code>def load(self):\n    \"\"\"\n    load TEDataFile (only if it has not been read yet)\n\n    Warns\n    ----------\n    warning\n        Warns if TEDF is already loaded\n    Returns\n    --------\n        TEDF\n            Returns the TEDF object it is called on\n    \"\"\"\n    if self._df is None:\n        self.read()\n    else:\n        warnings.warn('TEDF is already loaded. Please execute .read() if you want to load from file again.')\n\n    return self\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.read","title":"<code>read()</code>","text":"<p>read TEDF from CSV file</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is no file path from which to read</p> Source code in <code>python/posted/tedf.py</code> <pre><code>def read(self):\n    \"\"\"\n    read TEDF from CSV file\n\n    Raises\n    ------\n    Exception\n        If there is no file path from which to read\n    \"\"\"\n\n    if self._file_path is None:\n        raise Exception('Cannot read from file, as this TEDF object has been created from a dataframe.')\n\n    # read CSV file\n    self._df = pd.read_csv(\n        self._file_path,\n        sep=',',\n        quotechar='\"',\n        encoding='utf-8',\n    )\n\n    # check column IDs match base columns and fields\n    if not all(c in self._columns for c in self._df.columns):\n        raise Exception(f\"Column IDs used in CSV file do not match columns definition: {self._df.columns.tolist()}\")\n\n    # adjust row index to start at 1 instead of 0\n    self._df.index += 1\n\n    # insert missing columns and reorder via reindexing, then update dtypes\n    df_new = self._df.reindex(columns=list(self._columns.keys()))\n    for col_id, col in self._columns.items():\n        if col_id in self._df:\n            continue\n        df_new[col_id] = df_new[col_id].astype(col.dtype)\n        df_new[col_id] = col.default\n    self._df = df_new\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEDF.write","title":"<code>write()</code>","text":"<p>Write TEDF to CSV file</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there is no file path that specifies where to write</p> Source code in <code>python/posted/tedf.py</code> <pre><code>def write(self):\n    \"\"\"\n    Write TEDF to CSV file\n\n    Raises\n    ------\n    Exception\n        If there is no file path that specifies where to write\n    \"\"\"\n    if self._file_path is None:\n        raise Exception('Cannot write to file, as this TEDataFile object has been created from a dataframe. Please '\n                        'first set a file path on this object.')\n\n    self._df.to_csv(\n        self._file_path,\n        index=False,\n        sep=',',\n        quotechar='\"',\n        encoding='utf-8',\n        na_rep='',\n    )\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.TEDFInconsistencyException","title":"<code>TEDFInconsistencyException</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised for inconsistencies in TEDFs.</p> <p>Attributes:     message -- message explaining the inconsistency     row_id -- row where the inconsistency occurs     col_id -- column where the inconsistency occurs     file_path -- path to the file where the inconsistency occurs</p> Source code in <code>python/posted/tedf.py</code> <pre><code>class TEDFInconsistencyException(Exception):\n    \"\"\"Exception raised for inconsistencies in TEDFs.\n\n    Attributes:\n        message -- message explaining the inconsistency\n        row_id -- row where the inconsistency occurs\n        col_id -- column where the inconsistency occurs\n        file_path -- path to the file where the inconsistency occurs\n    \"\"\"\n    def __init__(self, message: str = \"Inconsistency detected\", row_id: None | int = None,\n                 col_id: None | str = None, file_path: None | Path = None):\n        self.message: str = message\n        self.row_id: None | int = row_id\n        self.col_id: None | str = col_id\n        self.file_path: None | Path = file_path\n\n        # add tokens at the end of the error message\n        message_tokens = []\n        if file_path is not None:\n            message_tokens.append(f\"file \\\"{file_path}\\\"\")\n        if row_id is not None:\n            message_tokens.append(f\"line {row_id}\")\n        if col_id is not None:\n            message_tokens.append(f\"in column \\\"{col_id}\\\"\")\n\n        # compose error message from tokens\n        exception_message: str = message\n        if message_tokens:\n            exception_message += f\"\\n    \" + (\", \".join(message_tokens)).capitalize()\n\n        super().__init__(exception_message)\n</code></pre>"},{"location":"code/python/tedf/#python.posted.tedf.new_inconsistency","title":"<code>new_inconsistency(raise_exception, **kwargs)</code>","text":"<p>Create new inconsistency object based on kwqargs</p> Source code in <code>python/posted/tedf.py</code> <pre><code>def new_inconsistency(raise_exception: bool, **kwargs) -&gt; TEDFInconsistencyException:\n    \"\"\"\n    Create new inconsistency object based on kwqargs\n\n    Parameters\n    ----------\n\n    \"\"\"\n    exception = TEDFInconsistencyException(**kwargs)\n    if raise_exception:\n        raise exception\n    else:\n        return exception\n</code></pre>"},{"location":"code/python/units/","title":"units","text":""},{"location":"code/python/units/#python.posted.units.ctx_kwargs_for_variants","title":"<code>ctx_kwargs_for_variants(variants, flow_id)</code>","text":"<p>Generates a dictionary of context key-word arguments for unit conversion for context from flow specs</p> <p>Parameters:</p> Name Type Description Default <code>variants</code> <code>list[str | None]</code> <p>A list of variant names or None values.</p> required <code>flow_id</code> <code>str</code> <p>Identifier for the specific flow or process.</p> required <p>Returns:</p> Type Description <code>    dict</code> <p>Dictionary containing default conversion parameters for energy content and density,</p> Source code in <code>python/posted/units.py</code> <pre><code>def ctx_kwargs_for_variants(variants: list[str | None], flow_id: str):\n    '''\n    Generates a dictionary of context key-word arguments for unit conversion for context from flow specs\n\n\n    Parameters\n    ----------\n    variants : list[str | None]\n        A list of variant names or None values.\n    flow_id : str\n        Identifier for the specific flow or process.\n\n\n    Returns\n    -------\n        dict\n            Dictionary containing default conversion parameters for energy content and density,\n\n    '''\n    # set default conversion parameters to NaN, such that conversion fails with a meaningful error message in their\n    # absence. when this is left out, the conversion fails will throw a division-by-zero error message.\n    ctx_kwargs = {'energycontent': np.nan, 'density': np.nan}\n    ctx_kwargs |= {\n        unit_variants[v]['param']: flows[flow_id][unit_variants[v]['value']]\n        for v in variants if v is not None\n    }\n    return ctx_kwargs\n</code></pre>"},{"location":"code/python/units/#python.posted.units.split_off_variant","title":"<code>split_off_variant(unit)</code>","text":"<p>Takes a unit string and splits it into a pure unit and a variant, if present, based on a semicolon separator, e.g. MWh;LHV into MWh and LHV.</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <code>str</code> <p>String that may contain a unit and its variant separated by a semicolon.</p> required <p>Returns:</p> Type Description <code>    tuple</code> <p>Returns a tuple containing the pure unit and the variant (if present) after splitting the input unit string by semi-colons.</p> Source code in <code>python/posted/units.py</code> <pre><code>def split_off_variant(unit: str):\n    '''\n    Takes a unit string and splits it into a pure unit and a variant,\n    if present, based on a semicolon separator, e.g. MWh;LHV into MWh and LHV.\n\n    Parameters\n    ----------\n    unit : str\n        String that may contain a unit and its variant separated by a semicolon.\n\n    Returns\n    -------\n        tuple\n            Returns a tuple containing the pure unit and the variant (if\n            present) after splitting the input unit string by semi-colons.\n\n    '''\n    tokens = unit.split(';')\n    if len(tokens) == 1:\n        pure_unit = unit\n        variant = None\n    elif len(tokens) &gt; 2:\n        raise Exception(f\"Too many semi-colons in unit '{unit}'.\")\n    else:\n        pure_unit, variant = tokens\n    if variant is not None and variant not in unit_variants:\n        raise Exception(f\"Cannot find unit variant '{variant}'.\")\n    return pure_unit, variant\n</code></pre>"},{"location":"code/python/units/#python.posted.units.unit_allowed","title":"<code>unit_allowed(unit, flow_id, dimension)</code>","text":"<p>Checks if a given unit is allowed for a specific dimension and flow ID, handling unit variants and compatibility checks.</p> <p>Parameters:</p> Name Type Description Default <code>unit</code> <code>str</code> <p>The Unit to Check</p> required <code>flow_id</code> <code>None | str</code> <p>Identifier for the specific flow or process.</p> required <code>dimension</code> <code>str</code> <p>Expected dimension of the unit.</p> required <p>Returns:</p> Type Description <code>    tuple(bool, str)</code> <p>Tuple with a boolean value and a message. The boolean value indicates whether the unit is allowed based on the provided conditions, and the message provides additional information or an error message related to the unit validation process.</p> Source code in <code>python/posted/units.py</code> <pre><code>def unit_allowed(unit: str, flow_id: None | str, dimension: str):\n    '''Checks if a given unit is allowed for a specific dimension and flow ID,\n    handling unit variants and compatibility checks.\n\n    Parameters\n    ----------\n    unit : str\n        The Unit to Check\n    flow_id : None | str\n        Identifier for the specific flow or process.\n    dimension : str\n        Expected dimension of the unit.\n\n    Returns\n    -------\n        tuple(bool, str)\n            Tuple with a boolean value and a message. The boolean value indicates\n            whether the unit is allowed based on the provided conditions, and the message\n            provides additional information or an error message related to the unit validation process.\n    '''\n    if not isinstance(unit, str):\n        raise Exception('Unit to check must be string.')\n\n    # split unit into pure unit and variant\n    try:\n        unit, variant = split_off_variant(unit)\n    except:\n        return False, f\"Inconsistent unit variant format in '{unit}'.\"\n\n    try:\n        unit_registered = ureg(unit)\n    except:\n        return False, f\"Unknown unit '{unit}'.\"\n\n    if flow_id is None:\n        if '[flow]' in dimension:\n            return False, f\"No flow_id provided even though [flow] is in dimension.\"\n        if variant is not None:\n            return False, f\"Unexpected unit variant '{variant}' for dimension [{dimension}].\"\n        if (dimension == 'dimensionless' and unit_registered.dimensionless) or unit_registered.check(dimension):\n            return True, ''\n        else:\n            return False, f\"Unit '{unit}' does not match expected dimension [{dimension}].\"\n    else:\n        if '[flow]' not in dimension:\n            if (dimension == 'dimensionless' and unit_registered.dimensionless) or unit_registered.check(dimension):\n                return True, ''\n        else:\n            check_dimensions = [\n                (dimension.replace(\n                    '[flow]', f\"[{dimension_base}]\"), dimension_base, base_unit)\n                for dimension_base, base_unit in [('mass', 'kg'), ('energy', 'kWh'), ('volume', 'm**3')]\n            ]\n            for check_dimension, check_dimension_base, check_base_unit in check_dimensions:\n                if unit_registered.check(check_dimension):\n                    if variant is None:\n                        if any(\n                            (check_dimension_base == variant_specs['dimension']) and\n                            flows[flow_id][variant_specs['value']] is not np.nan\n                            for variant, variant_specs in unit_variants.items()\n                        ):\n                            return False, (f\"Missing unit variant for dimension [{check_dimension_base}] for unit \"\n                                           f\"'{unit}'.\")\n                    elif unit_variants[variant]['dimension'] != check_dimension_base:\n                        return False, f\"Variant '{variant}' incompatible with unit '{unit}'.\"\n\n                    default_unit, default_variant = split_off_variant(\n                        flows[flow_id]['default_unit'])\n                    ctx_kwargs = ctx_kwargs_for_variants(\n                        [variant, default_variant], flow_id)\n\n                    if ureg(check_base_unit).is_compatible_with(default_unit, 'flocon', **ctx_kwargs):\n                        return True, ''\n                    else:\n                        return False, f\"Unit '{unit}' not compatible with flow '{flow_id}'.\"\n\n        return False, f\"Unit '{unit}' is not compatible with dimension [{dimension}].\"\n</code></pre>"},{"location":"code/python/units/#python.posted.units.unit_convert","title":"<code>unit_convert(unit_from, unit_to, flow_id=None)</code>","text":"<p>Converts units with optional flow context handling based on specified variants and flow ID. The function checks if the input units are not NaN, then it proceeds to handle different cases based on the presence of a flow context and unit variants.</p> <p>Parameters:</p> Name Type Description Default <code>unit_from</code> <code>str | float</code> <p>Unit to convert from.</p> required <code>unit_to</code> <code>str | float</code> <p>Unit to convert to.</p> required <code>flow_id</code> <code>None | str</code> <p>Identifier for the specific flow or process.</p> <code>None</code> <p>Returns:</p> Type Description <code>    float</code> <p>Conversion factor between unit_from and unit_to</p> Source code in <code>python/posted/units.py</code> <pre><code>def unit_convert(unit_from: str | float, unit_to: str | float, flow_id: None | str = None) -&gt; float:\n    '''\n    Converts units with optional flow context handling based on\n    specified variants and flow ID. The function checks if the input units are not NaN,\n    then it proceeds to handle different cases based on the presence of a flow context and unit\n    variants.\n\n    Parameters\n    ----------\n    unit_from : str | float\n        Unit to convert from.\n    unit_to : str | float\n        Unit to convert to.\n    flow_id : None | str\n        Identifier for the specific flow or process.\n\n    Returns\n    -------\n        float\n            Conversion factor between unit_from and unit_to\n\n    '''\n    # return nan if unit_from or unit_to is nan\n    if unit_from is np.nan or unit_to is np.nan:\n        return np.nan\n\n    # replace \"No Unit\" by \"Dimensionless\"\n    if unit_from == 'No Unit':\n        unit_from = 'dimensionless'\n    if unit_to == 'No Unit':\n        unit_to = 'dimensionless'\n\n    # skip flow conversion if no flow_id specified\n    if flow_id is None or pd.isna(flow_id):\n        return ureg(unit_from).to(unit_to).magnitude\n\n    # get variants from units\n    pure_units = []\n    variants = []\n    for u in (unit_from, unit_to):\n        pure_unit, variant = split_off_variant(u)\n        pure_units.append(pure_unit)\n        variants.append(variant)\n\n    unit_from, unit_to = pure_units\n\n    # if no variants a specified, we may proceed without a flow context\n    if not any(variants):\n        return ureg(unit_from).to(unit_to).magnitude\n\n    # if both variants refer to the same dimension, we need to manually calculate the conversion factor and proceed\n    # without a flow context\n    if len(variants) == 2:\n        variant_params = {\n            unit_variants[v]['param'] if v is not None else None\n            for v in variants\n        }\n        if len(variant_params) == 1:\n            param = next(iter(variant_params))\n            value_from, value_to = (\n                flows[flow_id][unit_variants[v]['value']] for v in variants)\n\n            conv_factor = (ureg(value_from) / ureg(value_to)\n                           if param == 'energycontent' else\n                           ureg(value_to) / ureg(value_from))\n\n            return conv_factor.magnitude * ureg(unit_from).to(unit_to).magnitude\n\n    # perform the actual conversion step with all required variants\n    ctx_kwargs = ctx_kwargs_for_variants(variants, flow_id)\n    return ureg(unit_from).to(unit_to, 'flocon', **ctx_kwargs).magnitude\n</code></pre>"},{"location":"tutorials/R/overview/","title":"Overview","text":"In\u00a0[1]: Copied! <pre>devtools::load_all()\n</pre> devtools::load_all() <pre>\u2139 Loading posted\n</pre> <pre>\nAttaching package: \u2018dplyr\u2019\n\n\n</pre> <pre>The following objects are masked from \u2018package:stats\u2019:\n\n    filter, lag\n\n\n</pre> <pre>The following objects are masked from \u2018package:base\u2019:\n\n    intersect, setdiff, setequal, union\n\n\n</pre> <pre>\nAttaching package: \u2018docstring\u2019\n\n\n</pre> <pre>The following object is masked from \u2018package:utils\u2019:\n\n    ?\n\n\n</pre> In\u00a0[2]: Copied! <pre>par(bg = \"white\")\nplot(1:10)\n</pre> par(bg = \"white\") plot(1:10) In\u00a0[3]: Copied! <pre>tedf &lt;- TEDF$new(\"Tech|Electrolysis\")$load()\ntedf$data\n</pre> tedf &lt;- TEDF$new(\"Tech|Electrolysis\")$load() tedf$data A data.frame: 95 \u00d7 14 subtechsizeregionperiodvariablereference_variablevalueuncertaintyunitreference_valuereference_unitcommentsourcesource_detail &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt; AEL 100 MW      *2020CAPEXInput Capacity|Electricity 400  0EUR_20201kW   Vartiainen22Page 4, Figure 4             AEL 100 MW      *2030CAPEXInput Capacity|Electricity 240 50EUR_20201kW   Vartiainen22Page 4, Figure 4             AEL 100 MW      *2040CAPEXInput Capacity|Electricity 140 75EUR_20201kW   Vartiainen22Page 4, Figure 4             AEL 100 MW      *2050CAPEXInput Capacity|Electricity  80 75EUR_20201kW   Vartiainen22Page 4, Figure 4             AEL 100 MW      *2020CAPEXInput Capacity|Electricity 663 NAEUR_20201kW   Holst21     Appendix A                   AEL 100 MW      *2030CAPEXInput Capacity|Electricity 444 NAEUR_20201kW   Holst21     Appendix A                   PEM 100 MW      *2020CAPEXInput Capacity|Electricity 718 NAEUR_20201kW   Holst21     Appendix A                   PEM 100 MW      *2030CAPEXInput Capacity|Electricity 502 NAEUR_20201kW   Holst21     Appendix A                   AEL 5 MW        *2020CAPEXInput Capacity|Electricity 949 NAEUR_20201kW   Holst21     Appendix A                   AEL 5 MW        *2030CAPEXInput Capacity|Electricity 726 NAEUR_20201kW   Holst21     Appendix A                   PEM 5 MW        *2020CAPEXInput Capacity|Electricity 978 NAEUR_20201kW   Holst21     Appendix A                   PEM 5 MW        *2030CAPEXInput Capacity|Electricity 718 NAEUR_20201kW   Holst21     Appendix A                   AEL *           *2030CAPEXInput Capacity|Electricity 536152USD_20211kW   IRENA22     Page 23                      AEL *           *2050CAPEXInput Capacity|Electricity 230 96USD_20211kW   IRENA22     Page 23                      AEL 130000 Nm\u00b3/h*2020CAPEXInput Capacity|Electricity1150350EUR_20191kW   Tenhumberg20Table 2, Page 1588 (3/10)    PEM 130000 Nm\u00b3/h*2020CAPEXInput Capacity|Electricity1750350EUR_20191kW   Tenhumberg20Table 2, Page 1588 (3/10)    SOEC130000 Nm\u00b3/h*2020CAPEXInput Capacity|Electricity2000 NAEUR_20191kW  Provided as a lower thresholdTenhumberg20Table 2, Page 1588 (3/10)    AEL 130000 Nm\u00b3/h*2030CAPEXInput Capacity|Electricity 450 NAEUR_20191kW   Tenhumberg20Supplement, Table S2, Page 3 PEM 130000 Nm\u00b3/h*2030CAPEXInput Capacity|Electricity 810 NAEUR_20191kW   Tenhumberg20Supplement, Table S2, Page 3 SOEC130000 Nm\u00b3/h*2030CAPEXInput Capacity|Electricity 800 NAEUR_20191kW   Tenhumberg20Supplement, Table S2, Page 3 AEL 1 MW        *2020CAPEXOutput Capacity|Hydrogen  1566918EUR_20201kg/d DEARF23     Sheet 86 AEC 1MW, Row 24     AEL 1 MW        *2030CAPEXOutput Capacity|Hydrogen  1164 NAEUR_20201kg/d DEARF23     Sheet 86 AEC 1MW, Row 24     AEL 1 MW        *2040CAPEXOutput Capacity|Hydrogen   874 NAEUR_20201kg/d DEARF23     Sheet 86 AEC 1MW, Row 24     AEL 1 MW        *2050CAPEXOutput Capacity|Hydrogen   648438EUR_20201kg/d DEARF23     Sheet 86 AEC 1MW, Row 24     AEL 100 MW      *2020CAPEXOutput Capacity|Hydrogen  1358374EUR_20201kg/d DEARF23     Sheet 86 AEC 100MW, Row 24   AEL 100 MW      *2030CAPEXOutput Capacity|Hydrogen   919 NAEUR_20201kg/d DEARF23     Sheet 86 AEC 100MW, Row 24   AEL 100 MW      *2040CAPEXOutput Capacity|Hydrogen   583 NAEUR_20201kg/d DEARF23     Sheet 86 AEC 100MW, Row 24   AEL 100 MW      *2050CAPEXOutput Capacity|Hydrogen   463201EUR_20201kg/d DEARF23     Sheet 86 AEC 100MW, Row 24   PEM 1 MW        *2020CAPEXOutput Capacity|Hydrogen  2215497EUR_20201kg/d DEARF23     Sheet 86 PEMEC 1MW, Row 24   PEM 1 MW        *2030CAPEXOutput Capacity|Hydrogen  1378 NAEUR_20201kg/d DEARF23     Sheet 86 PEMEC 1MW, Row 24   \u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee SOEC*           *2030Output|Hydrogen                 Input|Electricity    24.20  NAkg        1.0MWh     DEARF23     Sheet 86 SOEC 1MW, Row 16        SOEC*           *2040Output|Hydrogen                 Input|Electricity    24.60  NAkg        1.0MWh     DEARF23     Sheet 86 SOEC 1MW, Row 16        SOEC*           *2050Output|Hydrogen                 Input|Electricity    25.100.50kg        1.0MWh     DEARF23     Sheet 86 SOEC 1MW, Row 16        SOEC*           *2020Input|Heat                      Input|Electricity    20.502.00pct MWh  79.5pct MWh DEARF23     Sheet 86 SOEC 1MW, Rows 9 and 10 SOEC*           *2030Input|Heat                      Input|Electricity    19.50  NApct MWh  80.5pct MWh DEARF23     Sheet 86 SOEC 1MW, Rows 9 and 10 SOEC*           *2040Input|Heat                      Input|Electricity    18.60  NApct MWh  81.4pct MWh DEARF23     Sheet 86 SOEC 1MW, Rows 9 and 10 SOEC*           *2050Input|Heat                      Input|Electricity    18.601.00pct MWh  81.4pct MWh DEARF23     Sheet 86 SOEC 1MW, Rows 9 and 10 AEL *           *2020Input|Electricity               Output|Hydrogen      48.30  NAkWh       1.0kg      Holst21     Table 3-2, Page 42               AEL *           *2030Input|Electricity               Output|Hydrogen      45.60  NAkWh       1.0kg      Holst21     Table 3-2, Page 42               PEM *           *2020Input|Electricity               Output|Hydrogen      51.00  NAkWh       1.0kg      Holst21     Table 3-7, Page 50               PEM *           *2030Input|Electricity               Output|Hydrogen      45.70  NAkWh       1.0kg      Holst21     Table 3-7, Page 50               AEL *           *2030Input|Electricity               Output|Hydrogen      50.351.85kWh       1.0kg      IRENA22     Footnote 12, Page 21             AEL *           *2050Input|Electricity               Output|Hydrogen      46.501.50kWh       1.0kg      IRENA22     Footnote 12, Page 21             PEM *           *2021Input|Electricity               Output|Hydrogen      54.20  NAkWh       1.0kg     Based on life cycle inventory, from Ecoinvent v3.4Al-Qahtani21Table 2, Page 3                  AEL 130000 Nm\u00b3/h*2020Input|Electricity               Output|Hydrogen       5.450.45kWh       1.0m\u00b3;normTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        PEM 130000 Nm\u00b3/h*2020Input|Electricity               Output|Hydrogen       5.750.75kWh       1.0m\u00b3;normTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        SOEC130000 Nm\u00b3/h*2020Input|Electricity               Output|Hydrogen       3.800.10kWh       1.0m\u00b3;normTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        SOEC130000 Nm\u00b3/h*2020Input|Heat                      Output|Hydrogen       0.70  NAkWh       1.0m\u00b3;normTotal system demand                               Tenhumberg20Table 3, Page 1590 (5/10)        AEL 130000 Nm\u00b3/h*2030Input|Electricity               Output|Hydrogen       4.42  NAkWh       1.0m\u00b3;normTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        PEM 130000 Nm\u00b3/h*2030Input|Electricity               Output|Hydrogen       4.81  NAkWh       1.0m\u00b3;normTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        SOEC130000 Nm\u00b3/h*2030Input|Electricity               Output|Hydrogen       3.55  NAkWh       1.0m\u00b3;normTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        SOEC130000 Nm\u00b3/h*2030Input|Heat                      Output|Hydrogen       0.70  NAkWh       1.0m\u00b3;normTotal system demand                               Tenhumberg20Table 3, Page 1590 (5/10)        AEL *           *2020Input|Electricity               Output|Hydrogen      54.004.00kWh       1.0kg      Yates20     Table 3, Page 10                 PEM *           *2020Input|Water                     Output|Hydrogen      10.00  NAkg        1.0kg     Based on life cycle inventory, from Ecoinvent v3.4Al-Qahtani21Table 2, Page 3                  *   *           *2021Input|Water                     Output|Hydrogen       9.00  NAkg        1.0kg      IEA-GHR21    AEL *           *2020Input|Water                     Output|Hydrogen      10.001.00L;norm    1.0kg      Yates20     Table 3, Page 10                 *   1 MW        **   Total Input Capacity|Electricity      1.00  NAMW         NA *            *   5 MW        **   Total Input Capacity|Electricity      5.00  NAMW         NA *            *   100 MW      **   Total Input Capacity|Electricity    100.00  NAMW         NA *            *   130000 Nm\u00b3/h**   Total Output Capacity|Hydrogen   130000.00  NAm\u00b3/h;norm  NA *            In\u00a0[4]: Copied! <pre>DataSet$new('Tech|Electrolysis')$normalise(override=list('Tech|Electrolysis|Input Capacity|elec'= 'kW', 'Tech|Electrolysis|Output Capacity|h2'= 'kW;LHV'))  %&gt;% filter(source=='Vartiainen22')\n</pre>  DataSet$new('Tech|Electrolysis')$normalise(override=list('Tech|Electrolysis|Input Capacity|elec'= 'kW', 'Tech|Electrolysis|Output Capacity|h2'= 'kW;LHV'))  %&gt;% filter(source=='Vartiainen22') A data.frame: 12 \u00d7 15 parent_variablesubtechsizeregionperiodvariablereference_variablevalueuncertaintyunitreference_valuereference_unitcommentsourcesource_detail &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt; Tech|ElectrolysisAEL100 MW*2020CAPEX          Input Capacity|Electricity45.403859630.000000USD_2005  1MWh/a Vartiainen22Page 4, Figure 4 Tech|ElectrolysisAEL100 MW*2030CAPEX          Input Capacity|Electricity27.242315785.675482USD_2005  1MWh/a Vartiainen22Page 4, Figure 4 Tech|ElectrolysisAEL100 MW*2040CAPEX          Input Capacity|Electricity15.891350878.513224USD_2005  1MWh/a Vartiainen22Page 4, Figure 4 Tech|ElectrolysisAEL100 MW*2050CAPEX          Input Capacity|Electricity 9.080771938.513224USD_2005  1MWh/a Vartiainen22Page 4, Figure 4 Tech|ElectrolysisAEL100 MW*2020OPEX Fixed     Input Capacity|Electricity 0.68105789      NAUSD_2005/a1MWh/a1.5% of CAPEX; reported in units of electric capacity                     Vartiainen22Page 4           Tech|ElectrolysisAEL100 MW*2030OPEX Fixed     Input Capacity|Electricity 0.23837026      NAUSD_2005/a1MWh/a10% LR decrease                                                           Vartiainen22Page 4           Tech|ElectrolysisAEL100 MW*2040OPEX Fixed     Input Capacity|Electricity 0.08286204      NAUSD_2005/a1MWh/a10% LR decrease                                                           Vartiainen22Page 4           Tech|ElectrolysisAEL100 MW*2050OPEX Fixed     Input Capacity|Electricity 0.02837741      NAUSD_2005/a1MWh/a10% LR decrease                                                           Vartiainen22Page 4           Tech|ElectrolysisAEL100 MW*2020Output|HydrogenInput|Electricity          0.67000000      NAMWh;LHV   1MWh  1.5% of CAPEX; reported in units of electric capacity; 67% assumes the LHVVartiainen22Page 4           Tech|ElectrolysisAEL100 MW*2030Output|HydrogenInput|Electricity          0.70000000      NAMWh;LHV   1MWh  10% LR decrease                                                           Vartiainen22Page 4           Tech|ElectrolysisAEL100 MW*2040Output|HydrogenInput|Electricity          0.73000000      NAMWh;LHV   1MWh  10% LR decrease                                                           Vartiainen22Page 4           Tech|ElectrolysisAEL100 MW*2050Output|HydrogenInput|Electricity          0.76000000      NAMWh;LHV   1MWh  10% LR decrease                                                           Vartiainen22Page 4           In\u00a0[5]: Copied! <pre>DataSet$new('Tech|Electrolysis')$normalise(override=list('Tech|Electrolysis|Output Capacity|h2'= 'kW;LHV'))\n</pre> DataSet$new('Tech|Electrolysis')$normalise(override=list('Tech|Electrolysis|Output Capacity|h2'= 'kW;LHV')) A data.frame: 95 \u00d7 15 parent_variablesubtechsizeregionperiodvariablereference_variablevalueuncertaintyunitreference_valuereference_unitcommentsourcesource_detail &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt; Tech|ElectrolysisAEL 100 MW      *2020CAPEXInput Capacity|Electricity 45.403860 0.000000USD_20051MWh/a     Vartiainen22Page 4, Figure 4             Tech|ElectrolysisAEL 100 MW      *2030CAPEXInput Capacity|Electricity 27.242316 5.675482USD_20051MWh/a     Vartiainen22Page 4, Figure 4             Tech|ElectrolysisAEL 100 MW      *2040CAPEXInput Capacity|Electricity 15.891351 8.513224USD_20051MWh/a     Vartiainen22Page 4, Figure 4             Tech|ElectrolysisAEL 100 MW      *2050CAPEXInput Capacity|Electricity  9.080772 8.513224USD_20051MWh/a     Vartiainen22Page 4, Figure 4             Tech|ElectrolysisAEL 100 MW      *2020CAPEXInput Capacity|Electricity 75.256897       NAUSD_20051MWh/a     Holst21     Appendix A                   Tech|ElectrolysisAEL 100 MW      *2030CAPEXInput Capacity|Electricity 50.398284       NAUSD_20051MWh/a     Holst21     Appendix A                   Tech|ElectrolysisPEM 100 MW      *2020CAPEXInput Capacity|Electricity 81.499928       NAUSD_20051MWh/a     Holst21     Appendix A                   Tech|ElectrolysisPEM 100 MW      *2030CAPEXInput Capacity|Electricity 56.981844       NAUSD_20051MWh/a     Holst21     Appendix A                   Tech|ElectrolysisAEL 5 MW        *2020CAPEXInput Capacity|Electricity107.720657       NAUSD_20051MWh/a     Holst21     Appendix A                   Tech|ElectrolysisAEL 5 MW        *2030CAPEXInput Capacity|Electricity 82.408005       NAUSD_20051MWh/a     Holst21     Appendix A                   Tech|ElectrolysisPEM 5 MW        *2020CAPEXInput Capacity|Electricity111.012437       NAUSD_20051MWh/a     Holst21     Appendix A                   Tech|ElectrolysisPEM 5 MW        *2030CAPEXInput Capacity|Electricity 81.499928       NAUSD_20051MWh/a     Holst21     Appendix A                   Tech|ElectrolysisAEL *           *2030CAPEXInput Capacity|Electricity 45.03196212.770258USD_20051MWh/a     IRENA22     Page 23                      Tech|ElectrolysisAEL *           *2050CAPEXInput Capacity|Electricity 19.323417 8.065426USD_20051MWh/a     IRENA22     Page 23                      Tech|ElectrolysisAEL 130000 Nm\u00b3/h*2020CAPEXInput Capacity|Electricity132.89985740.447783USD_20051MWh/a     Tenhumberg20Table 2, Page 1588 (3/10)    Tech|ElectrolysisPEM 130000 Nm\u00b3/h*2020CAPEXInput Capacity|Electricity202.23891340.447783USD_20051MWh/a     Tenhumberg20Table 2, Page 1588 (3/10)    Tech|ElectrolysisSOEC130000 Nm\u00b3/h*2020CAPEXInput Capacity|Electricity231.130187       NAUSD_20051MWh/a    Provided as a lower thresholdTenhumberg20Table 2, Page 1588 (3/10)    Tech|ElectrolysisAEL 130000 Nm\u00b3/h*2030CAPEXInput Capacity|Electricity 52.004292       NAUSD_20051MWh/a     Tenhumberg20Supplement, Table S2, Page 3 Tech|ElectrolysisPEM 130000 Nm\u00b3/h*2030CAPEXInput Capacity|Electricity 93.607726       NAUSD_20051MWh/a     Tenhumberg20Supplement, Table S2, Page 3 Tech|ElectrolysisSOEC130000 Nm\u00b3/h*2030CAPEXInput Capacity|Electricity 92.452075       NAUSD_20051MWh/a     Tenhumberg20Supplement, Table S2, Page 3 Tech|ElectrolysisAEL 1 MW        *2020CAPEXOutput Capacity|Hydrogen  127.99852175.033616USD_20051MWh/a;LHV DEARF23     Sheet 86 AEC 1MW, Row 24     Tech|ElectrolysisAEL 1 MW        *2030CAPEXOutput Capacity|Hydrogen   95.140663       NAUSD_20051MWh/a;LHV DEARF23     Sheet 86 AEC 1MW, Row 24     Tech|ElectrolysisAEL 1 MW        *2040CAPEXOutput Capacity|Hydrogen   71.437233       NAUSD_20051MWh/a;LHV DEARF23     Sheet 86 AEC 1MW, Row 24     Tech|ElectrolysisAEL 1 MW        *2050CAPEXOutput Capacity|Hydrogen   52.96490535.800353USD_20051MWh/a;LHV DEARF23     Sheet 86 AEC 1MW, Row 24     Tech|ElectrolysisAEL 100 MW      *2020CAPEXOutput Capacity|Hydrogen  110.99744030.569251USD_20051MWh/a;LHV DEARF23     Sheet 86 AEC 100MW, Row 24   Tech|ElectrolysisAEL 100 MW      *2030CAPEXOutput Capacity|Hydrogen   75.115352       NAUSD_20051MWh/a;LHV DEARF23     Sheet 86 AEC 100MW, Row 24   Tech|ElectrolysisAEL 100 MW      *2040CAPEXOutput Capacity|Hydrogen   47.652068       NAUSD_20051MWh/a;LHV DEARF23     Sheet 86 AEC 100MW, Row 24   Tech|ElectrolysisAEL 100 MW      *2050CAPEXOutput Capacity|Hydrogen   37.84375216.428929USD_20051MWh/a;LHV DEARF23     Sheet 86 AEC 100MW, Row 24   Tech|ElectrolysisPEM 1 MW        *2020CAPEXOutput Capacity|Hydrogen  181.04516340.622775USD_20051MWh/a;LHV DEARF23     Sheet 86 PEMEC 1MW, Row 24   Tech|ElectrolysisPEM 1 MW        *2030CAPEXOutput Capacity|Hydrogen  112.632160       NAUSD_20051MWh/a;LHV DEARF23     Sheet 86 PEMEC 1MW, Row 24   \u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee Tech|ElectrolysisSOEC*           *2030Output|Hydrogen                 Input|Electricity8.065777e-01        NAMWh;LHV   1MWh     DEARF23     Sheet 86 SOEC 1MW, Row 16        Tech|ElectrolysisSOEC*           *2040Output|Hydrogen                 Input|Electricity8.199095e-01        NAMWh;LHV   1MWh     DEARF23     Sheet 86 SOEC 1MW, Row 16        Tech|ElectrolysisSOEC*           *2050Output|Hydrogen                 Input|Electricity8.365744e-010.01666483MWh;LHV   1MWh     DEARF23     Sheet 86 SOEC 1MW, Row 16        Tech|ElectrolysisSOEC*           *2020Input|Heat                      Input|Electricity2.578616e-010.02515723MWh       1MWh     DEARF23     Sheet 86 SOEC 1MW, Rows 9 and 10 Tech|ElectrolysisSOEC*           *2030Input|Heat                      Input|Electricity2.422360e-01        NAMWh       1MWh     DEARF23     Sheet 86 SOEC 1MW, Rows 9 and 10 Tech|ElectrolysisSOEC*           *2040Input|Heat                      Input|Electricity2.285012e-01        NAMWh       1MWh     DEARF23     Sheet 86 SOEC 1MW, Rows 9 and 10 Tech|ElectrolysisSOEC*           *2050Input|Heat                      Input|Electricity2.285012e-010.01228501MWh       1MWh     DEARF23     Sheet 86 SOEC 1MW, Rows 9 and 10 Tech|ElectrolysisAEL *           *2020Input|Electricity               Output|Hydrogen  1.449160e+00        NAMWh       1MWh;LHV Holst21     Table 3-2, Page 42               Tech|ElectrolysisAEL *           *2030Input|Electricity               Output|Hydrogen  1.368151e+00        NAMWh       1MWh;LHV Holst21     Table 3-2, Page 42               Tech|ElectrolysisPEM *           *2020Input|Electricity               Output|Hydrogen  1.530169e+00        NAMWh       1MWh;LHV Holst21     Table 3-7, Page 50               Tech|ElectrolysisPEM *           *2030Input|Electricity               Output|Hydrogen  1.371151e+00        NAMWh       1MWh;LHV Holst21     Table 3-7, Page 50               Tech|ElectrolysisAEL *           *2030Input|Electricity               Output|Hydrogen  1.510667e+000.05550612MWh       1MWh;LHV IRENA22     Footnote 12, Page 21             Tech|ElectrolysisAEL *           *2050Input|Electricity               Output|Hydrogen  1.395154e+000.04500496MWh       1MWh;LHV IRENA22     Footnote 12, Page 21             Tech|ElectrolysisPEM *           *2021Input|Electricity               Output|Hydrogen  1.626179e+00        NAMWh       1MWh;LHVBased on life cycle inventory, from Ecoinvent v3.4Al-Qahtani21Table 2, Page 3                  Tech|ElectrolysisAEL 130000 Nm\u00b3/h*2020Input|Electricity               Output|Hydrogen  1.952408e+000.16120799MWh       1MWh;LHVTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        Tech|ElectrolysisPEM 130000 Nm\u00b3/h*2020Input|Electricity               Output|Hydrogen  2.059880e+000.26867998MWh       1MWh;LHVTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        Tech|ElectrolysisSOEC130000 Nm\u00b3/h*2020Input|Electricity               Output|Hydrogen  1.361312e+000.03582400MWh       1MWh;LHVTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        Tech|ElectrolysisSOEC130000 Nm\u00b3/h*2020Input|Heat                      Output|Hydrogen  2.507680e-01        NAMWh       1MWh;LHVTotal system demand                               Tenhumberg20Table 3, Page 1590 (5/10)        Tech|ElectrolysisAEL 130000 Nm\u00b3/h*2030Input|Electricity               Output|Hydrogen  1.583421e+00        NAMWh       1MWh;LHVTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        Tech|ElectrolysisPEM 130000 Nm\u00b3/h*2030Input|Electricity               Output|Hydrogen  1.723134e+00        NAMWh       1MWh;LHVTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        Tech|ElectrolysisSOEC130000 Nm\u00b3/h*2030Input|Electricity               Output|Hydrogen  1.271752e+00        NAMWh       1MWh;LHVTotal system demand                               Tenhumberg20Table 2, Page 1588 (3/10)        Tech|ElectrolysisSOEC130000 Nm\u00b3/h*2030Input|Heat                      Output|Hydrogen  2.507680e-01        NAMWh       1MWh;LHVTotal system demand                               Tenhumberg20Table 3, Page 1590 (5/10)        Tech|ElectrolysisAEL *           *2020Input|Electricity               Output|Hydrogen  1.620179e+000.12001324MWh       1MWh;LHV Yates20     Table 3, Page 10                 Tech|ElectrolysisPEM *           *2020Input|Water                     Output|Hydrogen  3.000331e-01        NAt         1MWh;LHVBased on life cycle inventory, from Ecoinvent v3.4Al-Qahtani21Table 2, Page 3                  Tech|Electrolysis*   *           *2021Input|Water                     Output|Hydrogen  2.700298e-01        NAt         1MWh;LHV IEA-GHR21    Tech|ElectrolysisAEL *           *2020Input|Water                     Output|Hydrogen  2.994960e-010.02994960t         1MWh;LHV Yates20     Table 3, Page 10                 Tech|Electrolysis*   1 MW        **   Total Input Capacity|Electricity 8.760000e+03        NAMWh/a    NANA      *            Tech|Electrolysis*   5 MW        **   Total Input Capacity|Electricity 4.380000e+04        NAMWh/a    NANA      *            Tech|Electrolysis*   100 MW      **   Total Input Capacity|Electricity 8.760000e+05        NAMWh/a    NANA      *            Tech|Electrolysis*   130000 Nm\u00b3/h**   Total Output Capacity|Hydrogen   3.178875e+06        NAMWh/a;LHVNANA      *            In\u00a0[6]: Copied! <pre>DataSet$new('Tech|Electrolysis')$select(period=2020, subtech='AEL', size='100 MW', override=list('Tech|Electrolysis|Output Capacity|h2'= 'kW;LHV'))\n</pre> DataSet$new('Tech|Electrolysis')$select(period=2020, subtech='AEL', size='100 MW', override=list('Tech|Electrolysis|Output Capacity|h2'= 'kW;LHV')) A data.frame: 21 \u00d7 7 sourcevariablereference_variableregionperiodunitvalue &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;list&gt;&lt;dbl&gt; DEARF23     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2020a * USD_2005/MWh1.665e+02 DEARF23     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2020MWh/MWh2.250e+00 DEARF23     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2020USD_2005/MWh3.330e+00 DEARF23     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2020MWh/a8.760e+05 Holst21     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2020a * USD_2005/MWh1.091e+02 Holst21     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2020MWh/MWh2.100e+00 Holst21     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2020USD_2005/MWh3.290e+00 Holst21     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2020MWh/a8.760e+05 IEA-GHR21   Tech|Electrolysis|Input|Water                     Tech|Electrolysis|Output|Hydrogen         World2020t/MWh7.292e-02 IEA-GHR21   Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2020MWh/a8.760e+05 IRENA22     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2020a * USD_2005/MWh6.803e+01 IRENA22     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2020MWh/MWh2.282e+00 IRENA22     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2020MWh/a8.760e+05 Vartiainen22Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2020a * USD_2005/MWh6.777e+01 Vartiainen22Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2020MWh/MWh2.228e+00 Vartiainen22Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2020USD_2005/MWh1.017e+00 Vartiainen22Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2020MWh/a8.760e+05 Yates20     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2020MWh/MWh2.625e+00 Yates20     Tech|Electrolysis|Input|Water                     Tech|Electrolysis|Output|Hydrogen         World2020t/MWh4.852e-01 Yates20     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2020USD_2005/MWh2.418e+00 Yates20     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2020MWh/a8.760e+05 In\u00a0[7]: Copied! <pre>DataSet$new('Tech|Electrolysis')$select(period=2030, source='Yates20', subtech='AEL', size='100 MW', override={'Tech|Electrolysis|Output Capacity|h2'= 'kW;LHV'}, extrapolate_period=FALSE)\n</pre> DataSet$new('Tech|Electrolysis')$select(period=2030, source='Yates20', subtech='AEL', size='100 MW', override={'Tech|Electrolysis|Output Capacity|h2'= 'kW;LHV'}, extrapolate_period=FALSE) A data.frame: 1 \u00d7 7 sourcevariablereference_variableregionperiodunitvalue &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;list&gt;&lt;dbl&gt; Yates20Tech|Electrolysis|Total Input Capacity|ElectricityNAWorld2030MWh/a876000 In\u00a0[8]: Copied! <pre>DataSet$new('Tech|Electrolysis')$select(subtech=c('AEL', 'PEM'), size='100 MW', override={'Tech|Electrolysis|Input Capacity|Electricity'= 'kW'})\n</pre> DataSet$new('Tech|Electrolysis')$select(subtech=c('AEL', 'PEM'), size='100 MW', override={'Tech|Electrolysis|Input Capacity|Electricity'= 'kW'}) A data.frame: 114 \u00d7 8 subtechsourcevariablereference_variableregionperiodunitvalue &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;list&gt;&lt;dbl&gt; AELAl-Qahtani21Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 AELAl-Qahtani21Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 AELAl-Qahtani21Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 AELDEARF23     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2030a * USD_2005/MWh1.105e+02 AELDEARF23     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2040a * USD_2005/MWh6.650e+01 AELDEARF23     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2050a * USD_2005/MWh5.046e+01 AELDEARF23     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2030MWh/MWh2.163e+00 AELDEARF23     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2040MWh/MWh1.947e+00 AELDEARF23     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2050MWh/MWh1.778e+00 AELDEARF23     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2030USD_2005/MWh2.210e+00 AELDEARF23     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2040USD_2005/MWh1.330e+00 AELDEARF23     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2050USD_2005/MWh1.009e+00 AELDEARF23     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 AELDEARF23     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 AELDEARF23     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 AELHolst21     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2030a * USD_2005/MWh6.895e+01 AELHolst21     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2040a * USD_2005/MWh6.895e+01 AELHolst21     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2050a * USD_2005/MWh6.895e+01 AELHolst21     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2030MWh/MWh1.872e+00 AELHolst21     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2040MWh/MWh1.872e+00 AELHolst21     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2050MWh/MWh1.872e+00 AELHolst21     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2030USD_2005/MWh3.106e+00 AELHolst21     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2040USD_2005/MWh3.106e+00 AELHolst21     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2050USD_2005/MWh3.106e+00 AELHolst21     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 AELHolst21     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 AELHolst21     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 AELIEA-GHR21   Tech|Electrolysis|Input|Water                     Tech|Electrolysis|Output|Hydrogen         World2030t/MWh7.292e-02 AELIEA-GHR21   Tech|Electrolysis|Input|Water                     Tech|Electrolysis|Output|Hydrogen         World2040t/MWh7.292e-02 AELIEA-GHR21   Tech|Electrolysis|Input|Water                     Tech|Electrolysis|Output|Hydrogen         World2050t/MWh7.292e-02 \u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee PEMDEARF23     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 PEMDEARF23     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 PEMDEARF23     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 PEMHolst21     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2030a * USD_2005/MWh7.813e+01 PEMHolst21     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2040a * USD_2005/MWh7.813e+01 PEMHolst21     Tech|Electrolysis|CAPEX                           Tech|Electrolysis|Output Capacity|HydrogenWorld2050a * USD_2005/MWh7.813e+01 PEMHolst21     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2030MWh/MWh1.880e+00 PEMHolst21     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2040MWh/MWh1.880e+00 PEMHolst21     Tech|Electrolysis|Input|Electricity               Tech|Electrolysis|Output|Hydrogen         World2050MWh/MWh1.880e+00 PEMHolst21     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2030USD_2005/MWh2.335e+00 PEMHolst21     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2040USD_2005/MWh2.335e+00 PEMHolst21     Tech|Electrolysis|OPEX Fixed                      Tech|Electrolysis|Output Capacity|HydrogenWorld2050USD_2005/MWh2.335e+00 PEMHolst21     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 PEMHolst21     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 PEMHolst21     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 PEMIEA-GHR21   Tech|Electrolysis|Input|Water                     Tech|Electrolysis|Output|Hydrogen         World2030t/MWh7.292e-02 PEMIEA-GHR21   Tech|Electrolysis|Input|Water                     Tech|Electrolysis|Output|Hydrogen         World2040t/MWh7.292e-02 PEMIEA-GHR21   Tech|Electrolysis|Input|Water                     Tech|Electrolysis|Output|Hydrogen         World2050t/MWh7.292e-02 PEMIEA-GHR21   Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 PEMIEA-GHR21   Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 PEMIEA-GHR21   Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 PEMIRENA22     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 PEMIRENA22     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 PEMIRENA22     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 PEMVartiainen22Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 PEMVartiainen22Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 PEMVartiainen22Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 PEMYates20     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2030MWh/a8.760e+05 PEMYates20     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2040MWh/a8.760e+05 PEMYates20     Tech|Electrolysis|Total Input Capacity|ElectricityNA                                        World2050MWh/a8.760e+05 In\u00a0[9]: Copied! <pre>DataSet$new('Tech|Electrolysis')$aggregate(subtech='AEL', size='100 MW', agg='subtech', override={'Tech|Electrolysis|Output Capacity|Hydrogen'='kW;LHV'})\n</pre> DataSet$new('Tech|Electrolysis')$aggregate(subtech='AEL', size='100 MW', agg='subtech', override={'Tech|Electrolysis|Output Capacity|Hydrogen'='kW;LHV'}) A data.frame: 99 \u00d7 6 sourcevariableregionperiodunitvalue &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;named list&gt;&lt;dbl&gt; DEARF23Tech|Electrolysis|CAPEX                           World2030USD_20051.105e+02 DEARF23Tech|Electrolysis|CAPEX                           World2040USD_20056.650e+01 DEARF23Tech|Electrolysis|CAPEX                           World2050USD_20055.046e+01 DEARF23Tech|Electrolysis|Input|Electricity               World2030MWh2.163e+00 DEARF23Tech|Electrolysis|Input|Electricity               World2040MWh1.947e+00 DEARF23Tech|Electrolysis|Input|Electricity               World2050MWh1.778e+00 DEARF23Tech|Electrolysis|OPEX Fixed                      World2030USD_2005/a2.210e+00 DEARF23Tech|Electrolysis|OPEX Fixed                      World2040USD_2005/a1.330e+00 DEARF23Tech|Electrolysis|OPEX Fixed                      World2050USD_2005/a1.009e+00 DEARF23Tech|Electrolysis|Output Capacity|Hydrogen        World2030MWh/a1.000e+00 DEARF23Tech|Electrolysis|Output Capacity|Hydrogen        World2040MWh/a1.000e+00 DEARF23Tech|Electrolysis|Output Capacity|Hydrogen        World2050MWh/a1.000e+00 DEARF23Tech|Electrolysis|Output|Hydrogen                 World2030MWh1.000e+00 DEARF23Tech|Electrolysis|Output|Hydrogen                 World2040MWh1.000e+00 DEARF23Tech|Electrolysis|Output|Hydrogen                 World2050MWh1.000e+00 DEARF23Tech|Electrolysis|Total Input Capacity|ElectricityWorld2030MWh/a8.760e+05 DEARF23Tech|Electrolysis|Total Input Capacity|ElectricityWorld2040MWh/a8.760e+05 DEARF23Tech|Electrolysis|Total Input Capacity|ElectricityWorld2050MWh/a8.760e+05 Holst21Tech|Electrolysis|CAPEX                           World2030USD_20056.895e+01 Holst21Tech|Electrolysis|CAPEX                           World2040USD_20056.895e+01 Holst21Tech|Electrolysis|CAPEX                           World2050USD_20056.895e+01 Holst21Tech|Electrolysis|Input|Electricity               World2030MWh1.872e+00 Holst21Tech|Electrolysis|Input|Electricity               World2040MWh1.872e+00 Holst21Tech|Electrolysis|Input|Electricity               World2050MWh1.872e+00 Holst21Tech|Electrolysis|OPEX Fixed                      World2030USD_2005/a3.106e+00 Holst21Tech|Electrolysis|OPEX Fixed                      World2040USD_2005/a3.106e+00 Holst21Tech|Electrolysis|OPEX Fixed                      World2050USD_2005/a3.106e+00 Holst21Tech|Electrolysis|Output Capacity|Hydrogen        World2030MWh/a1.000e+00 Holst21Tech|Electrolysis|Output Capacity|Hydrogen        World2040MWh/a1.000e+00 Holst21Tech|Electrolysis|Output Capacity|Hydrogen        World2050MWh/a1.000e+00 \u22ee\u22ee\u22ee\u22ee\u22ee\u22ee Vartiainen22Tech|Electrolysis|OPEX Fixed                      World2030USD_2005/a3.405e-01 Vartiainen22Tech|Electrolysis|OPEX Fixed                      World2040USD_2005/a1.135e-01 Vartiainen22Tech|Electrolysis|OPEX Fixed                      World2050USD_2005/a3.734e-02 Vartiainen22Tech|Electrolysis|Output Capacity|Hydrogen        World2030MWh/a1.000e+00 Vartiainen22Tech|Electrolysis|Output Capacity|Hydrogen        World2040MWh/a1.000e+00 Vartiainen22Tech|Electrolysis|Output Capacity|Hydrogen        World2050MWh/a1.000e+00 Vartiainen22Tech|Electrolysis|Output|Hydrogen                 World2030MWh1.000e+00 Vartiainen22Tech|Electrolysis|Output|Hydrogen                 World2040MWh1.000e+00 Vartiainen22Tech|Electrolysis|Output|Hydrogen                 World2050MWh1.000e+00 Vartiainen22Tech|Electrolysis|Total Input Capacity|ElectricityWorld2030MWh/a8.760e+05 Vartiainen22Tech|Electrolysis|Total Input Capacity|ElectricityWorld2040MWh/a8.760e+05 Vartiainen22Tech|Electrolysis|Total Input Capacity|ElectricityWorld2050MWh/a8.760e+05 Yates20     Tech|Electrolysis|Input|Electricity               World2030MWh2.625e+00 Yates20     Tech|Electrolysis|Input|Electricity               World2040MWh2.625e+00 Yates20     Tech|Electrolysis|Input|Electricity               World2050MWh2.625e+00 Yates20     Tech|Electrolysis|Input|Water                     World2030t4.852e-01 Yates20     Tech|Electrolysis|Input|Water                     World2040t4.852e-01 Yates20     Tech|Electrolysis|Input|Water                     World2050t4.852e-01 Yates20     Tech|Electrolysis|OPEX Fixed                      World2030USD_2005/a2.418e+00 Yates20     Tech|Electrolysis|OPEX Fixed                      World2040USD_2005/a2.418e+00 Yates20     Tech|Electrolysis|OPEX Fixed                      World2050USD_2005/a2.418e+00 Yates20     Tech|Electrolysis|Output Capacity|Hydrogen        World2030MWh/a1.000e+00 Yates20     Tech|Electrolysis|Output Capacity|Hydrogen        World2040MWh/a1.000e+00 Yates20     Tech|Electrolysis|Output Capacity|Hydrogen        World2050MWh/a1.000e+00 Yates20     Tech|Electrolysis|Output|Hydrogen                 World2030MWh1.000e+00 Yates20     Tech|Electrolysis|Output|Hydrogen                 World2040MWh1.000e+00 Yates20     Tech|Electrolysis|Output|Hydrogen                 World2050MWh1.000e+00 Yates20     Tech|Electrolysis|Total Input Capacity|ElectricityWorld2030MWh/a8.760e+05 Yates20     Tech|Electrolysis|Total Input Capacity|ElectricityWorld2040MWh/a8.760e+05 Yates20     Tech|Electrolysis|Total Input Capacity|ElectricityWorld2050MWh/a8.760e+05 In\u00a0[10]: Copied! <pre># DataSet$new('Tech|Methane Reforming')$aggregate(period=2030).query(\"variable.str.contains('OM Cost')\"))\n# display(DataSet('Tech|Methane Reforming').aggregate(period=2030).query(\"variable.str.contains('Demand')\"))\nDataSet$new('Tech|Methane Reforming')$aggregate(period=2030) %&gt;% arrange(variable)\n</pre> # DataSet$new('Tech|Methane Reforming')$aggregate(period=2030).query(\"variable.str.contains('OM Cost')\")) # display(DataSet('Tech|Methane Reforming').aggregate(period=2030).query(\"variable.str.contains('Demand')\")) DataSet$new('Tech|Methane Reforming')$aggregate(period=2030) %&gt;% arrange(variable) A data.frame: 77 \u00d7 7 subtechcapture_ratevariableregionperiodunitvalue &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;named list&gt;&lt;dbl&gt; ATR94.50%Tech|Methane Reforming|CAPEX            World2030USD_20051.148e+01 SMR0.00% Tech|Methane Reforming|CAPEX            World2030USD_20057.169e+01 SMR55.70%Tech|Methane Reforming|CAPEX            World2030USD_20051.741e+02 SMR90.00%Tech|Methane Reforming|CAPEX            World2030USD_20052.797e+02 SMR96.20%Tech|Methane Reforming|CAPEX            World2030USD_20057.395e+00 ATR94.50%Tech|Methane Reforming|Capture Rate     World2030pct9.450e+01 SMR0.00% Tech|Methane Reforming|Capture Rate     World2030pct0.000e+00 SMR55.70%Tech|Methane Reforming|Capture Rate     World2030pct5.570e+01 SMR90.00%Tech|Methane Reforming|Capture Rate     World2030pct9.000e+01 SMR96.20%Tech|Methane Reforming|Capture Rate     World2030pct9.620e+01 ATR94.50%Tech|Methane Reforming|Input|ElectricityWorld2030MWh1.440e-02 SMR0.00% Tech|Methane Reforming|Input|ElectricityWorld2030MWh3.756e-04 SMR96.20%Tech|Methane Reforming|Input|ElectricityWorld2030MWh3.736e-03 ATR94.50%Tech|Methane Reforming|Input|Fossil Gas World2030MWh1.662e-01 SMR0.00% Tech|Methane Reforming|Input|Fossil Gas World2030MWh1.013e+00 SMR55.70%Tech|Methane Reforming|Input|Fossil Gas World2030MWh2.133e+00 SMR90.00%Tech|Methane Reforming|Input|Fossil Gas World2030MWh2.414e+00 SMR96.20%Tech|Methane Reforming|Input|Fossil Gas World2030MWh9.006e-02 ATR0.00% Tech|Methane Reforming|Lifetime         World2030a3.000e+01 ATR55.70%Tech|Methane Reforming|Lifetime         World2030a3.000e+01 ATR90.00%Tech|Methane Reforming|Lifetime         World2030a3.000e+01 ATR94.50%Tech|Methane Reforming|Lifetime         World2030a3.000e+01 ATR96.20%Tech|Methane Reforming|Lifetime         World2030a3.000e+01 SMR0.00% Tech|Methane Reforming|Lifetime         World2030a2.750e+01 SMR55.70%Tech|Methane Reforming|Lifetime         World2030a2.750e+01 SMR90.00%Tech|Methane Reforming|Lifetime         World2030a2.750e+01 SMR94.50%Tech|Methane Reforming|Lifetime         World2030a2.750e+01 SMR96.20%Tech|Methane Reforming|Lifetime         World2030a2.750e+01 ATR0.00% Tech|Methane Reforming|OCF              World2030pct9.000e+01 ATR55.70%Tech|Methane Reforming|OCF              World2030pct9.000e+01 \u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee SMR96.20%Tech|Methane Reforming|OPEX Variable                 World2030USD_20053.519e-01 ATR0.00% Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 ATR55.70%Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 ATR90.00%Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 ATR94.50%Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 ATR96.20%Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 SMR0.00% Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 SMR55.70%Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 SMR90.00%Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 SMR94.50%Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 SMR96.20%Tech|Methane Reforming|Output Capacity|Hydrogen      World2030MWh/a1.000e+00 SMR0.00% Tech|Methane Reforming|Output|Electricity            World2030MWh5.025e-02 SMR55.70%Tech|Methane Reforming|Output|Electricity            World2030MWh7.801e-03 SMR90.00%Tech|Methane Reforming|Output|Electricity            World2030MWh2.371e-03 ATR0.00% Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 ATR55.70%Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 ATR90.00%Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 ATR94.50%Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 ATR96.20%Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 SMR0.00% Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 SMR55.70%Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 SMR90.00%Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 SMR94.50%Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 SMR96.20%Tech|Methane Reforming|Output|Hydrogen               World2030MWh1.000e+00 ATR94.50%Tech|Methane Reforming|Total Output Capacity|HydrogenWorld2030MWh/a8.029e+06 SMR0.00% Tech|Methane Reforming|Total Output Capacity|HydrogenWorld2030MWh/a4.161e+06 SMR55.70%Tech|Methane Reforming|Total Output Capacity|HydrogenWorld2030MWh/a4.161e+06 SMR90.00%Tech|Methane Reforming|Total Output Capacity|HydrogenWorld2030MWh/a4.161e+06 SMR94.50%Tech|Methane Reforming|Total Output Capacity|HydrogenWorld2030MWh/a4.161e+06 SMR96.20%Tech|Methane Reforming|Total Output Capacity|HydrogenWorld2030MWh/a4.161e+06 In\u00a0[11]: Copied! <pre>DataSet$new('Tech|Direct Air Capture')$normalise()\n</pre> DataSet$new('Tech|Direct Air Capture')$normalise() A data.frame: 90 \u00d7 15 parent_variablesubtechcomponentregionperiodvariablereference_variablevalueuncertaintyunitreference_valuereference_unitcommentsourcesource_detail &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt; Tech|Direct Air CaptureHT-DAC#              *2018CAPEX              Output Capacity|Captured CO2951.5012518NAUSD_2005 1t/aEarly plant. Value reported for a capacity of 0.98 Mt-CO2/year. Corresponding to the Carbon Engineering pilot plant.                                     Keith18 Table 3       Tech|Direct Air CaptureHT-DAC#              *2018CAPEX              Output Capacity|Captured CO2658.2314748NAUSD_2005 1t/aNth plant. Value reported for a capacity of 0.98 Mt-CO2/year.                                                                                            Keith18 Table 3       Tech|Direct Air CaptureHT-DAC#              *2018OPEX Variable      Output|Captured CO2          21.5160205NAUSD_2005 1t  Corresponding to the Carbon Engineering pilot plant (scenario C).                                                                                        Keith18 Table 2       Tech|Direct Air CaptureHT-DAC#              *2018Input|Electricity  Output|Captured CO2           0.3660000NAMWh      1t  Corresponding to the Carbon Engineering pilot plant (scenario C).                                                                                        Keith18 Table 2       Tech|Direct Air CaptureHT-DAC#              *2018OCF                  90.0000000NApct     NANA Corresponding to the Carbon Engineering pilot plant (scenario C).                                                                                        Keith18 Table 2       Tech|Direct Air CaptureHT-DAC#              *2018Input|Heat         Output|Captured CO2           1.4583333NAMWh      1t  Is assumed to be NG consumption in the paper, as high temperatures are needed (900\u2009\u00b0C). Corresponding to the Carbon Engineering pilot plant (scenario C).Keith18 Table 2       Tech|Direct Air CaptureHT-DAC#              *2020CAPEX              Output Capacity|Captured CO2810.3907887NAUSD_2005 1t/aNo explicit cost basis given, assumed to be EUR2020 as this is the time of cost. Author's own assumption.                                                Fasihi19Table 4       Tech|Direct Air CaptureLT-DAC#              *2020CAPEX              Output Capacity|Captured CO2725.8715040NAUSD_2005 1t/aNo explicit cost basis given, assumed to be EUR2020 as this is the time of cost. Author's own assumption.                                                Fasihi19Table 4       Tech|Direct Air CaptureHT-DAC#              *2020OPEX Fixed Relative   3.7000000NApct     NANA Author's own assumption.                                                                                                                                 Fasihi19Table 4       Tech|Direct Air CaptureLT-DAC#              *2020OPEX Fixed Relative   4.0000000NApct     NANA Author's own assumption.                                                                                                                                 Fasihi19Table 4       Tech|Direct Air CaptureHT-DAC#              *2020Input|Electricity  Output|Captured CO2           1.5350000NAMWh      1t  Author's own assumption.                                                                                                                                 Fasihi19Table 4       Tech|Direct Air CaptureHT-DAC#              *2020Input|Heat         Output|Captured CO2           0.0000000NAMWh      1t  Author's own assumption.                                                                                                                                 Fasihi19Table 4       Tech|Direct Air CaptureLT-DAC#              *2020Input|Electricity  Output|Captured CO2           0.2500000NAMWh      1t  Author's own assumption.                                                                                                                                 Fasihi19Table 4       Tech|Direct Air CaptureLT-DAC#              *2020Input|Heat         Output|Captured CO2           1.7500000NAMWh      1t  Assuming low temperature (80\u2013100\u2009\u00b0C). Author's own assumption.                                                                                           Fasihi19Table 4       Tech|Direct Air CaptureHT-DAC#              *2020Lifetime             25.0000000NAa       NANA  Fasihi19Table 4       Tech|Direct Air CaptureLT-DAC#              *2020Lifetime             20.0000000NAa       NANA  Fasihi19Table 4       Tech|Direct Air Capture*     CO2 injection  *2021Input|Electricity  Output|Captured CO2           0.0070000NAMWh      1t   Madhu21 Table 1 and 2 Tech|Direct Air Capture*     CO2 compression*2021Input|Electricity  Output|Captured CO2           0.1110000NAMWh      1t   Madhu21 Table 1 and 2 Tech|Direct Air CaptureHT-DACCO2 capture    *2021Input|Electricity  Output|Captured CO2           0.3450000NAMWh      1t  Reference case                                                                                                                                           Madhu21 Table 1       Tech|Direct Air CaptureHT-DACCO2 capture    *2021Input|Electricity  Output|Captured CO2           0.3370000NAMWh      1t  Best case                                                                                                                                                Madhu21 Table 1       Tech|Direct Air CaptureHT-DACCO2 capture    *2021Input|Electricity  Output|Captured CO2           0.4490000NAMWh      1t  Worst case                                                                                                                                               Madhu21 Table 1       Tech|Direct Air CaptureHT-DAC#              *2021Input|Heat         Output|Captured CO2           1.2416667NAMWh      1t  Reference case. Assuming high temperature (850\u2013900\u2009\u00b0C).                                                                                                  Madhu21 Table 1       Tech|Direct Air CaptureHT-DAC#              *2021Input|Heat         Output|Captured CO2           1.1250000NAMWh      1t  Best case. Assuming high temperature (850\u2013900\u2009\u00b0C).                                                                                                       Madhu21 Table 1       Tech|Direct Air CaptureHT-DAC#              *2021Input|Heat         Output|Captured CO2           1.2416667NAMWh      1t  Worst case. Assuming high temperature (850\u2013900\u2009\u00b0C).                                                                                                      Madhu21 Table 1       Tech|Direct Air CaptureLT-DACCO2 capture    *2021Input|Electricity  Output|Captured CO2           0.1800000NAMWh      1t  Reference case.                                                                                                                                          Madhu21 Table 2       Tech|Direct Air CaptureLT-DACCO2 capture    *2021Input|Electricity  Output|Captured CO2           0.1300000NAMWh      1t  Best case.                                                                                                                                               Madhu21 Table 2       Tech|Direct Air CaptureLT-DACCO2 capture    *2021Input|Electricity  Output|Captured CO2           0.3500000NAMWh      1t  Worst case.                                                                                                                                              Madhu21 Table 2       Tech|Direct Air CaptureLT-DAC#              *2021Input|Heat         Output|Captured CO2           0.7222222NAMWh      1t  Reference case. Assuming low temperature (80\u2013120\u2009\u00b0C).                                                                                                    Madhu21 Table 2       Tech|Direct Air CaptureLT-DAC#              *2021Input|Heat         Output|Captured CO2           0.6388889NAMWh      1t  Best case. Assuming low temperature (80\u2013120\u2009\u00b0C).                                                                                                         Madhu21 Table 2       Tech|Direct Air CaptureLT-DAC#              *2021Input|Heat         Output|Captured CO2           1.7222222NAMWh      1t  Worst case. Assuming low temperature (80\u2013120\u2009\u00b0C).                                                                                                        Madhu21 Table 2       \u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee Tech|Direct Air CaptureHT-DAC#                      *2019CAPEX            Output Capacity|Captured CO21146.0000000NAUSD_2005 1t/aNo cost basis given. See Supplementary information. Low assumption. Legend states that reference plant size is used here, which is 1Mt/a for both. Realmonte19Table S4   Tech|Direct Air CaptureHT-DAC#                      *2019CAPEX            Output Capacity|Captured CO22060.0000000NAUSD_2005 1t/aNo cost basis given. See Supplementary information. High assumption. Legend states that reference plant size is used here, which is 1Mt/a for both.Realmonte19Table S4   Tech|Direct Air CaptureLT-DAC#                      *2019Input|ElectricityOutput|Captured CO2            0.1666667NAMWh      1t  See Supplementary information. Low assumption.                                                                                                     Realmonte19Table S4   Tech|Direct Air CaptureLT-DAC#                      *2019Input|ElectricityOutput|Captured CO2            0.3055556NAMWh      1t  See Supplementary information. High assumption.                                                                                                    Realmonte19Table S4   Tech|Direct Air CaptureLT-DAC#                      *2019Input|Heat       Output|Captured CO2            1.2222222NAMWh      1t  Low temperature (85\u00b0-120\u00b0C). See Supplementary information. Low assumption.                                                                        Realmonte19Table S4   Tech|Direct Air CaptureLT-DAC#                      *2019Input|Heat       Output|Captured CO2            2.0000000NAMWh      1t  Low temperature (85\u00b0-120\u00b0C). See Supplementary information. High assumption.                                                                       Realmonte19Table S4   Tech|Direct Air CaptureHT-DAC#                      *2019Input|ElectricityOutput|Captured CO2            0.3611111NAMWh      1t  See Supplementary information. Low assumption.                                                                                                     Realmonte19Table S4   Tech|Direct Air CaptureHT-DAC#                      *2019Input|ElectricityOutput|Captured CO2            0.5000000NAMWh      1t  See Supplementary information. High assumption.                                                                                                    Realmonte19Table S4   Tech|Direct Air CaptureHT-DAC#                      *2019Input|Heat       Output|Captured CO2            1.4722222NAMWh      1t  High temperature (T &gt; 800\u00b0C). See Supplementary information. Low assumption.                                                                       Realmonte19Table S4   Tech|Direct Air CaptureHT-DAC#                      *2019Input|Heat       Output|Captured CO2            2.2500000NAMWh      1t  High temperature (T &gt; 800\u00b0C). See Supplementary information. High assumption.                                                                      Realmonte19Table S4   Tech|Direct Air CaptureLT-DAC#                      *2019Lifetime            15.0000000NAa       NANA See Supplementary information.                                                                                                                     Realmonte19Figure S8  Tech|Direct Air CaptureHT-DAC#                      *2019Lifetime            20.0000000NAa       NANA See Supplementary information.                                                                                                                     Realmonte19Figure S8  Tech|Direct Air CaptureHT-DAC#                      *2019CAPEX            Output Capacity|Captured CO21038.5617586NAUSD_2005 1t/aUpper bound. Second line, for slaker causticizer, and clarificator mentions that currencies are converted to USD 2016.                             NASEM19    Table 5.3  Tech|Direct Air CaptureHT-DAC#                      *2019CAPEX            Output Capacity|Captured CO2 558.5889936NAUSD_2005 1t/aLower bound                                                                                                                                        NASEM19    Table 5.3  Tech|Direct Air CaptureHT-DACOperation &amp; Maintenance*2019OPEX Variable    Output|Captured CO2           14.8957065NAUSD_2005 1t  Lower bound                                                                                                                                        NASEM19    Table 5.3  Tech|Direct Air CaptureHT-DACOperation &amp; Maintenance*2019OPEX Variable    Output|Captured CO2           27.3087952NAUSD_2005 1t  Upper bound                                                                                                                                        NASEM19    Table 5.3  Tech|Direct Air CaptureHT-DACLabour                 *2019OPEX Variable    Output|Captured CO2            4.9652355NAUSD_2005 1t  Lower bound                                                                                                                                        NASEM19    Table 5.3  Tech|Direct Air CaptureHT-DACLabour                 *2019OPEX Variable    Output|Captured CO2            8.2753925NAUSD_2005 1t  Upper bound                                                                                                                                        NASEM19    Table 5.3  Tech|Direct Air CaptureHT-DACOther                  *2019OPEX Variable    Output|Captured CO2            4.1376962NAUSD_2005 1t  Lower bound                                                                                                                                        NASEM19    Table 5.3  Tech|Direct Air CaptureHT-DACOther                  *2019OPEX Variable    Output|Captured CO2            5.7927747NAUSD_2005 1t  Upper bound                                                                                                                                        NASEM19    Table 5.3  Tech|Direct Air CaptureLT-DACAdsorbent              *2019CAPEX            Output Capacity|Captured CO2  57.9277475NAUSD_2005 1t/aCase 2: low cost case.                                                                                                                             NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACAdsorbent              *2019CAPEX            Output Capacity|Captured CO2 153.9223005NAUSD_2005 1t/aCase 4: high cost case. Costs assumed to be in USD2016, like the HT case                                                                           NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACBlower                 *2019CAPEX            Output Capacity|Captured CO2   1.7378324NAUSD_2005 1t/aCase 2: low cost case.                                                                                                                             NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACBlower                 *2019CAPEX            Output Capacity|Captured CO2   5.5445130NAUSD_2005 1t/aCase 4: high cost case.                                                                                                                            NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACVacuum pump            *2019CAPEX            Output Capacity|Captured CO2   2.1516020NAUSD_2005 1t/aCase 2: low cost case.                                                                                                                             NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACVacuum pump            *2019CAPEX            Output Capacity|Captured CO2   7.0340836NAUSD_2005 1t/aCase 4: high cost case.                                                                                                                            NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACAdsorption             *2019OPEX Variable    Output|Captured CO2            7.4478532NAUSD_2005 1t  Case 2: low cost case.                                                                                                                             NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACAdsorption             *2019OPEX Variable    Output|Captured CO2           15.7232457NAUSD_2005 1t  Case 4: high cost case.                                                                                                                            NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACSteam                  *2019OPEX Variable    Output|Captured CO2            1.8205863NAUSD_2005 1t  Case 2: low cost case.                                                                                                                             NASEM19    Table 5.10 Tech|Direct Air CaptureLT-DACSteam                  *2019OPEX Variable    Output|Captured CO2            2.4826177NAUSD_2005 1t  Case 4: high cost case.                                                                                                                            NASEM19    Table 5.10 In\u00a0[12]: Copied! <pre>DataSet$new('Tech|Direct Air Capture')$select()\n</pre> DataSet$new('Tech|Direct Air Capture')$select() <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2030, 2030)No appropriate mapping found to convert row reference to primary output: c(\"HT-DAC\", \"HT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2030, 2030)No appropriate mapping found to convert row reference to primary output: c(\"HT-DAC\", \"HT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"Okzan22\", \"Okzan22\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2030, 2030)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"Okzan22\", \"Okzan22\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2030, 2030)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Adsorbent\", \"Adsorbent\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2030, 2030)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Blower\", \"Blower\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2030, 2030)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Vacuum pump\", \"Vacuum pump\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2040, 2040)No appropriate mapping found to convert row reference to primary output: c(\"HT-DAC\", \"HT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2040, 2040)No appropriate mapping found to convert row reference to primary output: c(\"HT-DAC\", \"HT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"Okzan22\", \"Okzan22\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2040, 2040)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"Okzan22\", \"Okzan22\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2040, 2040)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Adsorbent\", \"Adsorbent\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2040, 2040)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Blower\", \"Blower\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2040, 2040)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Vacuum pump\", \"Vacuum pump\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2050, 2050)No appropriate mapping found to convert row reference to primary output: c(\"HT-DAC\", \"HT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2050, 2050)No appropriate mapping found to convert row reference to primary output: c(\"HT-DAC\", \"HT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"Okzan22\", \"Okzan22\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2050, 2050)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"#\", \"#\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"Okzan22\", \"Okzan22\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2050, 2050)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Adsorbent\", \"Adsorbent\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2050, 2050)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Blower\", \"Blower\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> <pre>Warning message in private$..apply_mappings(selected, var_units):\n\u201cNo appropriate mapping found to convert row reference to primary output: c(2050, 2050)No appropriate mapping found to convert row reference to primary output: c(\"LT-DAC\", \"LT-DAC\")No appropriate mapping found to convert row reference to primary output: c(\"Vacuum pump\", \"Vacuum pump\")No appropriate mapping found to convert row reference to primary output: c(\"World\", \"World\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|CAPEX\", \"Tech|Direct Air Capture|CAPEX\")No appropriate mapping found to convert row reference to primary output: c(\"Tech|Direct Air Capture|Output Capacity|Captured CO2\", \"Tech|Direct Air Capture|Output Capacity|Captured CO2\")No appropriate mapping found to convert row reference to primary output: c(\"NASEM19\", \"NASEM19\")No appropriate mapping found to convert row reference to primary output: c(NA, NA)\u201d\n</pre> A data.frame: 240 \u00d7 9 subtechcomponentsourcevariablereference_variableregionperiodunitvalue &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;list&gt;&lt;dbl&gt; HT-DAC#Fasihi19 Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2030a * USD_2005/t1244.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2040a * USD_2005/t1244.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2050a * USD_2005/t1244.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2         World2030MWh/t   2.3560 HT-DAC#Fasihi19 Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2         World2040MWh/t   2.3560 HT-DAC#Fasihi19 Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2         World2050MWh/t   2.3560 HT-DAC#Fasihi19 Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2030MWh/t   0.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2040MWh/t   0.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2050MWh/t   0.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|Lifetime         NA                                                  World2030a  25.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|Lifetime         NA                                                  World2040a  25.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|Lifetime         NA                                                  World2050a  25.0000 HT-DAC#Fasihi19 Tech|Direct Air Capture|OPEX Fixed       Tech|Direct Air Capture|Output Capacity|Captured CO2World2030USD_2005/t  46.0300 HT-DAC#Fasihi19 Tech|Direct Air Capture|OPEX Fixed       Tech|Direct Air Capture|Output Capacity|Captured CO2World2040USD_2005/t  46.0300 HT-DAC#Fasihi19 Tech|Direct Air Capture|OPEX Fixed       Tech|Direct Air Capture|Output Capacity|Captured CO2World2050USD_2005/t  46.0300 HT-DAC#IEA-DAC22Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2030MWh/t   2.1670 HT-DAC#IEA-DAC22Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2040MWh/t   2.1670 HT-DAC#IEA-DAC22Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2050MWh/t   2.1670 HT-DAC#Keith18  Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2030a * USD_2005/t 348.2000 HT-DAC#Keith18  Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2030a * USD_2005/t 240.9000 HT-DAC#Keith18  Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2040a * USD_2005/t 348.2000 HT-DAC#Keith18  Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2040a * USD_2005/t 240.9000 HT-DAC#Keith18  Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2050a * USD_2005/t 348.2000 HT-DAC#Keith18  Tech|Direct Air Capture|CAPEX            Tech|Direct Air Capture|Output Capacity|Captured CO2World2050a * USD_2005/t 240.9000 HT-DAC#Keith18  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2         World2030MWh/t   0.1340 HT-DAC#Keith18  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2         World2040MWh/t   0.1340 HT-DAC#Keith18  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2         World2050MWh/t   0.1340 HT-DAC#Keith18  Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2030MWh/t   0.5338 HT-DAC#Keith18  Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2040MWh/t   0.5338 HT-DAC#Keith18  Tech|Direct Air Capture|Input|Heat       Tech|Direct Air Capture|Output|Captured CO2         World2050MWh/t   0.5338 \u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee\u22ee LT-DACCO2 capture    Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2040MWh/t3.240e-02 LT-DACCO2 capture    Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2040MWh/t2.340e-02 LT-DACCO2 capture    Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2040MWh/t6.300e-02 LT-DACCO2 capture    Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2050MWh/t3.240e-02 LT-DACCO2 capture    Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2050MWh/t2.340e-02 LT-DACCO2 capture    Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2050MWh/t6.300e-02 LT-DACCO2 compressionIEA-DAC22Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2030MWh/t1.929e-02 LT-DACCO2 compressionIEA-DAC22Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2040MWh/t1.929e-02 LT-DACCO2 compressionIEA-DAC22Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2050MWh/t1.929e-02 LT-DACCO2 compressionMadhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2030MWh/t1.232e-02 LT-DACCO2 compressionMadhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2040MWh/t1.232e-02 LT-DACCO2 compressionMadhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2050MWh/t1.232e-02 LT-DACCO2 injection  Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2030MWh/t4.900e-05 LT-DACCO2 injection  Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2040MWh/t4.900e-05 LT-DACCO2 injection  Madhu21  Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2050MWh/t4.900e-05 LT-DACNon-compressionIEA-DAC22Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2030MWh/t2.500e-01 LT-DACNon-compressionIEA-DAC22Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2040MWh/t2.500e-01 LT-DACNon-compressionIEA-DAC22Tech|Direct Air Capture|Input|ElectricityTech|Direct Air Capture|Output|Captured CO2World2050MWh/t2.500e-01 LT-DACO&amp;M            Okzan22  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2030USD_2005/t2.500e+01 LT-DACO&amp;M            Okzan22  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2030USD_2005/t2.500e+02 LT-DACO&amp;M            Okzan22  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2040USD_2005/t2.500e+01 LT-DACO&amp;M            Okzan22  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2040USD_2005/t2.500e+02 LT-DACO&amp;M            Okzan22  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2050USD_2005/t2.500e+01 LT-DACO&amp;M            Okzan22  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2050USD_2005/t2.500e+02 LT-DACSteam          NASEM19  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2030USD_2005/t3.315e+00 LT-DACSteam          NASEM19  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2030USD_2005/t4.520e+00 LT-DACSteam          NASEM19  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2040USD_2005/t3.315e+00 LT-DACSteam          NASEM19  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2040USD_2005/t4.520e+00 LT-DACSteam          NASEM19  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2050USD_2005/t3.315e+00 LT-DACSteam          NASEM19  Tech|Direct Air Capture|OPEX Variable    Tech|Direct Air Capture|Output|Captured CO2World2050USD_2005/t4.520e+00 In\u00a0[13]: Copied! <pre>TEDF$new('Tech|Haber-Bosch with ASU')$load()# $check()\nDataSet$new('Tech|Haber-Bosch with ASU')$normalise()\n</pre> TEDF$new('Tech|Haber-Bosch with ASU')$load()# $check() DataSet$new('Tech|Haber-Bosch with ASU')$normalise() <pre>&lt;TEDF&gt;\n  Inherits from: &lt;TEBase&gt;\n  Public:\n    check: function (raise_exception = TRUE) \n    check_row: function (row_id, raise_exception = TRUE) \n    clone: function (deep = FALSE) \n    data: active binding\n    file_path: active binding\n    inconsistencies: active binding\n    initialize: function (parent_variable, database_id = \"public\", file_path = NULL, \n    load: function () \n    parent_variable: active binding\n    read: function () \n    write: function () \n  Private:\n    ..columns: list\n    ..df: data.frame\n    ..fields: list\n    ..file_path: ./inst/extdata/database/tedfs/Tech/Haber-Bosch with ASU.csv\n    ..inconsistencies: list\n    ..parent_variable: Tech|Haber-Bosch with ASU\n    ..var_specs: list</pre> <pre>Warning message in readLines(file, warn = readLines.warn):\n\u201cincomplete final line found on './inst/extdata/database/masks/Tech/Haber-Bosch with ASU.yml'\u201d\n</pre> A data.frame: 23 \u00d7 14 parent_variablecomponentregionperiodvariablereference_variablevalueuncertaintyunitreference_valuereference_unitcommentsourcesource_detail &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt; Tech|Haber-Bosch with ASU#                        *2022CAPEX              Output Capacity|Ammonia 383.1801978          NAUSD_2005   1t/a    As stated in Section 2.4.1, the ammonia output of the plant is measured in units of its lower heating value.                                                                                           ArnaizdelPozo22Supplementary Information, Table 2 Tech|Haber-Bosch with ASU#                        *2018CAPEX              Output Capacity|Ammonia 539.0569917          NAUSD_2005   1t/a    Cost basis not given in the paper, assumed to be 2018.                                                                                                                                                 Ikaheimo18     Page 7, Table 3                    Tech|Haber-Bosch with ASU#                        *2025CAPEX              Output Capacity|Ammonia 849.4034361          NAUSD_2005   1t/a    Error in Table 7, units are given in EUR/MW. Table 3 contradicts this                                                                                                                                  Grahn22        Table 7                            Tech|Haber-Bosch with ASU#                        *2040CAPEX              Output Capacity|Ammonia 515.7092291          NAUSD_2005   1t/a    Error in Table 7, units are given in EUR/MW. Table 3 contradicts this                                                                                                                                  Grahn22        Table 7                            Tech|Haber-Bosch with ASU#                        *2022OPEX Fixed Relative    3.0000000          NApct       NANA      ArnaizdelPozo22Supplementary Information, Table 2 Tech|Haber-Bosch with ASU#                        *2025OPEX Fixed Relative    4.0000000          NApct       NANA      Grahn22        Table7                             Tech|Haber-Bosch with ASU#                        *2040OPEX Fixed Relative    4.0000000          NApct       NANA      Grahn22        Table7                             Tech|Haber-Bosch with ASU#                        *2018OPEX Fixed         Output Capacity|Ammonia  10.5332975          NAUSD_2005/a 1t/a    Cost basis not given in the paper, assumed to be 2018.                                                                                                                                                 Ikaheimo18     Page 7, Table 3                    Tech|Haber-Bosch with ASU#                        *2022OPEX Variable      Output|Ammonia         3132.1883891          NAUSD_2005   1t      As stated in Section 2.4.1, the ammonia output of the plant is measured in units of its lower heating value.                                                                                           ArnaizdelPozo22Supplementary Information, Table 2 Tech|Haber-Bosch with ASU#                        *2015Input|Hydrogen     Output|Ammonia            5.9326787          NAMWh;LHV    1t       Matzen15       Page 9 / Table 13                  Tech|Haber-Bosch with ASU#                        *2018Input|Hydrogen     Output|Ammonia            6.0270060          NAMWh;LHV    1t      Found in supplementary information                                                                                                                                                                     Stolz22        Table 5                            Tech|Haber-Bosch with ASUAir-separation unit      *2015Input|Electricity  Output|Ammonia            0.7236234          NAMWh        1t      This source claims 3.1 MJ/kg electricity denand for the ASU. Assuming a nitrogen demand of 0.84 tonnes per tonne of ammonia, this amounts of an electricity demand of 3.1\u00d70.85 GJ per tonne of ammonia.Matzen15       Page 8 / Table 9                   Tech|Haber-Bosch with ASUAir-separation unit      *2016Input|Electricity  Output|Ammonia            0.0500000          NAMWh        1t       GrinbergDana16 Supplementary Table 4              Tech|Haber-Bosch with ASUSynthesis process        *2016Input|Electricity  Output|Ammonia            0.4444444          NAMWh        1t       GrinbergDana16 Supplementary Table 4              Tech|Haber-Bosch with ASUAir-separation unit      *2014Input|Electricity  Output|Ammonia            0.0907563          NAMWh        1t      This line is masked later down the line, as not found in the original source (input error?)                                                                                                            Morgan14        Tech|Haber-Bosch with ASUSynthesis process        *2014Input|Electricity  Output|Ammonia            0.4000000          NAMWh        1t      his line is masked later down the line, as not found in the original source (input error?)                                                                                                             Morgan14        Tech|Haber-Bosch with ASU#                        *2018Input|Electricity  Output|Ammonia            0.6400000          NAMWh        1t       Ikaheimo18     Page 4                             Tech|Haber-Bosch with ASUCompressors              *2017Input|Electricity  Output|Ammonia            1.44444440.3611111114MWh        1t       Bazzanella17   Page 56, Section 4.2.2             Tech|Haber-Bosch with ASUAir-separation unit      *2017Input|Electricity  Output|Ammonia            0.3300000          NAMWh        1t       Bazzanella17   Page 57, Section 4.2.3             Tech|Haber-Bosch with ASUH2 compression and others*2018Input|Electricity  Output|Ammonia            0.3307503          NAMWh        1t      Found in supplementary information                                                                                                                                                                     Stolz22        Table 5                            Tech|Haber-Bosch with ASUAir-separation unit      *2018Input|Electricity  Output|Ammonia            0.13545010.0002625003MWh        1t      Data found in supplementary information. Manually calculated from 0.162 kWh/kg Nitrogen and 0.159kg nitrogen/kWh ammonia.                                                                              Stolz22        Table 5                            Tech|Haber-Bosch with ASU#                        *2025Output|Ammonia     Input|Hydrogen            0.1504760          NAt          1MWh;LHV Grahn22        Table 7                            Tech|Haber-Bosch with ASU#                        *2040Output|Ammonia     Input|Hydrogen            0.1504760          NAt          1MWh;LHV Grahn22        Table 7                            In\u00a0[14]: Copied! <pre>DataSet$new('Tech|Haber-Bosch with ASU')$select(period=2020)\n</pre> DataSet$new('Tech|Haber-Bosch with ASU')$select(period=2020) <pre>Warning message in readLines(file, warn = readLines.warn):\n\u201cincomplete final line found on './inst/extdata/database/masks/Tech/Haber-Bosch with ASU.yml'\u201d\n</pre> A data.frame: 20 \u00d7 8 componentsourcevariablereference_variableregionperiodunitvalue &lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;chr&gt;&lt;dbl&gt;&lt;chr&gt;&lt;dbl&gt; #                        ArnaizdelPozo22Tech|Haber-Bosch with ASU|CAPEX            Tech|Haber-Bosch with ASU|Output Capacity|AmmoniaWorld2020a * USD_2005/t1.200e+06 #                        ArnaizdelPozo22Tech|Haber-Bosch with ASU|OPEX Fixed       Tech|Haber-Bosch with ASU|Output Capacity|AmmoniaWorld2020USD_2005/t    3.601e+04 #                        ArnaizdelPozo22Tech|Haber-Bosch with ASU|OPEX Variable    Tech|Haber-Bosch with ASU|Output|Ammonia         World2020USD_2005/t    9.811e+06 #                        Grahn22        Tech|Haber-Bosch with ASU|CAPEX            Tech|Haber-Bosch with ASU|Output Capacity|AmmoniaWorld2020a * USD_2005/t5.645e+03 #                        Grahn22        Tech|Haber-Bosch with ASU|Input|Hydrogen   Tech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         4.416e+01 #                        Grahn22        Tech|Haber-Bosch with ASU|OPEX Fixed       Tech|Haber-Bosch with ASU|Output Capacity|AmmoniaWorld2020USD_2005/t    2.258e+02 #                        Ikaheimo18     Tech|Haber-Bosch with ASU|CAPEX            Tech|Haber-Bosch with ASU|Output Capacity|AmmoniaWorld2020a * USD_2005/t3.450e+02 #                        Ikaheimo18     Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         4.096e-01 #                        Ikaheimo18     Tech|Haber-Bosch with ASU|OPEX Fixed       Tech|Haber-Bosch with ASU|Output Capacity|AmmoniaWorld2020USD_2005/t    6.741e+00 #                        Matzen15       Tech|Haber-Bosch with ASU|Input|Hydrogen   Tech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         3.520e+01 #                        Stolz22        Tech|Haber-Bosch with ASU|Input|Hydrogen   Tech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         3.632e+01 Air-separation unit      Bazzanella17   Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         1.089e-01 Air-separation unit      GrinbergDana16 Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         2.500e-03 Air-separation unit      Matzen15       Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         5.236e-01 Air-separation unit      Morgan14       Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         8.237e-03 Air-separation unit      Stolz22        Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         1.835e-02 Compressors              Bazzanella17   Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         2.086e+00 H2 compression and othersStolz22        Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         1.094e-01 Synthesis process        GrinbergDana16 Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         1.975e-01 Synthesis process        Morgan14       Tech|Haber-Bosch with ASU|Input|ElectricityTech|Haber-Bosch with ASU|Output|Ammonia         World2020MWh/t         1.600e-01 In\u00a0[15]: Copied! <pre>DataSet$new('Tech|Haber-Bosch with ASU')$aggregate(period=2020)\n</pre> DataSet$new('Tech|Haber-Bosch with ASU')$aggregate(period=2020) <pre>Warning message in readLines(file, warn = readLines.warn):\n\u201cincomplete final line found on './inst/extdata/database/masks/Tech/Haber-Bosch with ASU.yml'\u201d\n</pre> <pre>\nError in eval(parse(text = cond)): object 'variable' not found\nTraceback:\n\n1. DataSet$new(\"Tech|Haber-Bosch with ASU\")$aggregate(period = 2020)\n2. mask$matches(rows)   # at line 910-912 of file /home/philippv/Documents/4-projects/10-posted/01-vcs/posted/R/noslag.R\n3. apply_cond(df, w)   # at line 81-83 of file /home/philippv/Documents/4-projects/10-posted/01-vcs/posted/R/masking.R\n4. filter(eval(parse(text = cond)))   # at line 20 of file /home/philippv/Documents/4-projects/10-posted/01-vcs/posted/R/masking.R\n5. eval(parse(text = cond))\n6. eval(parse(text = cond))</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/python/overview/","title":"Main POSTED tutorial for python","text":"<p>First, we import some general-purpose libraries. The python-side of <code>posted</code> depends on <code>pandas</code> for working with dataframes. Here we also use <code>plotly</code> and <code>itables</code> for plotting and inspecting data, but <code>posted</code> does not depend on those and other tools could be used instead. The package <code>igraph</code> is an optional dependency used for representing the interlinkages in value chains. The package <code>matplotlib</code> is only used for plotting igraphs, which is again optional.</p> In\u00a0[1]: Copied! <pre>import pandas as pd\n\nimport plotly\npd.options.plotting.backend = \"plotly\"\n\n# for process chains only\nimport igraph as ig\nimport matplotlib.pyplot as plt\n</pre> import pandas as pd  import plotly pd.options.plotting.backend = \"plotly\"  # for process chains only import igraph as ig import matplotlib.pyplot as plt <p>The <code>posted</code> package has to be installed in the python environment. If it is not installed yet, you can easily install it from the GitHub source code using <code>pip</code>.</p> In\u00a0[2]: Copied! <pre>try:\n    import posted\nexcept ImportError:\n    ! pip install git+https://github.com:PhilippVerpoort/posted.git@develop\n</pre> try:     import posted except ImportError:     ! pip install git+https://github.com:PhilippVerpoort/posted.git@develop <p>Import specific functions and classes from POSTED that will be used later.</p> In\u00a0[3]: Copied! <pre>from posted.tedf import TEDF\nfrom posted.noslag import DataSet\nfrom posted.units import Q, U\nfrom posted.team import CalcVariable, LCOX, FSCP, ProcessChain, annuity_factor\n</pre> from posted.tedf import TEDF from posted.noslag import DataSet from posted.units import Q, U from posted.team import CalcVariable, LCOX, FSCP, ProcessChain, annuity_factor <p>Use some basic plotly and pandas functions for plotting and output analysis</p> <p>Let's compare CAPEX data for electrolysis in years 2020\u20132050 for Alkaline and PEM across different sources (Danish Energy Agency, Breyer, Fraunhofer, IRENA) for different electrolyser plant sizes.</p> In\u00a0[4]: Copied! <pre># select data from TEDFs\ndf_elh2 = DataSet('Tech|Electrolysis').select(\n        period=[2020, 2030, 2040, 2050],\n        subtech=['AEL', 'PEM'],\n        override={'Tech|Electrolysis|Output Capacity|Hydrogen': 'kW;LHV'},\n        source=['DEARF23', 'Vartiainen22', 'Holst21', 'IRENA22'],\n        size=['1 MW', '5 MW', '100 MW'],\n        extrapolate_period=False\n    ).query(f\"variable=='Tech|Electrolysis|CAPEX'\")\n\n# display a few examples\ndisplay(df_elh2.sample(15).sort_index())\n\n# sort data and plot\ndf_elh2.assign(size_sort=lambda df: df['size'].str.split(' ', n=1, expand=True).iloc[:, 0].astype(int)) \\\n       .sort_values(by=['size_sort', 'period']) \\\n       .plot.line(x='period', y='value', color='source', facet_col='size', facet_row='subtech')\n</pre> # select data from TEDFs df_elh2 = DataSet('Tech|Electrolysis').select(         period=[2020, 2030, 2040, 2050],         subtech=['AEL', 'PEM'],         override={'Tech|Electrolysis|Output Capacity|Hydrogen': 'kW;LHV'},         source=['DEARF23', 'Vartiainen22', 'Holst21', 'IRENA22'],         size=['1 MW', '5 MW', '100 MW'],         extrapolate_period=False     ).query(f\"variable=='Tech|Electrolysis|CAPEX'\")  # display a few examples display(df_elh2.sample(15).sort_index())  # sort data and plot df_elh2.assign(size_sort=lambda df: df['size'].str.split(' ', n=1, expand=True).iloc[:, 0].astype(int)) \\        .sort_values(by=['size_sort', 'period']) \\        .plot.line(x='period', y='value', color='source', facet_col='size', facet_row='subtech') subtech size source variable reference_variable region period unit value 0 AEL 1 MW DEARF23 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2020 USD_2005/kW 1121.0 1 AEL 1 MW DEARF23 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2030 USD_2005/kW 833.4 37 AEL 100 MW DEARF23 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2020 USD_2005/kW 972.3 38 AEL 100 MW DEARF23 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2030 USD_2005/kW 658.0 53 AEL 100 MW Holst21 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2020 USD_2005/kW 955.4 54 AEL 100 MW Holst21 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2030 USD_2005/kW 604.0 64 AEL 100 MW IRENA22 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2050 USD_2005/kW 236.2 73 AEL 100 MW Vartiainen22 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2030 USD_2005/kW 340.9 74 AEL 100 MW Vartiainen22 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2040 USD_2005/kW 190.7 96 AEL 5 MW Holst21 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2020 USD_2005/kW 1367.0 107 AEL 5 MW IRENA22 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2050 USD_2005/kW 236.2 119 PEM 1 MW DEARF23 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2020 USD_2005/kW 1586.0 121 PEM 1 MW DEARF23 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2040 USD_2005/kW 658.0 150 PEM 100 MW DEARF23 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2020 USD_2005/kW 1586.0 167 PEM 100 MW Holst21 Tech|Electrolysis|CAPEX Tech|Electrolysis|Output Capacity|Hydrogen World 2030 USD_2005/kW 684.4 <p>Based on those many sources and cases (size and subtechnology), we can now aggregate the data for further use.</p> In\u00a0[5]: Copied! <pre>DataSet('Tech|Electrolysis').aggregate(\n        period=[2020, 2030, 2040, 2050],\n        subtech=['AEL', 'PEM'],\n        override={'Tech|Electrolysis|Output Capacity|Hydrogen': 'kW;LHV'},\n        source=['DEARF23', 'Vartiainen22', 'Holst21', 'IRENA22'],\n        size=['1 MW', '5 MW', '100 MW'],\n        agg=['subtech', 'size', 'source'],\n        extrapolate_period=False,\n    ).team.varsplit('Tech|Electrolysis|*variable') \\\n    .query(f\"variable.isin({['CAPEX', 'Output Capacity|Hydrogen']})\")\n</pre> DataSet('Tech|Electrolysis').aggregate(         period=[2020, 2030, 2040, 2050],         subtech=['AEL', 'PEM'],         override={'Tech|Electrolysis|Output Capacity|Hydrogen': 'kW;LHV'},         source=['DEARF23', 'Vartiainen22', 'Holst21', 'IRENA22'],         size=['1 MW', '5 MW', '100 MW'],         agg=['subtech', 'size', 'source'],         extrapolate_period=False,     ).team.varsplit('Tech|Electrolysis|*variable') \\     .query(f\"variable.isin({['CAPEX', 'Output Capacity|Hydrogen']})\") Out[5]: variable region period unit value 0 CAPEX World 2020 USD_2005 1046.0 1 CAPEX World 2030 USD_2005 737.4 2 CAPEX World 2040 USD_2005 586.1 3 CAPEX World 2050 USD_2005 499.3 12 Output Capacity|Hydrogen World 2020 kW 1.0 13 Output Capacity|Hydrogen World 2030 kW 1.0 14 Output Capacity|Hydrogen World 2040 kW 1.0 15 Output Capacity|Hydrogen World 2050 kW 1.0 <p>Next, let's compare the energy demand of methane reforming (for blue hydrogen) and different types of electrolysis (for green hydrogen).</p> In\u00a0[6]: Copied! <pre>pd.concat([\n        DataSet('Tech|Methane Reforming').aggregate(period=2030, source='Lewis22'),\n        DataSet('Tech|Electrolysis').aggregate(period=2030, agg=['source', 'size']),\n    ]) \\\n    .reset_index(drop=True) \\\n    .team.varsplit('Tech|?tech|Input|?fuel') \\\n    .assign(tech=lambda df: df.apply(lambda row: f\"{row['tech']}&lt;br&gt;({row['subtech']})\" if pd.isnull(row['capture_rate']) else f\"{row['tech']}&lt;br&gt;({row['subtech']}, {row['capture_rate']} CR)\", axis=1)) \\\n    .plot.bar(x='tech', y='value', color='fuel') \\\n    .update_layout(\n        xaxis_title='Technologies',\n        yaxis_title='Energy demand  ( MWh&lt;sub&gt;LHV&lt;/sub&gt; / MWh&lt;sub&gt;LHV&lt;/sub&gt; H&lt;sub&gt;2&lt;/sub&gt; )',\n        legend_title='Energy carriers',\n    )\n</pre> pd.concat([         DataSet('Tech|Methane Reforming').aggregate(period=2030, source='Lewis22'),         DataSet('Tech|Electrolysis').aggregate(period=2030, agg=['source', 'size']),     ]) \\     .reset_index(drop=True) \\     .team.varsplit('Tech|?tech|Input|?fuel') \\     .assign(tech=lambda df: df.apply(lambda row: f\"{row['tech']}({row['subtech']})\" if pd.isnull(row['capture_rate']) else f\"{row['tech']}({row['subtech']}, {row['capture_rate']} CR)\", axis=1)) \\     .plot.bar(x='tech', y='value', color='fuel') \\     .update_layout(         xaxis_title='Technologies',         yaxis_title='Energy demand  ( MWh<sub>LHV</sub> / MWh<sub>LHV</sub> H<sub>2</sub> )',         legend_title='Energy carriers',     ) <p>Next, let's compare the energy demand of iron direct reduction (production of low-carbon crude iron) across sources.</p> In\u00a0[7]: Copied! <pre>DataSet('Tech|Iron Direct Reduction') \\\n    .aggregate(period=2030, mode='h2', agg=[]) \\\n    .team.varsplit('Tech|Iron Direct Reduction|Input|?fuel') \\\n    .query(f\"fuel != 'Iron Ore'\") \\\n    .team.varcombine('{fuel} ({component})') \\\n    .plot.bar(x='source', y='value', color='variable') \\\n    .update_layout(\n        xaxis_title='Sources',\n        yaxis_title='Energy demand  ( MWh&lt;sub&gt;LHV&lt;/sub&gt; / t&lt;sub&gt;DRI&lt;/sub&gt; )',\n        legend_title='Energy carriers'\n    )\n</pre> DataSet('Tech|Iron Direct Reduction') \\     .aggregate(period=2030, mode='h2', agg=[]) \\     .team.varsplit('Tech|Iron Direct Reduction|Input|?fuel') \\     .query(f\"fuel != 'Iron Ore'\") \\     .team.varcombine('{fuel} ({component})') \\     .plot.bar(x='source', y='value', color='variable') \\     .update_layout(         xaxis_title='Sources',         yaxis_title='Energy demand  ( MWh<sub>LHV</sub> / t<sub>DRI</sub> )',         legend_title='Energy carriers'     ) <p>We can also compare the energy demand for operation with hydrogen or with fossil gas for only one source.</p> In\u00a0[8]: Copied! <pre>DataSet('Tech|Iron Direct Reduction') \\\n    .select(period=2030, source='Jacobasch21') \\\n    .team.varsplit('Tech|Iron Direct Reduction|Input|?fuel') \\\n    .query(f\"fuel.isin({['Electricity', 'Fossil Gas', 'Hydrogen']})\") \\\n    .plot.bar(x='mode', y='value', color='fuel') \\\n    .update_layout(\n        xaxis_title='Mode of operation',\n        yaxis_title='Energy demand  ( MWh&lt;sub&gt;LHV&lt;/sub&gt; / t&lt;sub&gt;DRI&lt;/sub&gt; )',\n        legend_title='Energy carriers'\n    )\n</pre> DataSet('Tech|Iron Direct Reduction') \\     .select(period=2030, source='Jacobasch21') \\     .team.varsplit('Tech|Iron Direct Reduction|Input|?fuel') \\     .query(f\"fuel.isin({['Electricity', 'Fossil Gas', 'Hydrogen']})\") \\     .plot.bar(x='mode', y='value', color='fuel') \\     .update_layout(         xaxis_title='Mode of operation',         yaxis_title='Energy demand  ( MWh<sub>LHV</sub> / t<sub>DRI</sub> )',         legend_title='Energy carriers'     ) <p>Finally, let's compare the energy demand of Haber-Bosch synthesis between an integrated SMR plant and a plant running on green hydrogen.</p> In\u00a0[9]: Copied! <pre>pd.concat([\n        DataSet('Tech|Haber-Bosch with ASU').aggregate(period=2024, agg='component'),\n        DataSet('Tech|Haber-Bosch with Reforming').aggregate(period=2024, agg='component')\n    ]) \\\n    .reset_index(drop=True) \\\n    .team.varsplit('Tech|?tech|*variable') \\\n    .query(f\"variable.str.startswith('Input|')\") \\\n    .plot.bar(x='source', y='value', color='variable') \\\n    .update_layout(\n        xaxis_title='Sources',\n        yaxis_title='Energy demand  ( MWh&lt;sub&gt;LHV&lt;/sub&gt; / t&lt;sub&gt;NH&lt;sub&gt;3&lt;/sub&gt;&lt;/sub&gt; )',\n        legend_title='Energy carriers'\n    )\n</pre> pd.concat([         DataSet('Tech|Haber-Bosch with ASU').aggregate(period=2024, agg='component'),         DataSet('Tech|Haber-Bosch with Reforming').aggregate(period=2024, agg='component')     ]) \\     .reset_index(drop=True) \\     .team.varsplit('Tech|?tech|*variable') \\     .query(f\"variable.str.startswith('Input|')\") \\     .plot.bar(x='source', y='value', color='variable') \\     .update_layout(         xaxis_title='Sources',         yaxis_title='Energy demand  ( MWh<sub>LHV</sub> / t<sub>NH<sub>3</sub></sub> )',         legend_title='Energy carriers'     ) <p>New variables can be calculated manually via the <code>CalcVariable</code> class. The next example demonstrates this for calculating the levelised cost of hydrogen.</p> In\u00a0[10]: Copied! <pre>assumptions = pd.DataFrame.from_records([\n    {'elec_price_case': f\"Case {i}\", 'variable': 'Price|Electricity', 'unit': 'EUR_2020/MWh', 'value': 30 + (i-1)*25}\n    for i in range(1, 4)\n] + [\n    {'variable': 'Tech|Electrolysis|OCF', 'value': 50, 'unit': 'pct'},\n    {'variable': 'Annuity Factor', 'value': annuity_factor(Q('5 pct'), Q('18 a')).m, 'unit': '1/a'},\n])\ndisplay(assumptions)\n</pre> assumptions = pd.DataFrame.from_records([     {'elec_price_case': f\"Case {i}\", 'variable': 'Price|Electricity', 'unit': 'EUR_2020/MWh', 'value': 30 + (i-1)*25}     for i in range(1, 4) ] + [     {'variable': 'Tech|Electrolysis|OCF', 'value': 50, 'unit': 'pct'},     {'variable': 'Annuity Factor', 'value': annuity_factor(Q('5 pct'), Q('18 a')).m, 'unit': '1/a'}, ]) display(assumptions) elec_price_case variable unit value 0 Case 1 Price|Electricity EUR_2020/MWh 30.000000 1 Case 2 Price|Electricity EUR_2020/MWh 55.000000 2 Case 3 Price|Electricity EUR_2020/MWh 80.000000 3 NaN Tech|Electrolysis|OCF pct 50.000000 4 NaN Annuity Factor 1/a 0.085546 In\u00a0[11]: Copied! <pre>df_calc = pd.concat([\n        DataSet('Tech|Electrolysis').aggregate(period=[2030, 2040, 2050], subtech=['AEL', 'PEM'], agg=['size', 'source']),\n        assumptions,\n    ]).team.perform(CalcVariable(**{\n        'LCOX|Green Hydrogen|Capital Cost': lambda x: (x['Annuity Factor'] * x['Tech|Electrolysis|CAPEX'] / x['Tech|Electrolysis|Output Capacity|Hydrogen'] / x['Tech|Electrolysis|OCF']),\n        'LCOX|Green Hydrogen|OM Cost Fixed': lambda x: x['Tech|Electrolysis|OPEX Fixed'] / x['Tech|Electrolysis|Output Capacity|Hydrogen'] / x['Tech|Electrolysis|OCF'],\n        'LCOX|Green Hydrogen|Input Cost|Electricity': lambda x: x['Price|Electricity'] * x['Tech|Electrolysis|Input|Electricity'] / x['Tech|Electrolysis|Output|Hydrogen'],\n    }), only_new=True) \\\n    .team.unit_convert(to='EUR_2020/MWh')\n\ndisplay(df_calc.sample(15).sort_index())\n</pre> df_calc = pd.concat([         DataSet('Tech|Electrolysis').aggregate(period=[2030, 2040, 2050], subtech=['AEL', 'PEM'], agg=['size', 'source']),         assumptions,     ]).team.perform(CalcVariable(**{         'LCOX|Green Hydrogen|Capital Cost': lambda x: (x['Annuity Factor'] * x['Tech|Electrolysis|CAPEX'] / x['Tech|Electrolysis|Output Capacity|Hydrogen'] / x['Tech|Electrolysis|OCF']),         'LCOX|Green Hydrogen|OM Cost Fixed': lambda x: x['Tech|Electrolysis|OPEX Fixed'] / x['Tech|Electrolysis|Output Capacity|Hydrogen'] / x['Tech|Electrolysis|OCF'],         'LCOX|Green Hydrogen|Input Cost|Electricity': lambda x: x['Price|Electricity'] * x['Tech|Electrolysis|Input|Electricity'] / x['Tech|Electrolysis|Output|Hydrogen'],     }), only_new=True) \\     .team.unit_convert(to='EUR_2020/MWh')  display(df_calc.sample(15).sort_index()) subtech region period elec_price_case variable value unit 1 AEL World 2030.0 Case 2 LCOX|Green Hydrogen|Capital Cost 12.824046 EUR_2020/MWh 10 PEM World 2030.0 Case 2 LCOX|Green Hydrogen|Capital Cost 19.839159 EUR_2020/MWh 12 PEM World 2040.0 Case 1 LCOX|Green Hydrogen|Capital Cost 17.258175 EUR_2020/MWh 13 PEM World 2040.0 Case 2 LCOX|Green Hydrogen|Capital Cost 17.258175 EUR_2020/MWh 19 AEL World 2030.0 Case 2 LCOX|Green Hydrogen|OM Cost Fixed 5.247678 EUR_2020/MWh 21 AEL World 2040.0 Case 1 LCOX|Green Hydrogen|OM Cost Fixed 4.887642 EUR_2020/MWh 22 AEL World 2040.0 Case 2 LCOX|Green Hydrogen|OM Cost Fixed 4.887642 EUR_2020/MWh 25 AEL World 2050.0 Case 2 LCOX|Green Hydrogen|OM Cost Fixed 4.670413 EUR_2020/MWh 31 PEM World 2040.0 Case 2 LCOX|Green Hydrogen|OM Cost Fixed 5.901375 EUR_2020/MWh 37 AEL World 2030.0 Case 2 LCOX|Green Hydrogen|Input Cost|Electricity 82.170000 EUR_2020/MWh 39 AEL World 2040.0 Case 1 LCOX|Green Hydrogen|Input Cost|Electricity 43.830000 EUR_2020/MWh 43 AEL World 2050.0 Case 2 LCOX|Green Hydrogen|Input Cost|Electricity 78.760000 EUR_2020/MWh 45 PEM World 2030.0 Case 1 LCOX|Green Hydrogen|Input Cost|Electricity 45.690000 EUR_2020/MWh 50 PEM World 2040.0 Case 3 LCOX|Green Hydrogen|Input Cost|Electricity 120.560000 EUR_2020/MWh 52 PEM World 2050.0 Case 2 LCOX|Green Hydrogen|Input Cost|Electricity 81.950000 EUR_2020/MWh In\u00a0[12]: Copied! <pre>df_calc.team.varsplit('LCOX|Green Hydrogen|?component') \\\n    .sort_values(by=['elec_price_case', 'value']) \\\n    .plot.bar(x='period', y='value', color='component', facet_col='elec_price_case', facet_row='subtech')\n</pre> df_calc.team.varsplit('LCOX|Green Hydrogen|?component') \\     .sort_values(by=['elec_price_case', 'value']) \\     .plot.bar(x='period', y='value', color='component', facet_col='elec_price_case', facet_row='subtech') <p>POSTED uses the <code>pivot</code> dataframe method to bring the data into a usable format.</p> In\u00a0[13]: Copied! <pre>pd.concat([\n        DataSet('Tech|Electrolysis').aggregate(period=[2030, 2040, 2050], subtech=['AEL', 'PEM'], agg=['size', 'source']),\n        assumptions,\n    ]).team.pivot_wide().pint.dequantify()\n</pre> pd.concat([         DataSet('Tech|Electrolysis').aggregate(period=[2030, 2040, 2050], subtech=['AEL', 'PEM'], agg=['size', 'source']),         assumptions,     ]).team.pivot_wide().pint.dequantify() Out[13]: variable Tech|Electrolysis|CAPEX Tech|Electrolysis|Input|Electricity Tech|Electrolysis|Input|Water Tech|Electrolysis|OPEX Fixed Tech|Electrolysis|Output Capacity|Hydrogen Tech|Electrolysis|Output|Hydrogen Tech|Electrolysis|Total Input Capacity|Electricity Tech|Electrolysis|Total Output Capacity|Hydrogen Price|Electricity Tech|Electrolysis|OCF Annuity Factor unit USD_2005 MWh t USD_2005/a MWh/a MWh MWh/a MWh/a EUR_2020/MWh pct 1/a subtech region period elec_price_case AEL World 2030.0 Case 1 74.53 1.494 0.2848 2.609 1.0 1.0 309500.0 3179000.0 30.0 50.0 0.085546 Case 2 74.53 1.494 0.2848 2.609 1.0 1.0 309500.0 3179000.0 55.0 50.0 0.085546 Case 3 74.53 1.494 0.2848 2.609 1.0 1.0 309500.0 3179000.0 80.0 50.0 0.085546 2040.0 Case 1 59.19 1.461 0.2848 2.430 1.0 1.0 309500.0 3179000.0 30.0 50.0 0.085546 Case 2 59.19 1.461 0.2848 2.430 1.0 1.0 309500.0 3179000.0 55.0 50.0 0.085546 Case 3 59.19 1.461 0.2848 2.430 1.0 1.0 309500.0 3179000.0 80.0 50.0 0.085546 2050.0 Case 1 47.46 1.432 0.2848 2.322 1.0 1.0 309500.0 3179000.0 30.0 50.0 0.085546 Case 2 47.46 1.432 0.2848 2.322 1.0 1.0 309500.0 3179000.0 55.0 50.0 0.085546 Case 3 47.46 1.432 0.2848 2.322 1.0 1.0 309500.0 3179000.0 80.0 50.0 0.085546 PEM World 2030.0 Case 1 115.30 1.523 0.2850 3.621 1.0 1.0 309500.0 3179000.0 30.0 50.0 0.085546 Case 2 115.30 1.523 0.2850 3.621 1.0 1.0 309500.0 3179000.0 55.0 50.0 0.085546 Case 3 115.30 1.523 0.2850 3.621 1.0 1.0 309500.0 3179000.0 80.0 50.0 0.085546 2040.0 Case 1 100.30 1.507 0.2850 2.934 1.0 1.0 309500.0 3179000.0 30.0 50.0 0.085546 Case 2 100.30 1.507 0.2850 2.934 1.0 1.0 309500.0 3179000.0 55.0 50.0 0.085546 Case 3 100.30 1.507 0.2850 2.934 1.0 1.0 309500.0 3179000.0 80.0 50.0 0.085546 2050.0 Case 1 96.00 1.490 0.2850 2.737 1.0 1.0 309500.0 3179000.0 30.0 50.0 0.085546 Case 2 96.00 1.490 0.2850 2.737 1.0 1.0 309500.0 3179000.0 55.0 50.0 0.085546 Case 3 96.00 1.490 0.2850 2.737 1.0 1.0 309500.0 3179000.0 80.0 50.0 0.085546 <p>POSTED also contains predefined methods for calculating LCOX. Here we apply it to blue and green hydrogen.</p> In\u00a0[14]: Copied! <pre>df_lcox_bluegreen = pd.concat([\n        pd.DataFrame.from_records([\n            {'elec_price_case': f\"Case {i}\", 'variable': 'Price|Electricity', 'unit': 'EUR_2020/MWh', 'value': 30 + (i-1)*25}\n            for i in range(1, 4)\n        ]),\n        pd.DataFrame.from_records([\n            {'ng_price_case': 'High' if i-1 else 'Low', 'variable': 'Price|Fossil Gas', 'unit': 'EUR_2020/MWh', 'value': 40 if i-1 else 20}\n            for i in range(1, 3)\n        ]),\n        DataSet('Tech|Electrolysis').aggregate(period=2030, subtech=['AEL', 'PEM'], agg=['size', 'subtech', 'source']),\n        DataSet('Tech|Methane Reforming').aggregate(period=2030, capture_rate=['55.70%', '94.50%'])\n            .team.varsplit('Tech|Methane Reforming|*comp')\n            .team.varcombine('{variable} {subtech} ({capture_rate})|{comp}')\n    ]) \\\n    .team.perform(\n        LCOX('Output|Hydrogen', 'Electrolysis', name='Green Hydrogen', interest_rate=0.1, book_lifetime=18),\n        LCOX('Output|Hydrogen', 'Methane Reforming SMR (55.70%)', name='Blue Hydrogen (Low CR)', interest_rate=0.1, book_lifetime=18),\n        LCOX('Output|Hydrogen', 'Methane Reforming ATR (94.50%)', name='Blue Hydrogen (High CR)', interest_rate=0.1, book_lifetime=18),\n        only_new=True,\n    ) \\\n    .team.unit_convert(to='EUR_2022/MWh')\n\ndisplay(df_lcox_bluegreen)\n</pre> df_lcox_bluegreen = pd.concat([         pd.DataFrame.from_records([             {'elec_price_case': f\"Case {i}\", 'variable': 'Price|Electricity', 'unit': 'EUR_2020/MWh', 'value': 30 + (i-1)*25}             for i in range(1, 4)         ]),         pd.DataFrame.from_records([             {'ng_price_case': 'High' if i-1 else 'Low', 'variable': 'Price|Fossil Gas', 'unit': 'EUR_2020/MWh', 'value': 40 if i-1 else 20}             for i in range(1, 3)         ]),         DataSet('Tech|Electrolysis').aggregate(period=2030, subtech=['AEL', 'PEM'], agg=['size', 'subtech', 'source']),         DataSet('Tech|Methane Reforming').aggregate(period=2030, capture_rate=['55.70%', '94.50%'])             .team.varsplit('Tech|Methane Reforming|*comp')             .team.varcombine('{variable} {subtech} ({capture_rate})|{comp}')     ]) \\     .team.perform(         LCOX('Output|Hydrogen', 'Electrolysis', name='Green Hydrogen', interest_rate=0.1, book_lifetime=18),         LCOX('Output|Hydrogen', 'Methane Reforming SMR (55.70%)', name='Blue Hydrogen (Low CR)', interest_rate=0.1, book_lifetime=18),         LCOX('Output|Hydrogen', 'Methane Reforming ATR (94.50%)', name='Blue Hydrogen (High CR)', interest_rate=0.1, book_lifetime=18),         only_new=True,     ) \\     .team.unit_convert(to='EUR_2022/MWh')  display(df_lcox_bluegreen) <pre>/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Water\n\n</pre> elec_price_case ng_price_case region period variable value unit 0 Case 1 High World 2030.0 LCOX|Green Hydrogen|Capital 11.746556 EUR_2022/MWh 1 Case 1 Low World 2030.0 LCOX|Green Hydrogen|Capital 11.746556 EUR_2022/MWh 2 Case 2 High World 2030.0 LCOX|Green Hydrogen|Capital 11.746556 EUR_2022/MWh 3 Case 2 Low World 2030.0 LCOX|Green Hydrogen|Capital 11.746556 EUR_2022/MWh 4 Case 3 High World 2030.0 LCOX|Green Hydrogen|Capital 11.746556 EUR_2022/MWh ... ... ... ... ... ... ... ... 73 Case 1 Low World 2030.0 LCOX|Blue Hydrogen (High CR)|Input Cost|Fossil... 30.112121 EUR_2022/MWh 74 Case 2 High World 2030.0 LCOX|Blue Hydrogen (High CR)|Input Cost|Fossil... 60.224242 EUR_2022/MWh 75 Case 2 Low World 2030.0 LCOX|Blue Hydrogen (High CR)|Input Cost|Fossil... 30.112121 EUR_2022/MWh 76 Case 3 High World 2030.0 LCOX|Blue Hydrogen (High CR)|Input Cost|Fossil... 60.224242 EUR_2022/MWh 77 Case 3 Low World 2030.0 LCOX|Blue Hydrogen (High CR)|Input Cost|Fossil... 30.112121 EUR_2022/MWh <p>78 rows \u00d7 7 columns</p> In\u00a0[15]: Copied! <pre>df_lcox_bluegreen.team.varsplit('LCOX|?fuel|*comp') \\\n    .plot.bar(x='fuel', y='value', color='comp', facet_col='elec_price_case', facet_row='ng_price_case')\n</pre> df_lcox_bluegreen.team.varsplit('LCOX|?fuel|*comp') \\     .plot.bar(x='fuel', y='value', color='comp', facet_col='elec_price_case', facet_row='ng_price_case') <p>Let's calculate the levelised cost of green methanol (from electrolytic hydrogen). First we can do this simply based on a hydrogen price (i.e. without accounting for electrolysis).</p> In\u00a0[16]: Copied! <pre>df_lcox_meoh = pd.concat([\n        DataSet('Tech|Methanol Synthesis').aggregate(period=[2030, 2050]),\n        pd.DataFrame.from_records([\n            {'period': 2030, 'variable': 'Price|Hydrogen', 'unit': 'EUR_2022/MWh', 'value': 120},\n            {'period': 2050, 'variable': 'Price|Hydrogen', 'unit': 'EUR_2022/MWh', 'value': 80},\n            {'period': 2030, 'variable': 'Price|Captured CO2', 'unit': 'EUR_2022/t', 'value': 150},\n            {'period': 2050, 'variable': 'Price|Captured CO2', 'unit': 'EUR_2022/t', 'value': 100},\n        ]),\n    ]) \\\n    .team.perform(LCOX(\n        'Output|Methanol', 'Methanol Synthesis', name='Green Methanol',\n        interest_rate=0.1, book_lifetime=10.0), only_new=True,\n    ) \\\n    .team.unit_convert('EUR_2022/MWh')\n\ndisplay(df_lcox_meoh)\n</pre> df_lcox_meoh = pd.concat([         DataSet('Tech|Methanol Synthesis').aggregate(period=[2030, 2050]),         pd.DataFrame.from_records([             {'period': 2030, 'variable': 'Price|Hydrogen', 'unit': 'EUR_2022/MWh', 'value': 120},             {'period': 2050, 'variable': 'Price|Hydrogen', 'unit': 'EUR_2022/MWh', 'value': 80},             {'period': 2030, 'variable': 'Price|Captured CO2', 'unit': 'EUR_2022/t', 'value': 150},             {'period': 2050, 'variable': 'Price|Captured CO2', 'unit': 'EUR_2022/t', 'value': 100},         ]),     ]) \\     .team.perform(LCOX(         'Output|Methanol', 'Methanol Synthesis', name='Green Methanol',         interest_rate=0.1, book_lifetime=10.0), only_new=True,     ) \\     .team.unit_convert('EUR_2022/MWh')  display(df_lcox_meoh) <pre>/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/noslag.py:368: UserWarning:\n\nUnknown variable, so dropping rows:\n36    Emissions|CO2\n37    Emissions|CO2\nName: variable, dtype: object\n\n</pre> <pre>/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Output|Heat\n\n</pre> region period variable value unit 0 World 2030 LCOX|Green Methanol|Capital 10.583450 EUR_2022/MWh 1 World 2050 LCOX|Green Methanol|Capital 9.831386 EUR_2022/MWh 2 World 2030 LCOX|Green Methanol|OM Fixed 3.231184 EUR_2022/MWh 3 World 2050 LCOX|Green Methanol|OM Fixed 2.984565 EUR_2022/MWh 4 World 2030 LCOX|Green Methanol|OM Variable 3.385836 EUR_2022/MWh 5 World 2050 LCOX|Green Methanol|OM Variable 3.385836 EUR_2022/MWh 6 World 2030 LCOX|Green Methanol|Input Cost|Captured CO2 38.355000 EUR_2022/MWh 7 World 2050 LCOX|Green Methanol|Input Cost|Captured CO2 25.570000 EUR_2022/MWh 8 World 2030 LCOX|Green Methanol|Input Cost|Hydrogen 140.760000 EUR_2022/MWh 9 World 2050 LCOX|Green Methanol|Input Cost|Hydrogen 93.840000 EUR_2022/MWh In\u00a0[17]: Copied! <pre>df_lcox_meoh.team.varsplit('LCOX|Green Methanol|*component') \\\n    .plot.bar(x='period', y='value', color='component')\n</pre> df_lcox_meoh.team.varsplit('LCOX|Green Methanol|*component') \\     .plot.bar(x='period', y='value', color='component') <p>Next, we can calculate the LCOX of green methanol for a the value chain consisting of electrolysis, low-temperature direct air capture, and methanol synthesis. The heat for the direct air capture will be provided by an industrial heat pump.</p> In\u00a0[18]: Copied! <pre>pc_green_meoh = ProcessChain(\n    'Green Methanol',\n    {'Methanol Synthesis': {'Methanol': Q('1 MWh')}},\n    'Heatpump for DAC -&gt; Heat =&gt; Direct Air Capture -&gt; Captured CO2 =&gt; Methanol Synthesis;Electrolysis -&gt; Hydrogen =&gt; Methanol Synthesis -&gt; Methanol',\n)\n\ng, lay = pc_green_meoh.igraph()\nfig, ax = plt.subplots()\nax.set_title(pc_green_meoh.name)\nig.plot(g, target=ax, layout=lay, vertex_label=[n.replace(' ', '\\n') for n in g.vs['name']], edge_label=[n.replace(' ', '\\n') for n in g.es['name']], vertex_label_size=8, edge_label_size=6)\n</pre> pc_green_meoh = ProcessChain(     'Green Methanol',     {'Methanol Synthesis': {'Methanol': Q('1 MWh')}},     'Heatpump for DAC -&gt; Heat =&gt; Direct Air Capture -&gt; Captured CO2 =&gt; Methanol Synthesis;Electrolysis -&gt; Hydrogen =&gt; Methanol Synthesis -&gt; Methanol', )  g, lay = pc_green_meoh.igraph() fig, ax = plt.subplots() ax.set_title(pc_green_meoh.name) ig.plot(g, target=ax, layout=lay, vertex_label=[n.replace(' ', '\\n') for n in g.vs['name']], edge_label=[n.replace(' ', '\\n') for n in g.es['name']], vertex_label_size=8, edge_label_size=6) Out[18]: <pre>&lt;igraph.drawing.matplotlib.graph.GraphArtist at 0x77a690c46a40&gt;</pre> In\u00a0[19]: Copied! <pre>df_lcox_meoh_pc = pd.concat([\n        DataSet('Tech|Electrolysis').aggregate(period=[2030, 2050], subtech=['AEL', 'PEM'], size=['1 MW', '100 MW'], agg=['subtech', 'size', 'source']),\n        DataSet('Tech|Direct Air Capture').aggregate(period=[2030, 2050], subtech='LT-DAC'),\n        DataSet('Tech|Heatpump for DAC').aggregate(period=[2030, 2050]),\n        DataSet('Tech|Methanol Synthesis').aggregate(period=[2030, 2050]),\n        pd.DataFrame.from_records([\n            {'period': 2030, 'variable': 'Price|Electricity', 'unit': 'EUR_2022/MWh', 'value': 50},\n            {'period': 2050, 'variable': 'Price|Electricity', 'unit': 'EUR_2022/MWh', 'value': 30},\n        ]),\n    ]) \\\n    .team.perform(pc_green_meoh) \\\n    .team.perform(LCOX(\n        'Methanol Synthesis|Output|Methanol', process_chain='Green Methanol',\n        interest_rate=0.1, book_lifetime=10.0,\n    ), only_new=True) \\\n    .team.unit_convert('EUR_2022/MWh')\n\ndisplay(df_lcox_meoh_pc)\n</pre> df_lcox_meoh_pc = pd.concat([         DataSet('Tech|Electrolysis').aggregate(period=[2030, 2050], subtech=['AEL', 'PEM'], size=['1 MW', '100 MW'], agg=['subtech', 'size', 'source']),         DataSet('Tech|Direct Air Capture').aggregate(period=[2030, 2050], subtech='LT-DAC'),         DataSet('Tech|Heatpump for DAC').aggregate(period=[2030, 2050]),         DataSet('Tech|Methanol Synthesis').aggregate(period=[2030, 2050]),         pd.DataFrame.from_records([             {'period': 2030, 'variable': 'Price|Electricity', 'unit': 'EUR_2022/MWh', 'value': 50},             {'period': 2050, 'variable': 'Price|Electricity', 'unit': 'EUR_2022/MWh', 'value': 30},         ]),     ]) \\     .team.perform(pc_green_meoh) \\     .team.perform(LCOX(         'Methanol Synthesis|Output|Methanol', process_chain='Green Methanol',         interest_rate=0.1, book_lifetime=10.0,     ), only_new=True) \\     .team.unit_convert('EUR_2022/MWh')  display(df_lcox_meoh_pc) <pre>/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/noslag.py:368: UserWarning:\n\nUnknown variable, so dropping rows:\n36    Emissions|CO2\n37    Emissions|CO2\nName: variable, dtype: object\n\n</pre> <pre>/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/.venv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1601: UnitStrippedWarning:\n\nThe unit of the quantity is stripped when downcasting to ndarray.\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Heat, Output|Captured CO2\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Water, Output|Hydrogen\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Output|Heat\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Captured CO2, Input|Hydrogen, Output|Heat, Output|Methanol\n\n</pre> region period variable value unit 0 World 2030 LCOX|Green Methanol|Direct Air Capture|OM Fixed 6.333591 EUR_2022/MWh 1 World 2050 LCOX|Green Methanol|Direct Air Capture|OM Fixed 5.237687 EUR_2022/MWh 2 World 2030 LCOX|Green Methanol|Direct Air Capture|OM Vari... 11.528534 EUR_2022/MWh 3 World 2050 LCOX|Green Methanol|Direct Air Capture|OM Vari... 11.528534 EUR_2022/MWh 4 World 2030 LCOX|Green Methanol|Direct Air Capture|Input C... 13.206905 EUR_2022/MWh 5 World 2050 LCOX|Green Methanol|Direct Air Capture|Input C... 7.924143 EUR_2022/MWh 6 World 2030 LCOX|Green Methanol|Electrolysis|Capital 16.639994 EUR_2022/MWh 7 World 2050 LCOX|Green Methanol|Electrolysis|Capital 10.030329 EUR_2022/MWh 8 World 2030 LCOX|Green Methanol|Electrolysis|OM Fixed 4.045965 EUR_2022/MWh 9 World 2050 LCOX|Green Methanol|Electrolysis|OM Fixed 3.059801 EUR_2022/MWh 10 World 2030 LCOX|Green Methanol|Electrolysis|Input Cost|El... 87.623100 EUR_2022/MWh 11 World 2050 LCOX|Green Methanol|Electrolysis|Input Cost|El... 50.603220 EUR_2022/MWh 12 World 2030 LCOX|Green Methanol|Heatpump for DAC|Capital 8.246405 EUR_2022/MWh 13 World 2050 LCOX|Green Methanol|Heatpump for DAC|Capital 7.408040 EUR_2022/MWh 14 World 2030 LCOX|Green Methanol|Heatpump for DAC|OM Fixed 1.013412 EUR_2022/MWh 15 World 2050 LCOX|Green Methanol|Heatpump for DAC|OM Fixed 0.910103 EUR_2022/MWh 16 World 2030 LCOX|Green Methanol|Heatpump for DAC|OM Variable 1.279063 EUR_2022/MWh 17 World 2050 LCOX|Green Methanol|Heatpump for DAC|OM Variable 1.203866 EUR_2022/MWh 18 World 2030 LCOX|Green Methanol|Heatpump for DAC|Input Cos... 0.009858 EUR_2022/MWh 19 World 2050 LCOX|Green Methanol|Heatpump for DAC|Input Cos... 0.005494 EUR_2022/MWh 20 World 2030 LCOX|Green Methanol|Methanol Synthesis|Capital 10.583450 EUR_2022/MWh 21 World 2050 LCOX|Green Methanol|Methanol Synthesis|Capital 9.831386 EUR_2022/MWh 22 World 2030 LCOX|Green Methanol|Methanol Synthesis|OM Fixed 3.231184 EUR_2022/MWh 23 World 2050 LCOX|Green Methanol|Methanol Synthesis|OM Fixed 2.984565 EUR_2022/MWh 24 World 2030 LCOX|Green Methanol|Methanol Synthesis|OM Vari... 3.385836 EUR_2022/MWh 25 World 2050 LCOX|Green Methanol|Methanol Synthesis|OM Vari... 3.385836 EUR_2022/MWh In\u00a0[20]: Copied! <pre>df_lcox_meoh_pc.team.varsplit('LCOX|Green Methanol|?process|*component') \\\n    .plot.bar(x='period', y='value', color='component', hover_data='process')\n</pre> df_lcox_meoh_pc.team.varsplit('LCOX|Green Methanol|?process|*component') \\     .plot.bar(x='period', y='value', color='component', hover_data='process') In\u00a0[21]: Copied! <pre>pc_green_ethylene = ProcessChain(\n    'Green Ethylene',\n    {'Electric Arc Furnace': {'Ethylene': Q('1t')}},\n    'Electrolysis -&gt; Hydrogen =&gt; Methanol Synthesis; Heatpump for DAC -&gt; Heat =&gt; Direct Air Capture -&gt; Captured CO2 =&gt; Methanol Synthesis -&gt; Methanol =&gt; Methanol to Olefines -&gt; Ethylene',\n)\n\ng, lay = pc_green_ethylene.igraph()\nfig, ax = plt.subplots()\nax.set_title(pc_green_ethylene.name)\nig.plot(g, target=ax, layout=lay, vertex_label=[n.replace(' ', '\\n') for n in g.vs['name']], edge_label=[n.replace(' ', '\\n') for n in g.es['name']], vertex_label_size=8, edge_label_size=6)\n</pre> pc_green_ethylene = ProcessChain(     'Green Ethylene',     {'Electric Arc Furnace': {'Ethylene': Q('1t')}},     'Electrolysis -&gt; Hydrogen =&gt; Methanol Synthesis; Heatpump for DAC -&gt; Heat =&gt; Direct Air Capture -&gt; Captured CO2 =&gt; Methanol Synthesis -&gt; Methanol =&gt; Methanol to Olefines -&gt; Ethylene', )  g, lay = pc_green_ethylene.igraph() fig, ax = plt.subplots() ax.set_title(pc_green_ethylene.name) ig.plot(g, target=ax, layout=lay, vertex_label=[n.replace(' ', '\\n') for n in g.vs['name']], edge_label=[n.replace(' ', '\\n') for n in g.es['name']], vertex_label_size=8, edge_label_size=6) Out[21]: <pre>&lt;igraph.drawing.matplotlib.graph.GraphArtist at 0x77a69275c910&gt;</pre> In\u00a0[22]: Copied! <pre>pc_green_steel = ProcessChain(\n    'Green Steel (H2-DR)',\n    {'Steel Hot Rolling': {'Steel Hot-rolled Coil': Q('1t')}},\n    'Electrolysis -&gt; Hydrogen =&gt; Iron Direct Reduction -&gt; Directly Reduced Iron =&gt; Electric Arc Furnace -&gt; Steel Liquid =&gt; Steel Casting -&gt; Steel Slab =&gt; Steel Hot Rolling -&gt; Steel Hot-rolled Coil',\n)\n\ng, lay = pc_green_steel.igraph()\nfig, ax = plt.subplots()\nax.set_title(pc_green_steel.name)\nig.plot(g, target=ax, layout=lay, vertex_label=[n.replace(' ', '\\n') for n in g.vs['name']], edge_label=[n.replace(' ', '\\n') for n in g.es['name']], vertex_label_size=8, edge_label_size=6)\n</pre> pc_green_steel = ProcessChain(     'Green Steel (H2-DR)',     {'Steel Hot Rolling': {'Steel Hot-rolled Coil': Q('1t')}},     'Electrolysis -&gt; Hydrogen =&gt; Iron Direct Reduction -&gt; Directly Reduced Iron =&gt; Electric Arc Furnace -&gt; Steel Liquid =&gt; Steel Casting -&gt; Steel Slab =&gt; Steel Hot Rolling -&gt; Steel Hot-rolled Coil', )  g, lay = pc_green_steel.igraph() fig, ax = plt.subplots() ax.set_title(pc_green_steel.name) ig.plot(g, target=ax, layout=lay, vertex_label=[n.replace(' ', '\\n') for n in g.vs['name']], edge_label=[n.replace(' ', '\\n') for n in g.es['name']], vertex_label_size=8, edge_label_size=6) Out[22]: <pre>&lt;igraph.drawing.matplotlib.graph.GraphArtist at 0x77a6927b2ec0&gt;</pre> In\u00a0[23]: Copied! <pre>df_lcox_green_steel = pd.concat([\n        DataSet('Tech|Electrolysis').aggregate(period=2030, subtech=['AEL', 'PEM'], size=['1 MW', '100 MW'], agg=['subtech', 'size', 'source'], override={'Tech|ELH2|Output Capacity|Hydrogen': 'kW;LHV'}),\n        DataSet('Tech|Iron Direct Reduction').aggregate(period=2030, mode='h2'),\n        DataSet('Tech|Electric Arc Furnace').aggregate(period=2030, mode='Primary'),\n        DataSet('Tech|Steel Casting').aggregate(period=2030),\n        DataSet('Tech|Steel Hot Rolling').aggregate(period=2030),\n        pd.DataFrame({'price_case': range(30, 60, 10), 'variable': 'Price|Electricity', 'unit': 'EUR_2020/MWh', 'value': range(30, 60, 10)}),\n    ]) \\\n    .team.perform(pc_green_steel) \\\n    .team.perform(LCOX(\n        'Steel Hot Rolling|Output|Steel Hot-rolled Coil', process_chain='Green Steel (H2-DR)',\n        interest_rate=0.1, book_lifetime=10.0,\n    ), only_new=True) \\\n    .team.unit_convert('EUR_2022/t')\n\ndisplay(df_lcox_green_steel)\n</pre> df_lcox_green_steel = pd.concat([         DataSet('Tech|Electrolysis').aggregate(period=2030, subtech=['AEL', 'PEM'], size=['1 MW', '100 MW'], agg=['subtech', 'size', 'source'], override={'Tech|ELH2|Output Capacity|Hydrogen': 'kW;LHV'}),         DataSet('Tech|Iron Direct Reduction').aggregate(period=2030, mode='h2'),         DataSet('Tech|Electric Arc Furnace').aggregate(period=2030, mode='Primary'),         DataSet('Tech|Steel Casting').aggregate(period=2030),         DataSet('Tech|Steel Hot Rolling').aggregate(period=2030),         pd.DataFrame({'price_case': range(30, 60, 10), 'variable': 'Price|Electricity', 'unit': 'EUR_2020/MWh', 'value': range(30, 60, 10)}),     ]) \\     .team.perform(pc_green_steel) \\     .team.perform(LCOX(         'Steel Hot Rolling|Output|Steel Hot-rolled Coil', process_chain='Green Steel (H2-DR)',         interest_rate=0.1, book_lifetime=10.0,     ), only_new=True) \\     .team.unit_convert('EUR_2022/t')  display(df_lcox_green_steel) <pre>/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/noslag.py:651: HarmoniseMappingFailure:\n\nNo OCF value matching a OPEX Fixed Specific value found.\n    period     reheating component region  \\\n73    2030  w/ reheating    Labour  World   \n\n                                         variable  \\\n73  Tech|Electric Arc Furnace|OPEX Fixed Specific   \n\n                               reference_variable  source      value  \n73  Tech|Electric Arc Furnace|Output|Steel Liquid  Vogl18  62.628147  \n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/noslag.py:651: HarmoniseMappingFailure:\n\nNo OCF value matching a OPEX Fixed Specific value found.\n    period      reheating component region  \\\n74    2030  w/o reheating    Labour  World   \n\n                                         variable  \\\n74  Tech|Electric Arc Furnace|OPEX Fixed Specific   \n\n                               reference_variable  source      value  \n74  Tech|Electric Arc Furnace|Output|Steel Liquid  Vogl18  62.628147  \n\n</pre> <pre>/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/.venv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py:1601: UnitStrippedWarning:\n\nThe unit of the quantity is stripped when downcasting to ndarray.\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Alloys, Input|Coal, Input|Directly Reduced Iron, Input|Fossil Gas, Input|Graphite Electrode, Input|Heat, Input|Lime, Input|Nitrogen, Input|Oxygen, Input|Steel Scrap, Output|Steel Liquid\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Water, Output|Hydrogen\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Fossil Gas, Input|Heat, Input|Hydrogen, Input|Iron Ore, Output|Directly Reduced Iron\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Heat, Input|Steel Liquid, Output|Steel Slab\n\n/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/team.py:582: UserWarning:\n\nThe following inputs/outputs are not used in LCOX, because they are neither the reference nor is an associated price given: Input|Heat, Input|Steel Slab, Output|Steel Hot-rolled Coil\n\n</pre> region period reheating price_case variable value unit 0 World 2030.0 w/ reheating 30.0 LCOX|Green Steel (H2-DR)|Electric Arc Furnace|... 51.314770 EUR_2022/t 1 World 2030.0 w/ reheating 40.0 LCOX|Green Steel (H2-DR)|Electric Arc Furnace|... 51.314770 EUR_2022/t 2 World 2030.0 w/ reheating 50.0 LCOX|Green Steel (H2-DR)|Electric Arc Furnace|... 51.314770 EUR_2022/t 3 World 2030.0 w/o reheating 30.0 LCOX|Green Steel (H2-DR)|Electric Arc Furnace|... 51.314770 EUR_2022/t 4 World 2030.0 w/o reheating 40.0 LCOX|Green Steel (H2-DR)|Electric Arc Furnace|... 51.314770 EUR_2022/t ... ... ... ... ... ... ... ... 79 World 2030.0 w/ reheating 40.0 LCOX|Green Steel (H2-DR)|Steel Hot Rolling|Inp... 4.101336 EUR_2022/t 80 World 2030.0 w/ reheating 50.0 LCOX|Green Steel (H2-DR)|Steel Hot Rolling|Inp... 5.126670 EUR_2022/t 81 World 2030.0 w/o reheating 30.0 LCOX|Green Steel (H2-DR)|Steel Hot Rolling|Inp... 3.076002 EUR_2022/t 82 World 2030.0 w/o reheating 40.0 LCOX|Green Steel (H2-DR)|Steel Hot Rolling|Inp... 4.101336 EUR_2022/t 83 World 2030.0 w/o reheating 50.0 LCOX|Green Steel (H2-DR)|Steel Hot Rolling|Inp... 5.126670 EUR_2022/t <p>84 rows \u00d7 7 columns</p> In\u00a0[24]: Copied! <pre>df_lcox_green_steel.team.varsplit('LCOX|Green Steel (H2-DR)|?process|*component') \\\n    .plot.bar(x='price_case', y='value', color='component', hover_data='process', facet_col='reheating')\n</pre> df_lcox_green_steel.team.varsplit('LCOX|Green Steel (H2-DR)|?process|*component') \\     .plot.bar(x='price_case', y='value', color='component', hover_data='process', facet_col='reheating') In\u00a0[25]: Copied! <pre>df_lcox_cement = pd.concat([\n        DataSet('Tech|Cement Production').aggregate(period=2030),\n        pd.DataFrame.from_records([\n            {'variable': 'Price|Electricity', 'unit': 'EUR_2022/MWh', 'value': 50},\n            {'variable': 'Price|Coal', 'unit': 'EUR_2022/GJ', 'value': 3},\n            {'variable': 'Price|Oxygen', 'unit': 'EUR_2022/t', 'value': 30},\n            {'variable': 'Price|Captured CO2', 'unit': 'EUR_2022/t', 'value': -30},\n        ]),\n    ]) \\\n    .team.perform(LCOX(\n        'Output|Cement', 'Cement Production',\n        interest_rate=0.1, book_lifetime=10.0,\n    ), only_new=True) \\\n    .team.unit_convert('EUR_2022/t')\n\ndisplay(df_lcox_cement)\n</pre> df_lcox_cement = pd.concat([         DataSet('Tech|Cement Production').aggregate(period=2030),         pd.DataFrame.from_records([             {'variable': 'Price|Electricity', 'unit': 'EUR_2022/MWh', 'value': 50},             {'variable': 'Price|Coal', 'unit': 'EUR_2022/GJ', 'value': 3},             {'variable': 'Price|Oxygen', 'unit': 'EUR_2022/t', 'value': 30},             {'variable': 'Price|Captured CO2', 'unit': 'EUR_2022/t', 'value': -30},         ]),     ]) \\     .team.perform(LCOX(         'Output|Cement', 'Cement Production',         interest_rate=0.1, book_lifetime=10.0,     ), only_new=True) \\     .team.unit_convert('EUR_2022/t')  display(df_lcox_cement) <pre>/home/philippv/Documents/4-projects/10-posted/01-vcs/posted/python/posted/noslag.py:368: UserWarning:\n\nUnknown variable, so dropping rows:\n37    Emissions|CO2\n38    Emissions|CO2\n39    Emissions|CO2\n40    Emissions|CO2\n41    Emissions|CO2\n42    Emissions|CO2\n43    Emissions|CO2\n44    Emissions|CO2\nName: variable, dtype: object\n\n</pre> subtech region period variable value unit 0 Integrated CaL (w/ CC) World 2030.0 LCOX|Cement Production|Capital 65.362053 EUR_2022/t 1 Standard (w/o CC) World 2030.0 LCOX|Cement Production|Capital 31.433808 EUR_2022/t 2 Tail-end CaL 20% IL (w/ CC) World 2030.0 LCOX|Cement Production|Capital 67.076978 EUR_2022/t 3 Tail-end CaL 50% IL (w/ CC) World 2030.0 LCOX|Cement Production|Capital 62.653250 EUR_2022/t 4 Integrated CaL (w/ CC) World 2030.0 LCOX|Cement Production|OM Variable 34.077011 EUR_2022/t 5 Standard (w/o CC) World 2030.0 LCOX|Cement Production|OM Variable 22.564951 EUR_2022/t 6 Tail-end CaL 20% IL (w/ CC) World 2030.0 LCOX|Cement Production|OM Variable 34.306596 EUR_2022/t 7 Tail-end CaL 50% IL (w/ CC) World 2030.0 LCOX|Cement Production|OM Variable 33.125872 EUR_2022/t 8 Integrated CaL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Coal 11.998800 EUR_2022/t 9 Standard (w/o CC) World 2030.0 LCOX|Cement Production|Input Cost|Coal 7.147440 EUR_2022/t 10 Tail-end CaL 20% IL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Coal 19.234800 EUR_2022/t 11 Tail-end CaL 50% IL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Coal 15.660000 EUR_2022/t 12 Integrated CaL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Electricity 6.395000 EUR_2022/t 13 Standard (w/o CC) World 2030.0 LCOX|Cement Production|Input Cost|Electricity 4.838000 EUR_2022/t 14 Tail-end CaL 20% IL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Electricity NaN EUR_2022/t 15 Tail-end CaL 50% IL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Electricity 2.121500 EUR_2022/t 16 Integrated CaL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Oxygen 8.382000 EUR_2022/t 17 Standard (w/o CC) World 2030.0 LCOX|Cement Production|Input Cost|Oxygen 11.250000 EUR_2022/t 18 Tail-end CaL 20% IL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Oxygen 11.250000 EUR_2022/t 19 Tail-end CaL 50% IL (w/ CC) World 2030.0 LCOX|Cement Production|Input Cost|Oxygen 9.705000 EUR_2022/t 20 Integrated CaL (w/ CC) World 2030.0 LCOX|Cement Production|Output Revenue|Captured... 21.996000 EUR_2022/t 21 Standard (w/o CC) World 2030.0 LCOX|Cement Production|Output Revenue|Captured... 0.000000 EUR_2022/t 22 Tail-end CaL 20% IL (w/ CC) World 2030.0 LCOX|Cement Production|Output Revenue|Captured... 27.720000 EUR_2022/t 23 Tail-end CaL 50% IL (w/ CC) World 2030.0 LCOX|Cement Production|Output Revenue|Captured... 25.488000 EUR_2022/t 24 Integrated CaL (w/ CC) World 2030.0 LCOX|Cement Production|Output Revenue|Electricity NaN EUR_2022/t 25 Standard (w/o CC) World 2030.0 LCOX|Cement Production|Output Revenue|Electricity NaN EUR_2022/t 26 Tail-end CaL 20% IL (w/ CC) World 2030.0 LCOX|Cement Production|Output Revenue|Electricity -4.055000 EUR_2022/t 27 Tail-end CaL 50% IL (w/ CC) World 2030.0 LCOX|Cement Production|Output Revenue|Electricity NaN EUR_2022/t <p>We first sort the dataframe by total LCOX for each subtech.</p> In\u00a0[26]: Copied! <pre>df_lcox_cement.team.varsplit('?variable|?process|*component') \\\n    .groupby('subtech') \\\n    .apply(lambda df: df.assign(order=df['value'].sum()), include_groups=False) \\\n    .sort_values(by='order') \\\n    .reset_index() \\\n    .plot.bar(x='subtech', y='value', color='component', hover_data='process')\n</pre> df_lcox_cement.team.varsplit('?variable|?process|*component') \\     .groupby('subtech') \\     .apply(lambda df: df.assign(order=df['value'].sum()), include_groups=False) \\     .sort_values(by='order') \\     .reset_index() \\     .plot.bar(x='subtech', y='value', color='component', hover_data='process')"},{"location":"tutorials/python/overview/#main-posted-tutorial-for-python","title":"Main POSTED tutorial for python\u00b6","text":""},{"location":"tutorials/python/overview/#prerequisits","title":"Prerequisits\u00b6","text":""},{"location":"tutorials/python/overview/#dependencies","title":"Dependencies\u00b6","text":""},{"location":"tutorials/python/overview/#importing-posted","title":"Importing POSTED\u00b6","text":""},{"location":"tutorials/python/overview/#noslag","title":"NOSLAG\u00b6","text":""},{"location":"tutorials/python/overview/#electrolysis-capex","title":"Electrolysis CAPEX\u00b6","text":""},{"location":"tutorials/python/overview/#energy-demand-of-green-vs-blue-hydrogen-production","title":"Energy demand of green vs. blue hydrogen production\u00b6","text":""},{"location":"tutorials/python/overview/#energy-demand-of-iron-direct-reduction","title":"Energy demand of iron direct reduction\u00b6","text":""},{"location":"tutorials/python/overview/#energy-demand-of-haber-bosch-synthesis","title":"Energy demand of Haber-Bosch synthesis\u00b6","text":""},{"location":"tutorials/python/overview/#team","title":"TEAM\u00b6","text":""},{"location":"tutorials/python/overview/#calcvariable","title":"CalcVariable\u00b6","text":""},{"location":"tutorials/python/overview/#pivot","title":"Pivot\u00b6","text":""},{"location":"tutorials/python/overview/#lcox-of-blue-and-green-hydrogen","title":"LCOX of blue and green hydrogen\u00b6","text":""},{"location":"tutorials/python/overview/#lcox-of-methanol","title":"LCOX of methanol\u00b6","text":""},{"location":"tutorials/python/overview/#lcox-of-green-ethylene-from-green-methanol","title":"LCOX of green ethylene (from green methanol)\u00b6","text":""},{"location":"tutorials/python/overview/#lcox-of-green-steel","title":"LCOX of green steel\u00b6","text":""},{"location":"tutorials/python/overview/#lcox-of-cement-w-and-wo-cc","title":"LCOX of cement w/ and w/o CC\u00b6","text":""}]}